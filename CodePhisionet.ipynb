{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Understanding the trajectory of text data in cancer studies\n",
    "## Troisieme rapport\n",
    "\n",
    "Dmytro Zhovtobriukh\n",
    "\\\n",
    "Anna Novototskykh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu111\n",
      "Requirement already satisfied: torch==1.9.1+cu111 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.9.1+cu111)\n",
      "Collecting torchvision==0.10.1+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Collecting torchaudio==0.9.1\n",
      "  Using cached torchaudio-0.9.1-cp37-cp37m-win_amd64.whl (216 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torch==1.9.1+cu111) (4.7.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision==0.10.1+cu111) (9.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision==0.10.1+cu111) (1.21.6)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.12.0+cpu\n",
      "    Uninstalling torchvision-0.12.0+cpu:\n",
      "      Successfully uninstalled torchvision-0.12.0+cpu\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.11.0+cpu\n",
      "    Uninstalling torchaudio-0.11.0+cpu:\n",
      "      Successfully uninstalled torchaudio-0.11.0+cpu\n",
      "Successfully installed torchaudio-0.9.1 torchvision-0.10.1+cu111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 --extra-index-url https://download.pytorch.org/whl/cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting signatory==1.2.6.1.9.0\n",
      "  Downloading signatory-1.2.6.1.9.0-cp37-cp37m-win_amd64.whl (225 kB)\n",
      "Installing collected packages: signatory\n",
      "Successfully installed signatory-1.2.6.1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.3.5 pytz-2023.3.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define whether to use demo dataset or the original\n",
    "USE_DEMO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 files\n",
      "ADMISSIONS.csv.gz\n",
      "CALLOUT.csv.gz\n",
      "CAREGIVERS.csv.gz\n",
      "CHARTEVENTS.csv.gz\n",
      "CPTEVENTS.csv.gz\n",
      "DATETIMEEVENTS.csv.gz\n",
      "DIAGNOSES_ICD.csv.gz\n",
      "DRGCODES.csv.gz\n",
      "D_CPT.csv.gz\n",
      "D_ICD_DIAGNOSES.csv.gz\n",
      "D_ICD_PROCEDURES.csv.gz\n",
      "D_ITEMS.csv.gz\n",
      "D_LABITEMS.csv.gz\n",
      "ICUSTAYS.csv.gz\n",
      "INPUTEVENTS_CV.csv.gz\n",
      "INPUTEVENTS_MV.csv.gz\n",
      "LABEVENTS.csv.gz\n",
      "MICROBIOLOGYEVENTS.csv.gz\n",
      "NOTEEVENTS.csv.gz\n",
      "OUTPUTEVENTS.csv.gz\n",
      "PATIENTS.csv.gz\n",
      "PRESCRIPTIONS.csv.gz\n",
      "PROCEDUREEVENTS_MV.csv.gz\n",
      "PROCEDURES_ICD.csv.gz\n",
      "SERVICES.csv.gz\n",
      "TRANSFERS.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if USE_DEMO:\n",
    "    pth = 'C:\\\\Users\\\\pc\\\\Documents\\\\Recherche\\\\physionet.org\\\\files\\\\mimiciii-demo\\\\1.4'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv')]\n",
    "else:\n",
    "    pth = 'C:\\\\Users\\\\pc\\\\Documents\\\\Recherche\\\\physionet.org\\\\files\\\\mimiciii\\\\1.4'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid `ParserError: Error tokenizing data. C error: out of memory`\n",
    "\n",
    "https://stackoverflow.com/questions/41303246/error-tokenizing-data-c-error-out-of-memory-pandas-python-large-file-csv\n",
    "\n",
    "```python\n",
    "mylist = []\n",
    "for chunk in  pd.read_csv('train.csv', chunksize=20000):\n",
    "    mylist.append(chunk)\n",
    "big_data = pd.concat(mylist, axis= 0)\n",
    "del mylist\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "big_data = pd.read_csv('train.csv', engine='python')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================ADMISSIONS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ADMITTIME\n",
      "DISCHTIME\n",
      "DEATHTIME\n",
      "ADMISSION_TYPE\n",
      "ADMISSION_LOCATION\n",
      "DISCHARGE_LOCATION\n",
      "INSURANCE\n",
      "LANGUAGE\n",
      "RELIGION\n",
      "MARITAL_STATUS\n",
      "ETHNICITY\n",
      "EDREGTIME\n",
      "EDOUTTIME\n",
      "DIAGNOSIS\n",
      "HOSPITAL_EXPIRE_FLAG\n",
      "HAS_CHARTEVENTS_DATA\n",
      "=================================CALLOUT.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SUBMIT_WARDID\n",
      "SUBMIT_CAREUNIT\n",
      "CURR_WARDID\n",
      "CURR_CAREUNIT\n",
      "CALLOUT_WARDID\n",
      "CALLOUT_SERVICE\n",
      "REQUEST_TELE\n",
      "REQUEST_RESP\n",
      "REQUEST_CDIFF\n",
      "REQUEST_MRSA\n",
      "REQUEST_VRE\n",
      "CALLOUT_STATUS\n",
      "CALLOUT_OUTCOME\n",
      "DISCHARGE_WARDID\n",
      "ACKNOWLEDGE_STATUS\n",
      "CREATETIME\n",
      "UPDATETIME\n",
      "ACKNOWLEDGETIME\n",
      "OUTCOMETIME\n",
      "FIRSTRESERVATIONTIME\n",
      "CURRENTRESERVATIONTIME\n",
      "===============================CAREGIVERS.csv.gz================================\n",
      "ROW_ID\n",
      "CGID\n",
      "LABEL\n",
      "DESCRIPTION\n",
      "===============================CHARTEVENTS.csv.gz===============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CGID\n",
      "VALUE\n",
      "VALUENUM\n",
      "VALUEUOM\n",
      "WARNING\n",
      "ERROR\n",
      "RESULTSTATUS\n",
      "STOPPED\n",
      "================================CPTEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "COSTCENTER\n",
      "CHARTDATE\n",
      "CPT_CD\n",
      "CPT_NUMBER\n",
      "CPT_SUFFIX\n",
      "TICKET_ID_SEQ\n",
      "SECTIONHEADER\n",
      "SUBSECTIONHEADER\n",
      "DESCRIPTION\n",
      "=============================DATETIMEEVENTS.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CGID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "WARNING\n",
      "ERROR\n",
      "RESULTSTATUS\n",
      "STOPPED\n",
      "==============================DIAGNOSES_ICD.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SEQ_NUM\n",
      "ICD9_CODE\n",
      "================================DRGCODES.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "DRG_TYPE\n",
      "DRG_CODE\n",
      "DESCRIPTION\n",
      "DRG_SEVERITY\n",
      "DRG_MORTALITY\n",
      "==================================D_CPT.csv.gz==================================\n",
      "ROW_ID\n",
      "CATEGORY\n",
      "SECTIONRANGE\n",
      "SECTIONHEADER\n",
      "SUBSECTIONRANGE\n",
      "SUBSECTIONHEADER\n",
      "CODESUFFIX\n",
      "MINCODEINSUBSECTION\n",
      "MAXCODEINSUBSECTION\n",
      "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
      "ROW_ID\n",
      "ICD9_CODE\n",
      "SHORT_TITLE\n",
      "LONG_TITLE\n",
      "============================D_ICD_PROCEDURES.csv.gz=============================\n",
      "ROW_ID\n",
      "ICD9_CODE\n",
      "SHORT_TITLE\n",
      "LONG_TITLE\n",
      "=================================D_ITEMS.csv.gz=================================\n",
      "ROW_ID\n",
      "ITEMID\n",
      "LABEL\n",
      "ABBREVIATION\n",
      "DBSOURCE\n",
      "LINKSTO\n",
      "CATEGORY\n",
      "UNITNAME\n",
      "PARAM_TYPE\n",
      "CONCEPTID\n",
      "===============================D_LABITEMS.csv.gz================================\n",
      "ROW_ID\n",
      "ITEMID\n",
      "LABEL\n",
      "FLUID\n",
      "CATEGORY\n",
      "LOINC_CODE\n",
      "================================ICUSTAYS.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "DBSOURCE\n",
      "FIRST_CAREUNIT\n",
      "LAST_CAREUNIT\n",
      "FIRST_WARDID\n",
      "LAST_WARDID\n",
      "INTIME\n",
      "OUTTIME\n",
      "LOS\n",
      "=============================INPUTEVENTS_CV.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "CHARTTIME\n",
      "ITEMID\n",
      "AMOUNT\n",
      "AMOUNTUOM\n",
      "RATE\n",
      "RATEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "STOPPED\n",
      "NEWBOTTLE\n",
      "ORIGINALAMOUNT\n",
      "ORIGINALAMOUNTUOM\n",
      "ORIGINALROUTE\n",
      "ORIGINALRATE\n",
      "ORIGINALRATEUOM\n",
      "ORIGINALSITE\n",
      "=============================INPUTEVENTS_MV.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTTIME\n",
      "ENDTIME\n",
      "ITEMID\n",
      "AMOUNT\n",
      "AMOUNTUOM\n",
      "RATE\n",
      "RATEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "ORDERCATEGORYNAME\n",
      "SECONDARYORDERCATEGORYNAME\n",
      "ORDERCOMPONENTTYPEDESCRIPTION\n",
      "ORDERCATEGORYDESCRIPTION\n",
      "PATIENTWEIGHT\n",
      "TOTALAMOUNT\n",
      "TOTALAMOUNTUOM\n",
      "ISOPENBAG\n",
      "CONTINUEINNEXTDEPT\n",
      "CANCELREASON\n",
      "STATUSDESCRIPTION\n",
      "COMMENTS_EDITEDBY\n",
      "COMMENTS_CANCELEDBY\n",
      "COMMENTS_DATE\n",
      "ORIGINALAMOUNT\n",
      "ORIGINALRATE\n",
      "================================LABEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "VALUE\n",
      "VALUENUM\n",
      "VALUEUOM\n",
      "FLAG\n",
      "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "CHARTDATE\n",
      "CHARTTIME\n",
      "SPEC_ITEMID\n",
      "SPEC_TYPE_DESC\n",
      "ORG_ITEMID\n",
      "ORG_NAME\n",
      "ISOLATE_NUM\n",
      "AB_ITEMID\n",
      "AB_NAME\n",
      "DILUTION_TEXT\n",
      "DILUTION_COMPARISON\n",
      "DILUTION_VALUE\n",
      "INTERPRETATION\n",
      "===============================NOTEEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "CHARTDATE\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CATEGORY\n",
      "DESCRIPTION\n",
      "CGID\n",
      "ISERROR\n",
      "TEXT\n",
      "==============================OUTPUTEVENTS.csv.gz===============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "CHARTTIME\n",
      "ITEMID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "STOPPED\n",
      "NEWBOTTLE\n",
      "ISERROR\n",
      "================================PATIENTS.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "GENDER\n",
      "DOB\n",
      "DOD\n",
      "DOD_HOSP\n",
      "DOD_SSN\n",
      "EXPIRE_FLAG\n",
      "==============================PRESCRIPTIONS.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTDATE\n",
      "ENDDATE\n",
      "DRUG_TYPE\n",
      "DRUG\n",
      "DRUG_NAME_POE\n",
      "DRUG_NAME_GENERIC\n",
      "FORMULARY_DRUG_CD\n",
      "GSN\n",
      "NDC\n",
      "PROD_STRENGTH\n",
      "DOSE_VAL_RX\n",
      "DOSE_UNIT_RX\n",
      "FORM_VAL_DISP\n",
      "FORM_UNIT_DISP\n",
      "ROUTE\n",
      "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTTIME\n",
      "ENDTIME\n",
      "ITEMID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "LOCATION\n",
      "LOCATIONCATEGORY\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "ORDERCATEGORYNAME\n",
      "SECONDARYORDERCATEGORYNAME\n",
      "ORDERCATEGORYDESCRIPTION\n",
      "ISOPENBAG\n",
      "CONTINUEINNEXTDEPT\n",
      "CANCELREASON\n",
      "STATUSDESCRIPTION\n",
      "COMMENTS_EDITEDBY\n",
      "COMMENTS_CANCELEDBY\n",
      "COMMENTS_DATE\n",
      "=============================PROCEDURES_ICD.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SEQ_NUM\n",
      "ICD9_CODE\n",
      "================================SERVICES.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "TRANSFERTIME\n",
      "PREV_SERVICE\n",
      "CURR_SERVICE\n",
      "================================TRANSFERS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "DBSOURCE\n",
      "EVENTTYPE\n",
      "PREV_CAREUNIT\n",
      "CURR_CAREUNIT\n",
      "PREV_WARDID\n",
      "CURR_WARDID\n",
      "INTIME\n",
      "OUTTIME\n",
      "LOS\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "===============================ADMISSIONS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ADMITTIME\n",
    "DISCHTIME\n",
    "DEATHTIME\n",
    "ADMISSION_TYPE\n",
    "ADMISSION_LOCATION\n",
    "DISCHARGE_LOCATION\n",
    "INSURANCE\n",
    "LANGUAGE\n",
    "RELIGION\n",
    "MARITAL_STATUS\n",
    "ETHNICITY\n",
    "EDREGTIME\n",
    "EDOUTTIME\n",
    "DIAGNOSIS\n",
    "HOSPITAL_EXPIRE_FLAG\n",
    "HAS_CHARTEVENTS_DATA\n",
    "=================================CALLOUT.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SUBMIT_WARDID\n",
    "SUBMIT_CAREUNIT\n",
    "CURR_WARDID\n",
    "CURR_CAREUNIT\n",
    "CALLOUT_WARDID\n",
    "CALLOUT_SERVICE\n",
    "REQUEST_TELE\n",
    "REQUEST_RESP\n",
    "REQUEST_CDIFF\n",
    "REQUEST_MRSA\n",
    "REQUEST_VRE\n",
    "CALLOUT_STATUS\n",
    "CALLOUT_OUTCOME\n",
    "DISCHARGE_WARDID\n",
    "ACKNOWLEDGE_STATUS\n",
    "CREATETIME\n",
    "UPDATETIME\n",
    "ACKNOWLEDGETIME\n",
    "OUTCOMETIME\n",
    "FIRSTRESERVATIONTIME\n",
    "CURRENTRESERVATIONTIME\n",
    "===============================CAREGIVERS.csv.gz================================\n",
    "ROW_ID\n",
    "CGID\n",
    "LABEL\n",
    "DESCRIPTION\n",
    "===============================CHARTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "================================CPTEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "COSTCENTER\n",
    "CHARTDATE\n",
    "CPT_CD\n",
    "CPT_NUMBER\n",
    "CPT_SUFFIX\n",
    "TICKET_ID_SEQ\n",
    "SECTIONHEADER\n",
    "SUBSECTIONHEADER\n",
    "DESCRIPTION\n",
    "=============================DATETIMEEVENTS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "==============================DIAGNOSES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================DRGCODES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "DRG_TYPE\n",
    "DRG_CODE\n",
    "DESCRIPTION\n",
    "DRG_SEVERITY\n",
    "DRG_MORTALITY\n",
    "==================================D_CPT.csv.gz==================================\n",
    "ROW_ID\n",
    "CATEGORY\n",
    "SECTIONRANGE\n",
    "SECTIONHEADER\n",
    "SUBSECTIONRANGE\n",
    "SUBSECTIONHEADER\n",
    "CODESUFFIX\n",
    "MINCODEINSUBSECTION\n",
    "MAXCODEINSUBSECTION\n",
    "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "============================D_ICD_PROCEDURES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "=================================D_ITEMS.csv.gz=================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "ABBREVIATION\n",
    "DBSOURCE\n",
    "LINKSTO\n",
    "CATEGORY\n",
    "UNITNAME\n",
    "PARAM_TYPE\n",
    "CONCEPTID\n",
    "===============================D_LABITEMS.csv.gz================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "FLUID\n",
    "CATEGORY\n",
    "LOINC_CODE\n",
    "================================ICUSTAYS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "FIRST_CAREUNIT\n",
    "LAST_CAREUNIT\n",
    "FIRST_WARDID\n",
    "LAST_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "=============================INPUTEVENTS_CV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALAMOUNTUOM\n",
    "ORIGINALROUTE\n",
    "ORIGINALRATE\n",
    "ORIGINALRATEUOM\n",
    "ORIGINALSITE\n",
    "=============================INPUTEVENTS_MV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCOMPONENTTYPEDESCRIPTION\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "PATIENTWEIGHT\n",
    "TOTALAMOUNT\n",
    "TOTALAMOUNTUOM\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALRATE\n",
    "================================LABEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "FLAG\n",
    "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "SPEC_ITEMID\n",
    "SPEC_TYPE_DESC\n",
    "ORG_ITEMID\n",
    "ORG_NAME\n",
    "ISOLATE_NUM\n",
    "AB_ITEMID\n",
    "AB_NAME\n",
    "DILUTION_TEXT\n",
    "DILUTION_COMPARISON\n",
    "DILUTION_VALUE\n",
    "INTERPRETATION\n",
    "===============================NOTEEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CATEGORY\n",
    "DESCRIPTION\n",
    "CGID\n",
    "ISERROR\n",
    "TEXT\n",
    "==============================OUTPUTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ISERROR\n",
    "================================PATIENTS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "GENDER\n",
    "DOB\n",
    "DOD\n",
    "DOD_HOSP\n",
    "DOD_SSN\n",
    "EXPIRE_FLAG\n",
    "==============================PRESCRIPTIONS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTDATE\n",
    "ENDDATE\n",
    "DRUG_TYPE\n",
    "DRUG\n",
    "DRUG_NAME_POE\n",
    "DRUG_NAME_GENERIC\n",
    "FORMULARY_DRUG_CD\n",
    "GSN\n",
    "NDC\n",
    "PROD_STRENGTH\n",
    "DOSE_VAL_RX\n",
    "DOSE_UNIT_RX\n",
    "FORM_VAL_DISP\n",
    "FORM_UNIT_DISP\n",
    "ROUTE\n",
    "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "LOCATION\n",
    "LOCATIONCATEGORY\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "=============================PROCEDURES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================SERVICES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "TRANSFERTIME\n",
    "PREV_SERVICE\n",
    "CURR_SERVICE\n",
    "================================TRANSFERS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "EVENTTYPE\n",
    "PREV_CAREUNIT\n",
    "CURR_CAREUNIT\n",
    "PREV_WARDID\n",
    "CURR_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=================================ADMISSIONS.csv=================================\n",
    "   row_id  subject_id  hadm_id            admittime            dischtime  \\\n",
    "0   12258       10006   142345  2164-10-23 21:09:00  2164-11-01 17:15:00   \n",
    "\n",
    "  deathtime admission_type    admission_location discharge_location insurance  \\\n",
    "0       NaN      EMERGENCY  EMERGENCY ROOM ADMIT   HOME HEALTH CARE  Medicare   \n",
    "\n",
    "  language  religion marital_status               ethnicity  \\\n",
    "0      NaN  CATHOLIC      SEPARATED  BLACK/AFRICAN AMERICAN   \n",
    "\n",
    "             edregtime            edouttime diagnosis  hospital_expire_flag  \\\n",
    "0  2164-10-23 16:43:00  2164-10-23 23:00:00    SEPSIS                     0   \n",
    "\n",
    "   has_chartevents_data  \n",
    "0                     1  \n",
    "==================================CALLOUT.csv===================================\n",
    "   row_id  subject_id  hadm_id  submit_wardid submit_careunit  curr_wardid  \\\n",
    "0    3917       10017   199207              7             NaN           45   \n",
    "\n",
    "  curr_careunit  callout_wardid callout_service  request_tele  ...  \\\n",
    "0           CCU               1             MED             1  ...   \n",
    "\n",
    "   callout_status  callout_outcome  discharge_wardid  acknowledge_status  \\\n",
    "0        Inactive       Discharged              45.0        Acknowledged   \n",
    "\n",
    "            createtime           updatetime      acknowledgetime  \\\n",
    "0  2149-05-31 10:44:34  2149-05-31 10:44:34  2149-05-31 15:08:04   \n",
    "\n",
    "           outcometime firstreservationtime currentreservationtime  \n",
    "0  2149-05-31 22:40:02                  NaN                    NaN  \n",
    "\n",
    "[1 rows x 24 columns]\n",
    "=================================CAREGIVERS.csv=================================\n",
    "   row_id   cgid label description\n",
    "0    2228  16174    RO   Read Only\n",
    "================================CHARTEVENTS.csv=================================\n",
    "    row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  5279021       40124   126179    279554.0  223761  2130-02-04 04:00:00   \n",
    "\n",
    "             storetime   cgid value  valuenum valueuom  warning  error  \\\n",
    "0  2130-02-04 04:35:00  19085  95.9      95.9       ?F      0.0    0.0   \n",
    "\n",
    "  resultstatus stopped  \n",
    "0          NaN     NaN  \n",
    "=================================CPTEVENTS.csv==================================\n",
    "   row_id  subject_id  hadm_id costcenter chartdate  cpt_cd  cpt_number  \\\n",
    "0    4615       10117   105150        ICU       NaN   99254       99254   \n",
    "\n",
    "   cpt_suffix  ticket_id_seq              sectionheader subsectionheader  \\\n",
    "0         NaN            1.0  Evaluation and management    Consultations   \n",
    "\n",
    "  description  \n",
    "0         NaN  \n",
    "===============================DATETIMEEVENTS.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  208474       10076   198503    201006.0    5684  2107-03-25 04:00:00   \n",
    "\n",
    "             storetime   cgid                value valueuom  warning  error  \\\n",
    "0  2107-03-25 04:34:00  20482  2107-03-24 00:00:00     Date      NaN    NaN   \n",
    "\n",
    "   resultstatus   stopped  \n",
    "0           NaN  NotStopd  \n",
    "===============================DIAGNOSES_ICD.csv================================\n",
    "   row_id  subject_id  hadm_id  seq_num icd9_code\n",
    "0  112344       10006   142345        1     99591\n",
    "==================================DRGCODES.csv==================================\n",
    "   row_id  subject_id  hadm_id drg_type  drg_code  \\\n",
    "0    1338       10130   156668     HCFA       148   \n",
    "\n",
    "                                         description  drg_severity  \\\n",
    "0  MAJOR SMALL & LARGE BOWEL PROCEDURES WITH COMP...           NaN   \n",
    "\n",
    "   drg_mortality  \n",
    "0            NaN  \n",
    "===================================D_CPT.csv====================================\n",
    "   row_id  category sectionrange              sectionheader subsectionrange  \\\n",
    "0       1         1  99201-99499  Evaluation and management     99201-99216   \n",
    "\n",
    "                   subsectionheader codesuffix  mincodeinsubsection  \\\n",
    "0  Office/other outpatient services        NaN                99201   \n",
    "\n",
    "   maxcodeinsubsection  \n",
    "0                99216  \n",
    "==============================D_ICD_DIAGNOSES.csv===============================\n",
    "   row_id icd9_code              short_title  \\\n",
    "0       1     01716  Erythem nod tb-oth test   \n",
    "\n",
    "                                          long_title  \n",
    "0  Erythema nodosum with hypersensitivity reactio...  \n",
    "==============================D_ICD_PROCEDURES.csv==============================\n",
    "   row_id  icd9_code               short_title  \\\n",
    "0       1       1423  Chorioret les xenon coag   \n",
    "\n",
    "                                          long_title  \n",
    "0  Destruction of chorioretinal lesion by xenon a...  \n",
    "==================================D_ITEMS.csv===================================\n",
    "   row_id  itemid               label abbreviation dbsource      linksto  \\\n",
    "0       1    1435  Sustained Nystamus          NaN  carevue  chartevents   \n",
    "\n",
    "  category unitname param_type  conceptid  \n",
    "0      NaN      NaN        NaN        NaN  \n",
    "=================================D_LABITEMS.csv=================================\n",
    "   row_id  itemid          label  fluid   category loinc_code\n",
    "0       1   50800  SPECIMEN TYPE  BLOOD  BLOOD GAS        NaN\n",
    "==================================ICUSTAYS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource first_careunit  \\\n",
    "0   12742       10006   142345      206504  carevue           MICU   \n",
    "\n",
    "  last_careunit  first_wardid  last_wardid               intime  \\\n",
    "0          MICU            52           52  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime     los  \n",
    "0  2164-10-25 12:21:07  1.6325  \n",
    "===============================INPUTEVENTS_CV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    1184       10114   167957      234989  2171-11-03 15:00:00   30056   \n",
    "\n",
    "   amount amountuom  rate rateuom  ...  orderid  linkorderid  stopped  \\\n",
    "0   400.0        ml   NaN     NaN  ...  2557279      2557279      NaN   \n",
    "\n",
    "   newbottle originalamount  originalamountuom  originalroute originalrate  \\\n",
    "0        NaN            NaN                 ml           Oral          NaN   \n",
    "\n",
    "  originalrateuom  originalsite  \n",
    "0             NaN           NaN  \n",
    "\n",
    "[1 rows x 22 columns]\n",
    "===============================INPUTEVENTS_MV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0  118897       42367   139932      250305  2147-10-29 16:45:00   \n",
    "\n",
    "               endtime  itemid  amount amountuom  rate  ... totalamountuom  \\\n",
    "0  2147-10-29 16:46:00  225799    60.0        ml   NaN  ...             ml   \n",
    "\n",
    "  isopenbag  continueinnextdept  cancelreason  statusdescription  \\\n",
    "0         0                   0             0    FinishedRunning   \n",
    "\n",
    "  comments_editedby comments_canceledby comments_date originalamount  \\\n",
    "0               NaN                 NaN           NaN           60.0   \n",
    "\n",
    "   originalrate  \n",
    "0          60.0  \n",
    "\n",
    "[1 rows x 31 columns]\n",
    "=================================LABEVENTS.csv==================================\n",
    "    row_id  subject_id  hadm_id  itemid            charttime value  valuenum  \\\n",
    "0  6244563       10006      NaN   50868  2164-09-24 20:21:00    19      19.0   \n",
    "\n",
    "  valueuom flag  \n",
    "0    mEq/L  NaN  \n",
    "=============================MICROBIOLOGYEVENTS.csv=============================\n",
    "   row_id  subject_id  hadm_id            chartdate            charttime  \\\n",
    "0  134694       10006   142345  2164-10-23 00:00:00  2164-10-23 15:30:00   \n",
    "\n",
    "   spec_itemid spec_type_desc  org_itemid                            org_name  \\\n",
    "0        70012  BLOOD CULTURE     80155.0  STAPHYLOCOCCUS, COAGULASE NEGATIVE   \n",
    "\n",
    "   isolate_num  ab_itemid ab_name dilution_text dilution_comparison  \\\n",
    "0          2.0        NaN     NaN           NaN                 NaN   \n",
    "\n",
    "   dilution_value interpretation  \n",
    "0             NaN            NaN  \n",
    "=================================NOTEEVENTS.csv=================================\n",
    "Empty DataFrame\n",
    "Columns: [row_id, subject_id, hadm_id, chartdate, charttime, storetime, category, description, cgid, iserror, text]\n",
    "Index: []\n",
    "================================OUTPUTEVENTS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    6540       10114   167957    234989.0  2171-10-30 20:00:00   40055   \n",
    "\n",
    "   value valueuom            storetime   cgid  stopped  newbottle  iserror  \n",
    "0   39.0       ml  2171-10-30 20:38:00  15029      NaN        NaN      NaN  \n",
    "==================================PATIENTS.csv==================================\n",
    "   row_id  subject_id gender                  dob                  dod  \\\n",
    "0    9467       10006      F  2094-03-05 00:00:00  2165-08-12 00:00:00   \n",
    "\n",
    "              dod_hosp              dod_ssn  expire_flag  \n",
    "0  2165-08-12 00:00:00  2165-08-12 00:00:00            1  \n",
    "===============================PRESCRIPTIONS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            startdate  \\\n",
    "0   32600       42458   159647         NaN  2146-07-21 00:00:00   \n",
    "\n",
    "               enddate drug_type                         drug  \\\n",
    "0  2146-07-22 00:00:00      MAIN  Pneumococcal Vac Polyvalent   \n",
    "\n",
    "                 drug_name_poe            drug_name_generic formulary_drug_cd  \\\n",
    "0  Pneumococcal Vac Polyvalent  PNEUMOcoccal Vac Polyvalent           PNEU25I   \n",
    "\n",
    "       gsn        ndc     prod_strength dose_val_rx dose_unit_rx  \\\n",
    "0  48548.0  6494300.0  25mcg/0.5mL Vial         0.5           mL   \n",
    "\n",
    "  form_val_disp form_unit_disp route  \n",
    "0             1           VIAL    IM  \n",
    "=============================PROCEDUREEVENTS_MV.csv=============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0    8641       42367   139932      250305  2147-10-03 16:40:00   \n",
    "\n",
    "               endtime  itemid  value valueuom        location  ...  \\\n",
    "0  2147-10-06 20:00:00  224263   4520      min  Right Femoral.  ...   \n",
    "\n",
    "  ordercategoryname secondaryordercategoryname  ordercategorydescription  \\\n",
    "0    Invasive Lines                        NaN                      Task   \n",
    "\n",
    "   isopenbag  continueinnextdept cancelreason  statusdescription  \\\n",
    "0          1                   0            0    FinishedRunning   \n",
    "\n",
    "  comments_editedby  comments_canceledby  comments_date  \n",
    "0               NaN                  NaN            NaN  \n",
    "\n",
    "[1 rows x 25 columns]\n",
    "===============================PROCEDURES_ICD.csv===============================\n",
    "   row_id  subject_id  hadm_id  seq_num  icd9_code\n",
    "0    3994       10114   167957        1       3605\n",
    "==================================SERVICES.csv==================================\n",
    "   row_id  subject_id  hadm_id         transfertime prev_service curr_service\n",
    "0   14974       10006   142345  2164-10-23 21:10:15          NaN          MED\n",
    "=================================TRANSFERS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource eventtype prev_careunit  \\\n",
    "0   54440       10006   142345    206504.0  carevue     admit           NaN   \n",
    "\n",
    "  curr_careunit  prev_wardid  curr_wardid               intime  \\\n",
    "0          MICU          NaN         52.0  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime    los  \n",
    "0  2164-10-25 12:21:07  39.18  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données et les packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axJrX0R--kky"
   },
   "source": [
    "Dans cette section, nous incluons les bibliothèques nécessaires et chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures.\n",
    "\n",
    "Voici le but d'utilisation de modules:\n",
    "* Pandas - le module pour gérer facilement les tables (dataframes)\n",
    "* Numpy - le module de base pour la plupart de modules scientifiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW_ID                                                       174\n",
      "SUBJECT_ID                                                 22532\n",
      "HADM_ID                                                 167853.0\n",
      "CHARTDATE                                             2151-08-04\n",
      "CHARTTIME                                                    NaN\n",
      "STORETIME                                                    NaN\n",
      "CATEGORY                                       Discharge summary\n",
      "DESCRIPTION                                               Report\n",
      "CGID                                                         NaN\n",
      "ISERROR                                                      NaN\n",
      "TEXT           Admission Date:  [**2151-7-16**]       Dischar...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission Date:  [**2151-7-16**]       Discharge Date:  [**2151-8-4**]\n",
      "\n",
      "\n",
      "Service:\n",
      "ADDENDUM:\n",
      "\n",
      "RADIOLOGIC STUDIES:  Radiologic studies also included a chest\n",
      "CT, which confirmed cavitary lesions in the left lung apex\n",
      "consistent with infectious process/tuberculosis.  This also\n",
      "moderate-sized left pleural effusion.\n",
      "\n",
      "HEAD CT:  Head CT showed no intracranial hemorrhage or mass\n",
      "effect, but old infarction consistent with past medical\n",
      "history.\n",
      "\n",
      "ABDOMINAL CT:  Abdominal CT showed lesions of\n",
      "T10 and sacrum most likely secondary to osteoporosis. These can\n",
      "be followed by repeat imaging as an outpatient.\n",
      "\n",
      "\n",
      "\n",
      "                            [**First Name8 (NamePattern2) **] [**First Name4 (NamePattern1) 1775**] [**Last Name (NamePattern1) **], M.D.  [**MD Number(1) 1776**]\n",
      "\n",
      "Dictated By:[**Hospital 1807**]\n",
      "MEDQUIST36\n",
      "\n",
      "D:  [**2151-8-5**]  12:11\n",
      "T:  [**2151-8-5**]  12:21\n",
      "JOB#:  [**Job Number 1808**]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0,:]['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vtaj1bVu-kky"
   },
   "outputs": [],
   "source": [
    "# Lire les données à partir d'un fichier CSV dans un DataFrame Pandas\n",
    "#data = pd.read_csv(\"/kaggle/input/clean-data/clean_data.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "# Convertir la colonne 'TIME' en entiers\n",
    "# data['TIME'] = data['TIME'].astype(int)\n",
    "data['TIME'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58976"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(adm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>165315</td>\n",
       "      <td>2196-04-09 12:26:00</td>\n",
       "      <td>2196-04-10 15:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DISC-TRAN CANCER/CHLDRN H</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2196-04-09 10:06:00</td>\n",
       "      <td>2196-04-09 13:24:00</td>\n",
       "      <td>BENZODIAZEPINE OVERDOSE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>152223</td>\n",
       "      <td>2153-09-03 07:15:00</td>\n",
       "      <td>2153-09-08 19:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>124321</td>\n",
       "      <td>2157-10-18 19:34:00</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>161859</td>\n",
       "      <td>2139-06-06 16:14:00</td>\n",
       "      <td>2139-06-09 12:48:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR MYOCARDIAL INFARCTION</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>129635</td>\n",
       "      <td>2160-11-02 02:06:00</td>\n",
       "      <td>2160-11-05 14:55:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2160-11-02 01:01:00</td>\n",
       "      <td>2160-11-02 04:27:00</td>\n",
       "      <td>ACUTE CORONARY SYNDROME</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58971</th>\n",
       "      <td>58594</td>\n",
       "      <td>98800</td>\n",
       "      <td>191113</td>\n",
       "      <td>2131-03-30 21:13:00</td>\n",
       "      <td>2131-04-02 15:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-03-30 19:44:00</td>\n",
       "      <td>2131-03-30 22:41:00</td>\n",
       "      <td>TRAUMA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58972</th>\n",
       "      <td>58595</td>\n",
       "      <td>98802</td>\n",
       "      <td>101071</td>\n",
       "      <td>2151-03-05 20:00:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2151-03-05 17:23:00</td>\n",
       "      <td>2151-03-05 21:06:00</td>\n",
       "      <td>SAH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58973</th>\n",
       "      <td>58596</td>\n",
       "      <td>98805</td>\n",
       "      <td>122631</td>\n",
       "      <td>2200-09-12 07:15:00</td>\n",
       "      <td>2200-09-20 12:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENAL CANCER/SDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58974</th>\n",
       "      <td>58597</td>\n",
       "      <td>98813</td>\n",
       "      <td>170407</td>\n",
       "      <td>2128-11-11 02:29:00</td>\n",
       "      <td>2128-12-22 13:11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>SNF</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2128-11-10 23:48:00</td>\n",
       "      <td>2128-11-11 03:16:00</td>\n",
       "      <td>S/P FALL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58975</th>\n",
       "      <td>58598</td>\n",
       "      <td>98813</td>\n",
       "      <td>190264</td>\n",
       "      <td>2131-10-25 03:09:00</td>\n",
       "      <td>2131-10-26 17:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-10-25 00:08:00</td>\n",
       "      <td>2131-10-25 04:35:00</td>\n",
       "      <td>INTRACRANIAL HEMORRHAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58976 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
       "0          21          22   165315  2196-04-09 12:26:00  2196-04-10 15:54:00   \n",
       "1          22          23   152223  2153-09-03 07:15:00  2153-09-08 19:10:00   \n",
       "2          23          23   124321  2157-10-18 19:34:00  2157-10-25 14:00:00   \n",
       "3          24          24   161859  2139-06-06 16:14:00  2139-06-09 12:48:00   \n",
       "4          25          25   129635  2160-11-02 02:06:00  2160-11-05 14:55:00   \n",
       "...       ...         ...      ...                  ...                  ...   \n",
       "58971   58594       98800   191113  2131-03-30 21:13:00  2131-04-02 15:02:00   \n",
       "58972   58595       98802   101071  2151-03-05 20:00:00  2151-03-06 09:10:00   \n",
       "58973   58596       98805   122631  2200-09-12 07:15:00  2200-09-20 12:08:00   \n",
       "58974   58597       98813   170407  2128-11-11 02:29:00  2128-12-22 13:11:00   \n",
       "58975   58598       98813   190264  2131-10-25 03:09:00  2131-10-26 17:44:00   \n",
       "\n",
       "                 DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
       "0                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "1                      NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "2                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "3                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "4                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "...                    ...            ...                        ...   \n",
       "58971                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58972  2151-03-06 09:10:00      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58973                  NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "58974                  NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "58975                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "\n",
       "              DISCHARGE_LOCATION INSURANCE LANGUAGE           RELIGION  \\\n",
       "0      DISC-TRAN CANCER/CHLDRN H   Private      NaN       UNOBTAINABLE   \n",
       "1               HOME HEALTH CARE  Medicare      NaN           CATHOLIC   \n",
       "2               HOME HEALTH CARE  Medicare     ENGL           CATHOLIC   \n",
       "3                           HOME   Private      NaN  PROTESTANT QUAKER   \n",
       "4                           HOME   Private      NaN       UNOBTAINABLE   \n",
       "...                          ...       ...      ...                ...   \n",
       "58971                       HOME   Private     ENGL      NOT SPECIFIED   \n",
       "58972               DEAD/EXPIRED  Medicare     ENGL           CATHOLIC   \n",
       "58973           HOME HEALTH CARE   Private     ENGL      NOT SPECIFIED   \n",
       "58974                        SNF   Private     ENGL           CATHOLIC   \n",
       "58975                       HOME   Private     ENGL           CATHOLIC   \n",
       "\n",
       "      MARITAL_STATUS ETHNICITY            EDREGTIME            EDOUTTIME  \\\n",
       "0            MARRIED     WHITE  2196-04-09 10:06:00  2196-04-09 13:24:00   \n",
       "1            MARRIED     WHITE                  NaN                  NaN   \n",
       "2            MARRIED     WHITE                  NaN                  NaN   \n",
       "3             SINGLE     WHITE                  NaN                  NaN   \n",
       "4            MARRIED     WHITE  2160-11-02 01:01:00  2160-11-02 04:27:00   \n",
       "...              ...       ...                  ...                  ...   \n",
       "58971         SINGLE     WHITE  2131-03-30 19:44:00  2131-03-30 22:41:00   \n",
       "58972        WIDOWED     WHITE  2151-03-05 17:23:00  2151-03-05 21:06:00   \n",
       "58973        MARRIED     WHITE                  NaN                  NaN   \n",
       "58974        MARRIED     WHITE  2128-11-10 23:48:00  2128-11-11 03:16:00   \n",
       "58975        MARRIED     WHITE  2131-10-25 00:08:00  2131-10-25 04:35:00   \n",
       "\n",
       "                                               DIAGNOSIS  \\\n",
       "0                                BENZODIAZEPINE OVERDOSE   \n",
       "1      CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...   \n",
       "2                                             BRAIN MASS   \n",
       "3                         INTERIOR MYOCARDIAL INFARCTION   \n",
       "4                                ACUTE CORONARY SYNDROME   \n",
       "...                                                  ...   \n",
       "58971                                             TRAUMA   \n",
       "58972                                                SAH   \n",
       "58973                                   RENAL CANCER/SDA   \n",
       "58974                                           S/P FALL   \n",
       "58975                            INTRACRANIAL HEMORRHAGE   \n",
       "\n",
       "       HOSPITAL_EXPIRE_FLAG  HAS_CHARTEVENTS_DATA  \n",
       "0                         0                     1  \n",
       "1                         0                     1  \n",
       "2                         0                     1  \n",
       "3                         0                     1  \n",
       "4                         0                     1  \n",
       "...                     ...                   ...  \n",
       "58971                     0                     1  \n",
       "58972                     1                     1  \n",
       "58973                     0                     1  \n",
       "58974                     0                     0  \n",
       "58975                     0                     1  \n",
       "\n",
       "[58976 rows x 19 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire les données d'admission à partir d'un autre fichier CSV dans un DataFrame Pandas\n",
    "# adm = pd.read_csv(\"/kaggle/input/clean-data/clean_adm.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITTIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMTTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"SUBJECT_ID\"].isin(data[\"SUBJECT_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHigEHe_-kk1"
   },
   "source": [
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T13:22:27.799253Z",
     "iopub.status.busy": "2023-08-27T13:22:27.798779Z",
     "iopub.status.idle": "2023-08-27T13:22:41.546351Z",
     "shell.execute_reply": "2023-08-27T13:22:41.544957Z",
     "shell.execute_reply.started": "2023-08-27T13:22:27.799212Z"
    },
    "id": "f3I6vt3q-kk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46139\n"
     ]
    }
   ],
   "source": [
    "# Créer un dataframe vide avec deux colonnes pour le nouveau dataframe\n",
    "new_data = pd.DataFrame(columns=[\"SUBJECT_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
    "\n",
    "# Grouper les données d'admission par \"SUBJECT_ID\"\n",
    "grouped_adm = adm.groupby(\"SUBJECT_ID\")\n",
    "print(len(grouped_adm))\n",
    "\n",
    "# Parcourir chaque groupe de données associées à un \"SUBJECT_ID\"\n",
    "for subject_id, group in grouped_adm:\n",
    "    # Vérifier si le groupe contient au moins un enregistrement avec la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    if group[\"HOSPITAL_EXPIRE_FLAG\"].eq(1).any():\n",
    "        # Si oui, ajouter le \"SUBJECT_ID\" et la valeur 1 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [1]})], ignore_index=True)\n",
    "    else:\n",
    "        # Sinon, ajouter le \"SUBJECT_ID\" et la valeur 0 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [0]})], ignore_index=True)\n",
    "\n",
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0\n",
    "label_0 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "# Fonction pour diviser les documents en plus petits morceaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVkUXJXk-kk1"
   },
   "source": [
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7I4Tdbb-kk2"
   },
   "source": [
    "# Jeux de données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QDtdhfB-kk2"
   },
   "source": [
    "Cette section prépare l'ensemble de données d'apprentissage en extrayant et traitant les embeddings ClinicalBERT à partir des informations relatives aux patients. Ces embeddings sont ensuite organisés en vue d'analyses ultérieures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## [TRAIN] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36wqGMV3-kk3"
   },
   "source": [
    "Dans cette partie, nous chargeons le modèle ClinicalBERT à partir du référentiel Hugging Face et sélectionnons un échantillon spécifique de patients pour l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\n",
    "sample = pd.concat([label_1.sample(n=100, random_state=56), label_0.sample(n=100, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\n",
    "filtered_data = data[data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'SUBJECT_ID' en agrégeant les listes de 'TEXT' et 'TIME'\n",
    "grouped_sample = filtered_data.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## [TRAIN] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgSkdofN-kk3"
   },
   "source": [
    "Dans cette partie, le code traite les données textuelles en les divisant en segments plus petits et extrait les embeddings ClinicalBERT correspondants à ces segments. Le résultat est un dictionnaire qui associe chaque patient à ses embeddings ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "time_list = []\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Parcourir les patients et leurs données associées\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer la liste des documents\n",
    "    times = values['TIME']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "    embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "        # Diviser le texte en parties égales\n",
    "        encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "        n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "        n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "        parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "        # Stocker les embeddings des différentes parties du document\n",
    "        cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for partie in parties:\n",
    "            # Convertir la partie dans un format compatible avec le modèle\n",
    "            inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # Stocker l'embedding dans la liste\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "            # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "            time_list.append(time)\n",
    "\n",
    "        # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "    embeddings_dict[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## [TRAIN] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yid4ozu9-kk4"
   },
   "source": [
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### [TRAIN] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: scipy, joblib, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 10000, Embedding shape: (24, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "# Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings pour créer une matrice unique\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "# Diviser les embeddings réduits pour chaque patient\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### [TRAIN] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QjMOhW3-kk4"
   },
   "source": [
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliquée par chaque composante principale: [0.15687051 0.12194031 0.08685473 0.05406694 0.04830988 0.04441428\n",
      " 0.03371466 0.02746247 0.02233062 0.01886391 0.01859464 0.01683133\n",
      " 0.01429177 0.01289333 0.01203107 0.01050083 0.00956827 0.00914414\n",
      " 0.00852762 0.00787063 0.00720696 0.00691318 0.00658497 0.00617374\n",
      " 0.00557866 0.005488   0.00484132 0.00476844 0.00455266 0.00443011\n",
      " 0.00433773 0.00399491 0.0037172  0.00357913 0.00346258 0.00339894\n",
      " 0.00329882 0.0030952  0.00297395 0.00268589 0.00266932 0.00257588\n",
      " 0.00249187 0.00243291 0.00227161 0.00217559 0.00210547 0.00206206\n",
      " 0.00199798 0.00190873 0.0018846  0.00185075 0.00180102 0.00175915\n",
      " 0.00174075 0.00172576 0.00167542 0.001614   0.00156624 0.00150878\n",
      " 0.00146477 0.00143411 0.00140291 0.0013794  0.00133722 0.00130369\n",
      " 0.00129745 0.00128346 0.00126033 0.0012087  0.00117082 0.00114967\n",
      " 0.00112559 0.00112096 0.00109772 0.00105876 0.0010284  0.00102636\n",
      " 0.00100847 0.00098992 0.00097796 0.00095227 0.00094002 0.00092038\n",
      " 0.00088646 0.00087764 0.00086764 0.00085527 0.00084452 0.00081647\n",
      " 0.00081117 0.00080269 0.00079432 0.00077793 0.00075168 0.00072707\n",
      " 0.00071722 0.00070577 0.00069008 0.00067312]\n",
      "Variance totale expliquée: 0.906520561198704\n",
      "ID du patient : 10000, Forme des embeddings : (24, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings en une seule matrice\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Afficher la variance totale expliquée par toutes les composantes principales\n",
    "print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## [TRAIN] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHA3C_Ib-kk5"
   },
   "source": [
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronquée\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "for key, value in reduced_embeddings_dict.items():\n",
    "    # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "    tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "\n",
    "    # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "    # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "\n",
    "    # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "    log_signature_dict[key] = log_signature\n",
    "\n",
    "# À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10000': tensor([ 1.5807,  4.1867,  3.0287,  ..., -0.0092,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '10286': tensor([ 2.5318, -0.2515,  1.8915,  ...,  0.0343,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '10532': tensor([-0.4030,  3.5097, -0.5323,  ...,  0.0430,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11271': tensor([ 3.6321,  0.2937, -0.7424,  ...,  0.0546,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1136': tensor([ 3.3834,  0.7540,  0.6607,  ..., -0.0326,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11766': tensor([ 4.4018,  4.1407,  0.0046,  ..., -0.1344,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11873': tensor([-0.9014, -1.5977, -0.9957,  ..., -0.0239,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11960': tensor([1.2511, 4.1776, 3.0700,  ..., 0.0474, 0.0000, 0.0000], device='cuda:0'), '12': tensor([5.4604, 0.9422, 1.4187,  ..., 0.1017, 0.0000, 0.0000], device='cuda:0'), '12183': tensor([3.5300, 4.5022, 4.1352,  ..., 0.0863, 0.0000, 0.0000], device='cuda:0'), '12598': tensor([ 3.3212, -2.9122,  0.9391,  ...,  0.0751,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '12712': tensor([3.8651, 5.8274, 1.6218,  ..., 0.1687, 0.0000, 0.0000], device='cuda:0'), '12759': tensor([ 4.3779,  0.5389,  1.0512,  ..., -0.0607,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '12824': tensor([ 5.0613,  1.5975,  2.9631,  ..., -0.0220,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1333': tensor([ 3.3052, -1.1528,  1.2735,  ...,  0.0062,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1339': tensor([ 0.5142, -1.7884, -0.0713,  ...,  0.7048,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1351': tensor([ 1.5992, -1.1741,  0.7032,  ...,  0.0459,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '13695': tensor([1.9971, 0.8767, 0.6699,  ..., 0.0395, 0.0000, 0.0000], device='cuda:0'), '13744': tensor([ 4.9923,  1.0971,  1.7721,  ..., -0.0245,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14036': tensor([ 5.9998,  0.4681,  1.0532,  ..., -0.0535,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14080': tensor([3.1026, 3.5456, 2.5602,  ..., 0.0515, 0.0000, 0.0000], device='cuda:0'), '14218': tensor([ 5.0178,  1.5802,  2.1028,  ..., -0.1063,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1426': tensor([2.2174, 2.5090, 2.6471,  ..., 0.0435, 0.0000, 0.0000], device='cuda:0'), '14338': tensor([ 4.3874, -1.4642, -2.3702,  ..., -0.0815,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14366': tensor([ 4.7942, -0.2571,  0.8159,  ...,  0.0539,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1465': tensor([ 2.4285, -1.3811,  3.3521,  ..., -0.0297,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14847': tensor([ 3.1676, -1.2690,  2.3175,  ..., -0.0299,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14937': tensor([4.7316, 5.4671, 1.0546,  ..., 0.0318, 0.0000, 0.0000], device='cuda:0'), '1503': tensor([ 5.0051,  0.2059,  3.5410,  ..., -0.0178,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '15185': tensor([ 3.6136,  0.6481,  2.7206,  ..., -0.2585,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '15590': tensor([ 3.0548,  1.1607,  1.7483,  ..., -0.0206,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16026': tensor([ 3.5928,  0.7963,  1.3735,  ..., -0.0897,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16102': tensor([ 2.6173,  3.1008,  2.1598,  ..., -0.0534,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16176': tensor([4.8479, 0.4483, 2.1987,  ..., 0.0909, 0.0000, 0.0000], device='cuda:0'), '16189': tensor([ 2.7253,  1.0001,  2.2216,  ..., -0.0617,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16283': tensor([4.6229, 0.2980, 0.2789,  ..., 0.0304, 0.0000, 0.0000], device='cuda:0'), '164': tensor([ 2.9157,  0.0965,  1.6099,  ..., -0.0615,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16874': tensor([5.1412, 3.1146, 4.4286,  ..., 0.0927, 0.0000, 0.0000], device='cuda:0'), '17489': tensor([4.3073, 1.0093, 0.0560,  ..., 0.0552, 0.0000, 0.0000], device='cuda:0'), '17798': tensor([ 1.5020, -1.8774, -2.2531,  ...,  0.1496,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '17805': tensor([ 4.3211,  2.8190,  2.9103,  ..., -0.0359,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1787': tensor([4.3230, 0.8931, 3.0771,  ..., 0.0577, 0.0000, 0.0000], device='cuda:0'), '17907': tensor([ 2.4556,  2.1029,  1.5636,  ..., -0.0478,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '17924': tensor([3.2823, 4.4991, 2.8658,  ..., 0.3125, 0.0000, 0.0000], device='cuda:0'), '17972': tensor([ 2.2049, -1.1618,  3.6418,  ...,  0.1447,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18094': tensor([ 3.3648,  1.6715,  1.8700,  ..., -0.2297,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1855': tensor([ 1.3345, -1.0260,  0.5199,  ...,  0.0822,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18566': tensor([-0.2963,  0.0276,  0.4130,  ...,  0.0587,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18873': tensor([2.5414, 0.1039, 4.2670,  ..., 0.0237, 0.0000, 0.0000], device='cuda:0'), '19271': tensor([-0.7159, -2.7118, -1.2443,  ..., -0.0190,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1944': tensor([ 4.4603,  3.3799,  3.5925,  ..., -0.1350,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '19499': tensor([ 5.6195,  1.6633,  1.1754,  ..., -0.0522,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '19603': tensor([ 2.4078, -0.9668,  1.6626,  ...,  0.0392,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '20023': tensor([ 5.1323e+00, -3.2783e-04,  1.1528e+00,  ..., -4.8847e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '20062': tensor([ 4.2657e+00, -4.9050e-01,  9.3489e-01,  ...,  3.3600e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '20244': tensor([ 4.1657,  0.5081,  2.1965,  ..., -0.0243,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '20651': tensor([ 3.8625,  0.1595,  1.7197,  ..., -0.0466,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2121': tensor([4.2433, 2.7482, 1.7740,  ..., 0.0717, 0.0000, 0.0000], device='cuda:0'), '21402': tensor([ 1.4062, -0.5293,  1.0387,  ..., -0.0412,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '21842': tensor([ 5.5954,  3.3177,  5.3091,  ..., -0.1651,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '21862': tensor([ 4.5231,  0.9569,  1.8266,  ..., -0.0370,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22016': tensor([ 4.2063, -0.7874,  1.6900,  ..., -0.0096,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22220': tensor([ 3.0740, -0.0798,  3.4349,  ...,  1.8974,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22241': tensor([4.5646, 0.1156, 1.8938,  ..., 0.2013, 0.0000, 0.0000], device='cuda:0'), '22263': tensor([ 0.8448, -0.5012, -0.6397,  ...,  0.0643,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22660': tensor([2.1509, 4.4302, 2.9270,  ..., 0.0562, 0.0000, 0.0000], device='cuda:0'), '22732': tensor([3.9721, 0.5102, 1.0695,  ..., 0.1763, 0.0000, 0.0000], device='cuda:0'), '23527': tensor([ 2.3986,  2.0178,  1.4587,  ..., -0.0338,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '23610': tensor([ 1.5854, -0.2549, -0.1105,  ...,  0.0146,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2362': tensor([2.6044, 3.1822, 3.0983,  ..., 0.0953, 0.0000, 0.0000], device='cuda:0'), '23637': tensor([3.6533, 0.5286, 0.7543,  ..., 0.0675, 0.0000, 0.0000], device='cuda:0'), '23977': tensor([ 4.0364,  0.4551,  0.9694,  ..., -0.0992,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '24018': tensor([4.1902, 0.4768, 1.8847,  ..., 0.2919, 0.0000, 0.0000], device='cuda:0'), '24134': tensor([5.0070, 0.4500, 2.5564,  ..., 0.0161, 0.0000, 0.0000], device='cuda:0'), '2456': tensor([ 6.9571, -2.3057, -1.2135,  ...,  0.0512,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '24636': tensor([5.6836, 4.9166, 0.9738,  ..., 0.0320, 0.0000, 0.0000], device='cuda:0'), '24750': tensor([ 3.2951, -1.0787,  2.7410,  ..., -0.0892,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25165': tensor([ 5.1171,  1.5629,  0.9361,  ..., -0.2034,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25415': tensor([ 2.4478,  0.1037,  0.9744,  ..., -0.0449,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25447': tensor([ 1.6223, -0.0747, -0.2615,  ..., -0.0942,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25525': tensor([ 3.4420,  0.6419,  2.7697,  ..., -0.0221,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25671': tensor([3.7699, 5.0196, 1.9640,  ..., 0.0627, 0.0000, 0.0000], device='cuda:0'), '25894': tensor([ 2.7019, -0.3878,  1.3227,  ...,  0.0763,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26201': tensor([4.8523, 1.7473, 3.8526,  ..., 0.0876, 0.0000, 0.0000], device='cuda:0'), '26690': tensor([ 4.0022,  5.0103,  0.9514,  ..., -0.0158,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26815': tensor([ 3.0119, -2.9179, -0.2219,  ...,  0.0282,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26868': tensor([-1.4908, -1.1615,  0.5418,  ...,  0.3835,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26979': tensor([ 3.0688,  0.6516,  2.0855,  ..., -0.1884,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27378': tensor([ 3.7843,  4.0319,  3.8294,  ..., -0.2252,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27404': tensor([ 0.5549, -0.7247, -1.1293,  ...,  0.1193,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27421': tensor([ 1.6301,  2.7149, -0.3492,  ..., -0.0268,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27742': tensor([ 2.8850,  4.1987,  2.2821,  ..., -0.0621,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27884': tensor([ 2.9637,  5.9713,  3.2426,  ..., -0.0367,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2800': tensor([ 2.8816e+00,  3.5547e+00,  3.2019e+00,  ..., -3.0521e-04,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '28327': tensor([3.9285, 1.1768, 1.2186,  ..., 0.0905, 0.0000, 0.0000], device='cuda:0'), '28742': tensor([ 1.9628, -0.6619, -0.6313,  ...,  0.3263,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '28889': tensor([2.5639, 0.3373, 2.1202,  ..., 0.0756, 0.0000, 0.0000], device='cuda:0'), '29092': tensor([2.8482, 3.4629, 2.2126,  ..., 0.0684, 0.0000, 0.0000], device='cuda:0'), '29886': tensor([2.9235, 5.3285, 1.0225,  ..., 0.0605, 0.0000, 0.0000], device='cuda:0'), '29954': tensor([-2.8982,  3.4002, -0.1161,  ..., -0.0245,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30126': tensor([4.6065, 0.6217, 1.4491,  ..., 0.0398, 0.0000, 0.0000], device='cuda:0'), '30128': tensor([ 2.8725,  1.3262,  2.3913,  ..., -0.0702,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30184': tensor([ 0.8319, -0.3694,  0.5321,  ...,  0.3685,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30810': tensor([4.2399, 1.1161, 0.5793,  ..., 0.0474, 0.0000, 0.0000], device='cuda:0'), '30864': tensor([4.1791, 1.6684, 0.2752,  ..., 0.1240, 0.0000, 0.0000], device='cuda:0'), '31523': tensor([3.9500, 4.5470, 2.9779,  ..., 0.0427, 0.0000, 0.0000], device='cuda:0'), '31811': tensor([ 3.0876, -0.1670,  3.2383,  ..., -0.0060,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32050': tensor([3.8398, 0.1889, 1.1030,  ..., 0.0207, 0.0000, 0.0000], device='cuda:0'), '32246': tensor([ 1.6479,  2.2763,  1.6646,  ..., -0.0137,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32260': tensor([ 2.4066, -0.4634,  0.9954,  ..., -0.0440,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32489': tensor([1.1485, 5.4836, 3.6875,  ..., 0.0858, 0.0000, 0.0000], device='cuda:0'), '32606': tensor([ 3.4955, -0.3211,  2.5009,  ..., -0.1589,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32684': tensor([ 3.5830,  4.5982,  4.7143,  ..., -0.2980,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32711': tensor([-0.5785,  4.6630,  2.0929,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32804': tensor([2.4984, 5.7234, 3.8989,  ..., 0.0294, 0.0000, 0.0000], device='cuda:0'), '3590': tensor([ 4.8593,  0.9575,  1.4337,  ..., -0.0368,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '40310': tensor([ 0.5448, -0.5174,  0.2076,  ...,  0.8636,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '40440': tensor([-3.1391e+00,  3.3111e+00,  6.2716e-02,  ..., -1.8520e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '41203': tensor([-0.2010,  2.6495, -2.0458,  ...,  0.2844,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4158': tensor([ 2.1549,  0.4830,  1.5915,  ..., -0.0024,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '41696': tensor([-2.4719,  3.7148, -0.3909,  ...,  0.0773,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '42255': tensor([-2.9380,  2.4086,  0.1275,  ...,  0.1183,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '43276': tensor([-8.3479e-01,  5.9808e+00,  2.2052e-01,  ..., -5.5369e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '43323': tensor([-2.0659,  2.8380,  0.1549,  ...,  0.0140,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '43742': tensor([-0.9482,  1.7565, -0.0366,  ...,  0.1332,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '44265': tensor([-3.4389,  1.8984, -0.5269,  ...,  0.1985,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '44851': tensor([ 2.0915, -1.0396, -0.1240,  ...,  0.9133,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4552': tensor([ 2.9731, -0.3913,  0.3997,  ..., -0.0083,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4836': tensor([ 3.0618,  4.8347,  3.3022,  ..., -0.1432,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '49139': tensor([-0.4250,  1.8164, -0.6867,  ...,  0.0134,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5021': tensor([4.2630, 1.1796, 0.4083,  ..., 0.0335, 0.0000, 0.0000], device='cuda:0'), '50417': tensor([-4.6274,  2.3038, -0.4651,  ..., -0.0552,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '50486': tensor([-1.7454,  2.8284, -0.4835,  ...,  0.0554,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '50664': tensor([-2.2097,  3.7292, -1.9190,  ..., -0.2473,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51226': tensor([-1.9932,  1.3771,  0.1823,  ...,  0.0390,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51805': tensor([-2.5755,  7.1158,  4.5629,  ...,  0.1441,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51914': tensor([ 1.0817, -0.7826,  0.4629,  ..., -0.0432,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '52010': tensor([-2.2183,  2.8052, -0.4466,  ...,  0.0327,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5205': tensor([ 4.1909,  1.3073,  1.8316,  ..., -0.4203,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5453': tensor([ 2.7680,  5.1448,  2.9389,  ..., -0.0407,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '54979': tensor([-2.5607,  6.0574,  2.2624,  ...,  0.0663,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5506': tensor([ 3.0384,  4.6712,  1.4055,  ..., -0.1041,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5544': tensor([4.5185, 0.1265, 2.9002,  ..., 0.2294, 0.0000, 0.0000], device='cuda:0'), '55639': tensor([-1.7714,  4.3228, -0.3350,  ..., -0.0335,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '55728': tensor([-3.7171,  2.5824,  0.1835,  ...,  0.0377,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '56309': tensor([2.9358, 1.5061, 0.6421,  ..., 0.0170, 0.0000, 0.0000], device='cuda:0'), '5670': tensor([3.6826, 1.5115, 2.1010,  ..., 0.0746, 0.0000, 0.0000], device='cuda:0'), '57579': tensor([-2.1949,  3.5413,  0.2187,  ...,  0.0606,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '58154': tensor([-3.0650,  0.8027, -0.3258,  ..., -0.1228,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '58757': tensor([ 0.1753, -1.9894, -0.7190,  ..., -0.2196,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59067': tensor([-2.0403,  2.8158,  0.3716,  ...,  0.6985,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59133': tensor([-1.7572,  2.9731, -0.2037,  ...,  0.1074,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59725': tensor([-1.4323,  3.8671, -1.5763,  ...,  0.0433,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '61654': tensor([-0.1125,  0.6155, -0.5477,  ...,  0.1373,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '62157': tensor([-1.0018, -1.0633,  2.6217,  ..., -0.0906,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '62186': tensor([-1.5757, -0.7536,  5.8599,  ...,  0.9183,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '63492': tensor([-3.0908,  4.7231, -2.0699,  ..., -0.0346,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '63686': tensor([-2.0899,  3.3583, -0.8176,  ...,  0.0295,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '65665': tensor([-1.4744,  0.0220,  2.0382,  ...,  0.2239,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6638': tensor([ 5.0021,  1.5169,  2.9086,  ..., -0.0067,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6686': tensor([ 5.2556,  0.1199,  0.9969,  ..., -0.0189,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6702': tensor([ 5.5944,  3.8344,  4.1577,  ..., -0.5121,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6735': tensor([2.9176, 4.2295, 1.7984,  ..., 0.0683, 0.0000, 0.0000], device='cuda:0'), '69232': tensor([-1.8802,  5.7050,  2.7983,  ...,  0.1075,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6977': tensor([-2.6302,  1.9815, -0.9851,  ..., -0.0519,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '71233': tensor([-2.8294,  3.3073, -0.2915,  ..., -0.0220,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72154': tensor([-0.5969,  2.4439, -0.1137,  ..., -0.0430,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7230': tensor([ 3.2242, -0.3937,  0.3488,  ...,  0.1502,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72931': tensor([-1.5331,  1.7055, -1.0648,  ...,  0.4048,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72942': tensor([ 3.9158,  1.3077, -0.8563,  ...,  0.0093,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7343': tensor([ 2.8699,  1.5970,  0.7792,  ..., -0.0299,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7529': tensor([ 4.2342,  0.5296,  1.1309,  ..., -0.0088,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '77046': tensor([-2.6141e+00,  1.7258e+00, -2.4619e-01,  ...,  2.2737e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '7900': tensor([3.1430, 3.9256, 4.6933,  ..., 0.1929, 0.0000, 0.0000], device='cuda:0'), '7904': tensor([4.2094, 4.2963, 4.3769,  ..., 0.0216, 0.0000, 0.0000], device='cuda:0'), '7919': tensor([0.8806, 0.7279, 1.2024,  ..., 0.5148, 0.0000, 0.0000], device='cuda:0'), '80262': tensor([ 3.0174,  0.5617, -1.3250,  ...,  0.0499,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '80342': tensor([ 2.2279, -1.4361,  0.3224,  ..., -0.2012,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '81068': tensor([-3.2153,  3.4400, -0.4063,  ..., -0.0060,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '81866': tensor([-1.7349,  3.7383, -1.3181,  ...,  0.0236,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '819': tensor([3.5189, 0.9145, 1.5460,  ..., 0.0151, 0.0000, 0.0000], device='cuda:0'), '82393': tensor([-3.1009,  3.1278, -0.1004,  ...,  0.1783,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '82469': tensor([-0.7850,  3.6829, -1.1029,  ...,  0.0250,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83691': tensor([-2.8123,  2.8351, -0.8451,  ...,  0.1913,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83773': tensor([-0.8334,  3.5582, -1.8846,  ...,  0.0187,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83982': tensor([ 1.4270, -0.6556, -0.2883,  ..., -0.6180,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '8549': tensor([ 3.3002,  4.5928,  2.6465,  ..., -0.0206,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '86006': tensor([-1.9847,  2.8347, -1.2519,  ..., -0.0525,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '86702': tensor([-0.1583,  3.7829, -2.3965,  ...,  0.2357,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '87572': tensor([-2.1732,  3.2132,  0.1725,  ...,  0.1653,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '89840': tensor([-2.5655,  3.9363,  0.5281,  ...,  0.2233,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '91633': tensor([-3.1653,  4.3211,  0.2646,  ...,  0.0341,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '93528': tensor([-0.6939,  1.1882,  2.3405,  ...,  0.0809,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '94415': tensor([-1.4016,  3.6618, -1.2995,  ...,  0.0933,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '9444': tensor([ 4.9233, -1.7062,  1.4326,  ...,  0.0887,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '94617': tensor([-2.7639,  1.8104, -0.8737,  ...,  0.0426,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '9467': tensor([ 4.2338,  2.5515,  2.0898,  ..., -0.1350,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '975': tensor([-3.5765, -1.3259,  4.3683,  ..., -0.0769,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '99865': tensor([-0.4875,  1.1588, -0.3823,  ...,  0.1548,  0.0000,  0.0000],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(log_signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiJE3_iL-kk5"
   },
   "source": [
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.877945Z",
     "iopub.status.busy": "2023-08-27T15:50:54.877545Z",
     "iopub.status.idle": "2023-08-27T15:50:54.888896Z",
     "shell.execute_reply": "2023-08-27T15:50:54.887365Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.877912Z"
    },
    "id": "XZaS5YUq-kk6",
    "outputId": "470eeea0-678e-4f82-8b87-e5ab52c4a3f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 5153)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>5142</th>\n",
       "      <th>5143</th>\n",
       "      <th>5144</th>\n",
       "      <th>5145</th>\n",
       "      <th>5146</th>\n",
       "      <th>5147</th>\n",
       "      <th>5148</th>\n",
       "      <th>5149</th>\n",
       "      <th>5150</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>1.580732</td>\n",
       "      <td>4.186687</td>\n",
       "      <td>3.028702</td>\n",
       "      <td>-2.008340</td>\n",
       "      <td>-0.220417</td>\n",
       "      <td>2.378833</td>\n",
       "      <td>-1.820687</td>\n",
       "      <td>-2.836124</td>\n",
       "      <td>2.407619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034118</td>\n",
       "      <td>0.024424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037972</td>\n",
       "      <td>-0.091475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10286</td>\n",
       "      <td>2.531790</td>\n",
       "      <td>-0.251507</td>\n",
       "      <td>1.891476</td>\n",
       "      <td>-4.173205</td>\n",
       "      <td>0.223488</td>\n",
       "      <td>-0.381999</td>\n",
       "      <td>-1.663747</td>\n",
       "      <td>-2.654674</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250363</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.087456</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10532</td>\n",
       "      <td>-0.402976</td>\n",
       "      <td>3.509747</td>\n",
       "      <td>-0.532292</td>\n",
       "      <td>-0.727622</td>\n",
       "      <td>-0.759668</td>\n",
       "      <td>1.675333</td>\n",
       "      <td>0.747812</td>\n",
       "      <td>-0.011958</td>\n",
       "      <td>-1.794389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>0.043533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11271</td>\n",
       "      <td>3.632075</td>\n",
       "      <td>0.293749</td>\n",
       "      <td>-0.742413</td>\n",
       "      <td>-1.989862</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.957444</td>\n",
       "      <td>1.632290</td>\n",
       "      <td>2.513831</td>\n",
       "      <td>-2.261577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038107</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>0.049178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1136</td>\n",
       "      <td>3.383447</td>\n",
       "      <td>0.753994</td>\n",
       "      <td>0.660684</td>\n",
       "      <td>-3.736158</td>\n",
       "      <td>-0.761842</td>\n",
       "      <td>-0.661879</td>\n",
       "      <td>-0.313948</td>\n",
       "      <td>-2.516666</td>\n",
       "      <td>-1.984024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121096</td>\n",
       "      <td>-0.043274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>-0.037597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.032573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SUBJECT_ID         0         1         2         3         4         5  \\\n",
       "0      10000  1.580732  4.186687  3.028702 -2.008340 -0.220417  2.378833   \n",
       "1      10286  2.531790 -0.251507  1.891476 -4.173205  0.223488 -0.381999   \n",
       "2      10532 -0.402976  3.509747 -0.532292 -0.727622 -0.759668  1.675333   \n",
       "3      11271  3.632075  0.293749 -0.742413 -1.989862  0.867802  0.957444   \n",
       "4       1136  3.383447  0.753994  0.660684 -3.736158 -0.761842 -0.661879   \n",
       "\n",
       "          6         7         8  ...      5142      5143  5144      5145  \\\n",
       "0 -1.820687 -2.836124  2.407619  ... -0.034118  0.024424   0.0  0.037972   \n",
       "1 -1.663747 -2.654674  0.047086  ...  0.250363  0.162344   0.0 -0.087456   \n",
       "2  0.747812 -0.011958 -1.794389  ...  0.008459  0.020197   0.0 -0.025810   \n",
       "3  1.632290  2.513831 -2.261577  ... -0.038107  0.008433   0.0 -0.030156   \n",
       "4 -0.313948 -2.516666 -1.984024  ...  0.121096 -0.043274   0.0 -0.042306   \n",
       "\n",
       "       5146  5147      5148  5149  5150  HOSPITAL_EXPIRE_FLAG  \n",
       "0 -0.091475   0.0 -0.009156   0.0   0.0                     0  \n",
       "1  0.114900   0.0  0.034347   0.0   0.0                     0  \n",
       "2  0.043533   0.0  0.043025   0.0   0.0                     0  \n",
       "3  0.049178   0.0  0.054637   0.0   0.0                     1  \n",
       "4 -0.037597   0.0 -0.032573   0.0   0.0                     0  \n",
       "\n",
       "[5 rows x 5153 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXfOmn2w-kk6"
   },
   "source": [
    "# Jeux de données test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7D4ONXb-kk6"
   },
   "source": [
    "Cette partie prépare un ensemble de données distinct pour les tests en choisissant au hasard un groupe de patients avec des étiquettes équilibrées de décès à l'hôpital. Ces données seront utilisées pour évaluer les performances du modèle sur de nouvelles observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI4KlKLo-kk6"
   },
   "source": [
    "## [TEST] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_bJh3DE-kk6"
   },
   "source": [
    "Dans cette partie, le code charge le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients dans l'ensemble de données de test. Cela permet d'avoir accès au modèle pré-entraîné afin de traiter les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.891762Z",
     "iopub.status.busy": "2023-08-27T15:50:54.891334Z",
     "iopub.status.idle": "2023-08-27T15:50:57.431020Z",
     "shell.execute_reply": "2023-08-27T15:50:57.429444Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.891728Z"
    },
    "id": "ytlaOQLJ-kk6",
    "outputId": "9964e28b-6a66-47e5-e26a-e7e73547bf55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle pré-entraîné Bio_ClinicalBERT et son tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Filtrer les données test (new_data) pour ne conserver que les patients absents dans le dictionnaire des embeddings (flattened_embeddings_dict)\n",
    "new_data_test = new_data[~new_data[\"SUBJECT_ID\"].isin(flattened_embeddings_dict.keys())]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1.sample(n=15, random_state=56), label_0.sample(n=85, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"SUBJECT_ID\"].isin(sample_test[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kQsnSe8-kk7"
   },
   "source": [
    "## [TEST] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV77RZCc-kk7"
   },
   "source": [
    "Cette partie du code se focalise sur l'extraction des embeddings ClinicalBERT à partir du texte des patients présents dans le jeu de données de test. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Les embeddings de chaque patient sont sauvegardés pour une utilisation ultérieure lors de l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:35:17.753028Z",
     "iopub.status.busy": "2023-08-09T12:35:17.752625Z",
     "iopub.status.idle": "2023-08-09T15:35:47.773903Z",
     "shell.execute_reply": "2023-08-09T15:35:47.771299Z",
     "shell.execute_reply.started": "2023-08-09T12:35:17.752990Z"
    },
    "id": "ReL_-53L-kk7"
   },
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample_test.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Créer une liste pour stocker les valeurs de 'TIME' de chaque partie d'un document\n",
    "time_list_test = []\n",
    "\n",
    "# Créer un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict_test = {}\n",
    "\n",
    "# Parcourir les patients et leurs données textuelles\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer les documents textuels\n",
    "    times = values['TIME']  # Récupérer les valeurs de 'TIME' associées\n",
    "\n",
    "    embeddings_list = []  # Stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "\n",
    "        # Diviser le texte en parties de longueur appropriée\n",
    "        encoded_text = tokenizer.encode(text)\n",
    "        n_tokens = len(encoded_text)\n",
    "        n_chunks = max(1, n_tokens // 512)\n",
    "        windows = split_text(text, n_chunks)\n",
    "\n",
    "        cls_embeddings_list = []  # Stocker les embeddings CLS de chaque partie\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for window in windows:\n",
    "            # Convertir la fenêtre en un format compatible avec le modèle\n",
    "            inputs = tokenizer(window, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence sur le modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque fenêtre\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "\n",
    "            # Ajouter la valeur 'TIME' correspondante à la liste\n",
    "            time_list_test.append(time)\n",
    "\n",
    "        # Moyenne des embeddings CLS pour chaque fenêtre\n",
    "        #avg_cls_embedding = torch.mean(torch.stack(cls_embeddings_list), dim=0)\n",
    "        #embeddings_list.append(avg_cls_embedding)\n",
    "        # Ajouter la liste des embeddings CLS à la liste des embeddings pour ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans le dictionnaire avec le n° du patient comme clé\n",
    "    embeddings_dict_test[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS0t1T5G-kk8"
   },
   "source": [
    "## [TEST] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HDmvGSq-kk8"
   },
   "source": [
    "### [TEST] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyIj1Zak-kk8"
   },
   "source": [
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:57.434945Z",
     "iopub.status.busy": "2023-08-27T15:50:57.434509Z",
     "iopub.status.idle": "2023-08-27T15:50:58.953013Z",
     "shell.execute_reply": "2023-08-27T15:50:58.951571Z",
     "shell.execute_reply.started": "2023-08-27T15:50:57.434911Z"
    },
    "id": "eKOGB5UY-kk8",
    "outputId": "66191fbd-a498-45a8-d3f8-8f1a11663b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 10289, Embedding shape: (21, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe GaussianRandomProjection de la bibliothèque scikit-learn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque sujet\n",
    "lengths_test = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Appliquer la projection gaussienne aléatoire sur les embeddings\n",
    "reduced_embeddings_np_test = transformer.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths_test)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 101)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 101)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKwKl1V-kk8"
   },
   "source": [
    "### [TEST] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEl9Ashr-kk8"
   },
   "source": [
    "La technique de l'Analyse en Composantes Principales (ACP) est utilisée pour réduire la dimensionnalité des plongements dans le jeu de données de test, tout en maintenant les informations pertinentes. De plus, cette méthode permet d'afficher la variance expliquée par chaque composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-27T14:35:06.528046Z",
     "iopub.status.busy": "2023-08-27T14:35:06.527090Z",
     "iopub.status.idle": "2023-08-27T14:35:10.238543Z",
     "shell.execute_reply": "2023-08-27T14:35:10.237285Z",
     "shell.execute_reply.started": "2023-08-27T14:35:06.528009Z"
    },
    "id": "Ptm55O3q-kk9",
    "outputId": "28717a71-974b-4b8c-e5e5-87a83a4c65d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component: [0.17085595 0.12655854 0.07180104 0.05270758 0.04825578 0.04018151\n",
      " 0.03466578 0.02513221 0.02216924 0.02000224 0.01804894 0.01541976\n",
      " 0.01398654 0.01324861 0.0114102  0.01096654 0.01021872 0.00904915\n",
      " 0.00886639 0.00855818 0.00707977 0.0069404  0.00655178 0.00637795\n",
      " 0.00564005 0.00540953 0.00507004 0.00493546 0.00477063 0.00450308\n",
      " 0.00421699 0.00401662 0.0039828  0.00371767 0.00362881 0.0034303\n",
      " 0.00341119 0.00304949 0.00292737 0.00276166 0.00267746 0.00260731\n",
      " 0.00258352 0.00247502 0.00236587 0.00228805 0.00218283 0.0021599\n",
      " 0.00211235 0.00209489 0.00198502 0.00196362 0.00186125 0.00185165\n",
      " 0.00180456 0.00174533 0.00166968 0.00161585 0.00156355 0.00154717\n",
      " 0.00153203 0.00145485 0.00142228 0.00140095 0.00136783 0.00135508\n",
      " 0.00133627 0.00131552 0.00126198 0.00125601 0.00123491 0.00120362\n",
      " 0.00118936 0.00115279 0.00113946 0.0011192  0.00108613 0.00107388\n",
      " 0.00104935 0.001023   0.00101481 0.00101169 0.00099267 0.00096795\n",
      " 0.00092423 0.00091578 0.00089908 0.00088673 0.00087788 0.00086313\n",
      " 0.0008411  0.00082344 0.00081373 0.00079497 0.00077749 0.00076954\n",
      " 0.00074712 0.00074211 0.00072948 0.00070163]\n",
      "Total variance explained: 0.9077483667060733\n",
      "Subject ID: 10289, Embedding shape: (21, 101)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice numpy\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser l'analyse en composantes principales (ACP) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle ACP sur les données et les transformer\n",
    "reduced_embeddings_np_test = pca.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 100)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    # Créer un tableau vide pour stocker les embeddings et le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        # Concaténer l'embedding réduit avec la valeur 'time'\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETKJU6AP-kk9"
   },
   "source": [
    "## [TEST] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGvVJ66Q-kk9"
   },
   "source": [
    "Ici, le programme calcule les signatures logarithmiques pour les embeddings des données de test. Ces signatures contiennent des informations plus avancées qui peuvent être bénéfiques pour l'entraînement du modèle. Le résultat est un dictionnaire où les embeddings de chaque patient sont représentés sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:58.954909Z",
     "iopub.status.busy": "2023-08-27T15:50:58.954544Z",
     "iopub.status.idle": "2023-08-27T15:50:59.778955Z",
     "shell.execute_reply": "2023-08-27T15:50:59.777505Z",
     "shell.execute_reply.started": "2023-08-27T15:50:58.954877Z"
    },
    "id": "fcqBX74m-kk9"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronqué\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict_test = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits pour chaque patient\n",
    "for key, value in reduced_embeddings_dict_test.items():\n",
    "    # Convertir le numpy array en un torch tensor et spécifier le type comme float\n",
    "    tensor = torch.from_numpy(value).float()\n",
    "    # Alternative : utiliser directement torch.tensor(value, dtype=torch.float)\n",
    "\n",
    "    # Ajouter une dimension supplémentaire pour correspondre à l'exigence du modèle (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Calculer la log signature en utilisant le modèle signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension batch que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0)\n",
    "\n",
    "    # Ajouter le résultat au dictionnaire avec le numéro du patient comme clé\n",
    "    log_signature_dict_test[key] = log_signature\n",
    "\n",
    "# Maintenant, log_signature_dict est un dictionnaire où chaque clé correspond à un patient et chaque valeur est la log signature de ce sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgoeroMB-kk-"
   },
   "source": [
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:09:07.874069Z",
     "iopub.status.busy": "2023-08-27T16:09:07.873428Z",
     "iopub.status.idle": "2023-08-27T16:09:07.883912Z",
     "shell.execute_reply": "2023-08-27T16:09:07.882548Z",
     "shell.execute_reply.started": "2023-08-27T16:09:07.874025Z"
    },
    "id": "b74xdF2K-kk-",
    "outputId": "a38e34db-3786-4170-ea4c-4ddb26ba7da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5153"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 1 pour la colonne 'HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "# Reg Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VFYF2nn-kk-"
   },
   "source": [
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 65.00%\n",
      "Rappel: 66.67%\n",
      "F1-score: 36.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAFUB5vy-kk_"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGeOedqw-kk_"
   },
   "source": [
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T21:31:54.195444Z",
     "iopub.status.busy": "2023-08-26T21:31:54.194905Z",
     "iopub.status.idle": "2023-08-26T21:34:30.027397Z",
     "shell.execute_reply": "2023-08-26T21:34:30.025943Z",
     "shell.execute_reply.started": "2023-08-26T21:31:54.195399Z"
    },
    "id": "AYUwBEIa-kk_",
    "outputId": "8ba3b6c2-e274-4acd-d3a0-899e27ed97d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 48.00%\n",
      "Rappel: 60.00%\n",
      "F1-score: 25.71%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "best_model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB3vSRvc-kk_"
   },
   "source": [
    "# Sauvegarder les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KdnmRvR-kk_"
   },
   "source": [
    "Nous procédons à une sauvegarde des données en enregistrant les plongements traités ainsi que les valeurs temporelles dans des fichiers JSON et Pickle, afin de pouvoir les utiliser ou les analyser ultérieurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:12:12.863066Z",
     "iopub.status.busy": "2023-08-09T18:12:12.861870Z",
     "iopub.status.idle": "2023-08-09T18:12:37.434603Z",
     "shell.execute_reply": "2023-08-09T18:12:37.432844Z",
     "shell.execute_reply.started": "2023-08-09T18:12:12.863011Z"
    },
    "id": "5oTPx7uX-kk_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis convertis en listes\n",
    "flattened_embeddings_dict_converted = {}\n",
    "\n",
    "# Parcourir le dictionnaire flattened_embeddings_dict_test\n",
    "for clé, valeur in flattened_embeddings_dict_test.items():\n",
    "    # Convertir chaque valeur (numpy array) en liste et stocker dans le nouveau dictionnaire\n",
    "    flattened_embeddings_dict_converted[clé] = valeur.tolist()\n",
    "\n",
    "# Sauvegarder le dictionnaire converti en tant que fichier JSON\n",
    "\n",
    "# Spécifier le chemin du fichier JSON où vous souhaitez enregistrer les données\n",
    "with open('embeddings_dict_test.json', 'w') as fichier:\n",
    "    # Utiliser la bibliothèque JSON pour écrire le dictionnaire converti dans le fichier JSON\n",
    "    json.dump(flattened_embeddings_dict_converted, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:13:16.081552Z",
     "iopub.status.busy": "2023-08-09T18:13:16.081068Z",
     "iopub.status.idle": "2023-08-09T18:13:16.091682Z",
     "shell.execute_reply": "2023-08-09T18:13:16.090132Z",
     "shell.execute_reply.started": "2023-08-09T18:13:16.081516Z"
    },
    "id": "OZs6btSi-klA"
   },
   "outputs": [],
   "source": [
    "# Importer le module pour la serialization des objets\n",
    "import pickle\n",
    "\n",
    "# Sauvegarder la liste dans un fichier binaire (pickle)\n",
    "with open('time_list_test.pkl', 'wb') as f:\n",
    "    # Utiliser la bibliothèque pickle pour écrire la liste dans le fichier binaire\n",
    "    pickle.dump(time_list_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "id": "TLL-YYePVIg6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (7.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6; python_version < \"3.10\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (6.7.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.7.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (4.12.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (4.12.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (6.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (3.1.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (5.8.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (2.1.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (2.17.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (5.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (23.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.2.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=3.6; python_version < \"3.10\"->nbconvert) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=3.6; python_version < \"3.10\"->nbconvert) (4.7.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (7.4.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.4.1)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (306)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbformat>=5.7->nbconvert) (2.19.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbformat>=5.7->nbconvert) (4.17.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.5.8)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (0.4)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.19.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (5.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (23.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nbconvert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAxM8z23VLKT",
    "outputId": "86d1a041-91f5-4e80-c4fa-6794021f39a5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1309005235.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_25592\\1309005235.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    jupyter nbconvert \"troisieme-rapport.ipynb\" --to html\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#jupyter nbconvert \"troisieme-rapport.ipynb\" --to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85uj_x33VV4A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
