{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Understanding the trajectory of text data in cancer studies\n",
    "## Troisieme rapport\n",
    "\n",
    "Dmytro Zhovtobriukh\n",
    "\\\n",
    "Anna Novototskykh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10 (v3.8.10:3d8993a744, May  3 2021, 09:09:08) \\n[Clang 12.0.5 (clang-1205.0.22.9)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu111\n",
      "Requirement already satisfied: torch==1.9.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: torchvision==0.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: torchaudio==0.9.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch==1.9.0) (4.9.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchvision==0.10.0) (10.2.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchvision==0.10.0) (1.24.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.1 --extra-index-url https://download.pytorch.org/whl/cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting signatory==1.2.6.1.9.0\n",
      "  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 5.8 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: signatory\n",
      "  Building wheel for signatory (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /usr/local/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-wheel-t0a06wi3\n",
      "       cwd: /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/\n",
      "  Complete output (108 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-11-universal2-3.8\n",
      "  creating build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/signature_inversion_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/logsignature_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/signature_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/unstable.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/utility.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/deprecated.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/__init__.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/augment.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/impl.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  copying src/signatory/path.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "  running build_ext\n",
      "  building '_impl' extension\n",
      "  creating /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8\n",
      "  creating /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src\n",
      "  Emitting ninja build file /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/tensor_algebra_ops.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/tensor_algebra_ops.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  [2/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/logsignature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/logsignature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  [3/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/lyndon.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/lyndon.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  [4/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/pytorchbind.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/pytorchbind.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  [5/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/signature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/signature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  [6/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/misc.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o\n",
      "  c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/misc.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  ninja: build stopped: subcommand failed.\n",
      "  Traceback (most recent call last):\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1666, in _run_ninja_build\n",
      "      subprocess.run(\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 516, in run\n",
      "      raise CalledProcessError(retcode, process.args,\n",
      "  subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py\", line 53, in <module>\n",
      "      setuptools.setup(name=metadata.project,\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "      self.run_command(\"build\")\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 79, in run\n",
      "      _build_ext.run(self)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 340, in run\n",
      "      self.build_extensions()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 709, in build_extensions\n",
      "      build_ext.build_extensions(self)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 449, in build_extensions\n",
      "      self._build_extensions_serial()\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 474, in _build_extensions_serial\n",
      "      self.build_extension(ext)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 196, in build_extension\n",
      "      _build_ext.build_extension(self, ext)\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 528, in build_extension\n",
      "      objects = self.compiler.compile(sources,\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 530, in unix_wrap_ninja_compile\n",
      "      _write_ninja_file_and_compile_objects(\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1355, in _write_ninja_file_and_compile_objects\n",
      "      _run_ninja_build(\n",
      "    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1682, in _run_ninja_build\n",
      "      raise RuntimeError(message) from e\n",
      "  RuntimeError: Error compiling objects for extension\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for signatory\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for signatory\n",
      "Failed to build signatory\n",
      "Installing collected packages: signatory\n",
      "    Running setup.py install for signatory ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /usr/local/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-record-c0jfthu7/install-record.txt --single-version-externally-managed --compile --install-headers /Library/Frameworks/Python.framework/Versions/3.8/include/python3.8/signatory\n",
      "         cwd: /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/\n",
      "    Complete output (110 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-11-universal2-3.8\n",
      "    creating build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/signature_inversion_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/logsignature_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/signature_module.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/unstable.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/utility.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/deprecated.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/__init__.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/augment.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/impl.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    copying src/signatory/path.py -> build/lib.macosx-11-universal2-3.8/signatory\n",
      "    running build_ext\n",
      "    building '_impl' extension\n",
      "    creating /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8\n",
      "    creating /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src\n",
      "    Emitting ninja build file /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/logsignature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/logsignature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/logsignature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    [2/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/lyndon.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/lyndon.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/lyndon.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    [3/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/signature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/signature.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/signature.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    [4/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/misc.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/misc.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/misc.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    [5/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/pytorchbind.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/pytorchbind.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/pytorchbind.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    [6/6] c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/tensor_algebra_ops.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    \u001b[31mFAILED: \u001b[0m/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o\n",
      "    c++ -MMD -MF /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o.d -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/TH -I/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/include/THC -I/Library/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c -c /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/src/tensor_algebra_ops.cpp -o /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/build/temp.macosx-11-universal2-3.8/src/tensor_algebra_ops.o -fvisibility=hidden -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_clang\"' '-DPYBIND11_STDLIB=\"_libcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1002\"' -DTORCH_EXTENSION_NAME=_impl -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    ninja: build stopped: subcommand failed.\n",
      "    Traceback (most recent call last):\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1666, in _run_ninja_build\n",
      "        subprocess.run(\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 516, in run\n",
      "        raise CalledProcessError(retcode, process.args,\n",
      "    subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "    \n",
      "    The above exception was the direct cause of the following exception:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py\", line 53, in <module>\n",
      "        setuptools.setup(name=metadata.project,\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/command/install.py\", line 61, in run\n",
      "        return orig.install.run(self)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/install.py\", line 545, in run\n",
      "        self.run_command('build')\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "        self.run_command(cmd_name)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 79, in run\n",
      "        _build_ext.run(self)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 340, in run\n",
      "        self.build_extensions()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 709, in build_extensions\n",
      "        build_ext.build_extensions(self)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 449, in build_extensions\n",
      "        self._build_extensions_serial()\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 474, in _build_extensions_serial\n",
      "        self.build_extension(ext)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 196, in build_extension\n",
      "        _build_ext.build_extension(self, ext)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/distutils/command/build_ext.py\", line 528, in build_extension\n",
      "        objects = self.compiler.compile(sources,\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 530, in unix_wrap_ninja_compile\n",
      "        _write_ninja_file_and_compile_objects(\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1355, in _write_ninja_file_and_compile_objects\n",
      "        _run_ninja_build(\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1682, in _run_ninja_build\n",
      "        raise RuntimeError(message) from e\n",
      "    RuntimeError: Error compiling objects for extension\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /usr/local/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-install-qabykiil/signatory_b74920fe26424d09b5c8a4396eaea6de/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/pip-record-c0jfthu7/install-record.txt --single-version-externally-managed --compile --install-headers /Library/Frameworks/Python.framework/Versions/3.8/include/python3.8/signatory Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.38.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/stuart/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stuart/Library/Python/3.8/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stuart/Library/Python/3.8/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 files\n",
      "DATETIMEEVENTS.csv.gz\n",
      "CPTEVENTS.csv.gz\n",
      "PATIENTS.csv.gz\n",
      "DIAGNOSES_ICD.csv.gz\n",
      "CAREGIVERS.csv.gz\n",
      "PRESCRIPTIONS.csv.gz\n",
      "INPUTEVENTS_MV.csv.gz\n",
      "DRGCODES.csv.gz\n",
      "D_ICD_DIAGNOSES.csv.gz\n",
      "D_LABITEMS.csv.gz\n",
      "TRANSFERS.csv.gz\n",
      "ADMISSIONS.csv.gz\n",
      "D_ITEMS.csv.gz\n",
      "CALLOUT.csv.gz\n",
      "D_CPT.csv.gz\n",
      "LABEVENTS.csv.gz\n",
      "PROCEDURES_ICD.csv.gz\n",
      "CHARTEVENTS.csv.gz\n",
      "SERVICES.csv.gz\n",
      "D_ICD_PROCEDURES.csv.gz\n",
      "ICUSTAYS.csv.gz\n",
      "INPUTEVENTS_CV.csv.gz\n",
      "PROCEDUREEVENTS_MV.csv.gz\n",
      "NOTEEVENTS.csv.gz\n",
      "OUTPUTEVENTS.csv.gz\n",
      "MICROBIOLOGYEVENTS.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"# Répertoire actuel\n",
    "current_directory = os.getcwd()\n",
    "print(\"Vous êtes dans le répertoire:\", current_directory)\n",
    "\n",
    "# Accéder au répertoire parent (A)\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Changer de répertoire de travail vers le répertoire parent\n",
    "os.chdir(parent_directory)\n",
    "# Vérifier le nouveau répertoire\n",
    "new_directory = os.getcwd()\n",
    "print(\"Vous êtes maintenant dans le répertoire:\", new_directory)\"\"\"\n",
    "\n",
    "# define whether to use demo dataset or the original\n",
    "USE_DEMO = False\n",
    "\n",
    "if USE_DEMO:\n",
    "    pth = 'C:\\\\Users\\\\pc\\\\Documents\\\\Recherche\\\\physionet.org\\\\files\\\\mimiciii-demo\\\\1.4'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv')]\n",
    "else:\n",
    "    pth = 'data/mimiciii'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid `ParserError: Error tokenizing data. C error: out of memory`\n",
    "\n",
    "https://stackoverflow.com/questions/41303246/error-tokenizing-data-c-error-out-of-memory-pandas-python-large-file-csv\n",
    "\n",
    "```python\n",
    "mylist = []\n",
    "for chunk in  pd.read_csv('train.csv', chunksize=20000):\n",
    "    mylist.append(chunk)\n",
    "big_data = pd.concat(mylist, axis= 0)\n",
    "del mylist\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "big_data = pd.read_csv('train.csv', engine='python')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================DATETIMEEVENTS.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CGID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "WARNING\n",
      "ERROR\n",
      "RESULTSTATUS\n",
      "STOPPED\n",
      "================================CPTEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "COSTCENTER\n",
      "CHARTDATE\n",
      "CPT_CD\n",
      "CPT_NUMBER\n",
      "CPT_SUFFIX\n",
      "TICKET_ID_SEQ\n",
      "SECTIONHEADER\n",
      "SUBSECTIONHEADER\n",
      "DESCRIPTION\n",
      "================================PATIENTS.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "GENDER\n",
      "DOB\n",
      "DOD\n",
      "DOD_HOSP\n",
      "DOD_SSN\n",
      "EXPIRE_FLAG\n",
      "==============================DIAGNOSES_ICD.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SEQ_NUM\n",
      "ICD9_CODE\n",
      "===============================CAREGIVERS.csv.gz================================\n",
      "ROW_ID\n",
      "CGID\n",
      "LABEL\n",
      "DESCRIPTION\n",
      "==============================PRESCRIPTIONS.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTDATE\n",
      "ENDDATE\n",
      "DRUG_TYPE\n",
      "DRUG\n",
      "DRUG_NAME_POE\n",
      "DRUG_NAME_GENERIC\n",
      "FORMULARY_DRUG_CD\n",
      "GSN\n",
      "NDC\n",
      "PROD_STRENGTH\n",
      "DOSE_VAL_RX\n",
      "DOSE_UNIT_RX\n",
      "FORM_VAL_DISP\n",
      "FORM_UNIT_DISP\n",
      "ROUTE\n",
      "=============================INPUTEVENTS_MV.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTTIME\n",
      "ENDTIME\n",
      "ITEMID\n",
      "AMOUNT\n",
      "AMOUNTUOM\n",
      "RATE\n",
      "RATEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "ORDERCATEGORYNAME\n",
      "SECONDARYORDERCATEGORYNAME\n",
      "ORDERCOMPONENTTYPEDESCRIPTION\n",
      "ORDERCATEGORYDESCRIPTION\n",
      "PATIENTWEIGHT\n",
      "TOTALAMOUNT\n",
      "TOTALAMOUNTUOM\n",
      "ISOPENBAG\n",
      "CONTINUEINNEXTDEPT\n",
      "CANCELREASON\n",
      "STATUSDESCRIPTION\n",
      "COMMENTS_EDITEDBY\n",
      "COMMENTS_CANCELEDBY\n",
      "COMMENTS_DATE\n",
      "ORIGINALAMOUNT\n",
      "ORIGINALRATE\n",
      "================================DRGCODES.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "DRG_TYPE\n",
      "DRG_CODE\n",
      "DESCRIPTION\n",
      "DRG_SEVERITY\n",
      "DRG_MORTALITY\n",
      "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
      "ROW_ID\n",
      "ICD9_CODE\n",
      "SHORT_TITLE\n",
      "LONG_TITLE\n",
      "===============================D_LABITEMS.csv.gz================================\n",
      "ROW_ID\n",
      "ITEMID\n",
      "LABEL\n",
      "FLUID\n",
      "CATEGORY\n",
      "LOINC_CODE\n",
      "================================TRANSFERS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "DBSOURCE\n",
      "EVENTTYPE\n",
      "PREV_CAREUNIT\n",
      "CURR_CAREUNIT\n",
      "PREV_WARDID\n",
      "CURR_WARDID\n",
      "INTIME\n",
      "OUTTIME\n",
      "LOS\n",
      "===============================ADMISSIONS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ADMITTIME\n",
      "DISCHTIME\n",
      "DEATHTIME\n",
      "ADMISSION_TYPE\n",
      "ADMISSION_LOCATION\n",
      "DISCHARGE_LOCATION\n",
      "INSURANCE\n",
      "LANGUAGE\n",
      "RELIGION\n",
      "MARITAL_STATUS\n",
      "ETHNICITY\n",
      "EDREGTIME\n",
      "EDOUTTIME\n",
      "DIAGNOSIS\n",
      "HOSPITAL_EXPIRE_FLAG\n",
      "HAS_CHARTEVENTS_DATA\n",
      "=================================D_ITEMS.csv.gz=================================\n",
      "ROW_ID\n",
      "ITEMID\n",
      "LABEL\n",
      "ABBREVIATION\n",
      "DBSOURCE\n",
      "LINKSTO\n",
      "CATEGORY\n",
      "UNITNAME\n",
      "PARAM_TYPE\n",
      "CONCEPTID\n",
      "=================================CALLOUT.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SUBMIT_WARDID\n",
      "SUBMIT_CAREUNIT\n",
      "CURR_WARDID\n",
      "CURR_CAREUNIT\n",
      "CALLOUT_WARDID\n",
      "CALLOUT_SERVICE\n",
      "REQUEST_TELE\n",
      "REQUEST_RESP\n",
      "REQUEST_CDIFF\n",
      "REQUEST_MRSA\n",
      "REQUEST_VRE\n",
      "CALLOUT_STATUS\n",
      "CALLOUT_OUTCOME\n",
      "DISCHARGE_WARDID\n",
      "ACKNOWLEDGE_STATUS\n",
      "CREATETIME\n",
      "UPDATETIME\n",
      "ACKNOWLEDGETIME\n",
      "OUTCOMETIME\n",
      "FIRSTRESERVATIONTIME\n",
      "CURRENTRESERVATIONTIME\n",
      "==================================D_CPT.csv.gz==================================\n",
      "ROW_ID\n",
      "CATEGORY\n",
      "SECTIONRANGE\n",
      "SECTIONHEADER\n",
      "SUBSECTIONRANGE\n",
      "SUBSECTIONHEADER\n",
      "CODESUFFIX\n",
      "MINCODEINSUBSECTION\n",
      "MAXCODEINSUBSECTION\n",
      "================================LABEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "VALUE\n",
      "VALUENUM\n",
      "VALUEUOM\n",
      "FLAG\n",
      "=============================PROCEDURES_ICD.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "SEQ_NUM\n",
      "ICD9_CODE\n",
      "===============================CHARTEVENTS.csv.gz===============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "ITEMID\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CGID\n",
      "VALUE\n",
      "VALUENUM\n",
      "VALUEUOM\n",
      "WARNING\n",
      "ERROR\n",
      "RESULTSTATUS\n",
      "STOPPED\n",
      "================================SERVICES.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "TRANSFERTIME\n",
      "PREV_SERVICE\n",
      "CURR_SERVICE\n",
      "============================D_ICD_PROCEDURES.csv.gz=============================\n",
      "ROW_ID\n",
      "ICD9_CODE\n",
      "SHORT_TITLE\n",
      "LONG_TITLE\n",
      "================================ICUSTAYS.csv.gz=================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "DBSOURCE\n",
      "FIRST_CAREUNIT\n",
      "LAST_CAREUNIT\n",
      "FIRST_WARDID\n",
      "LAST_WARDID\n",
      "INTIME\n",
      "OUTTIME\n",
      "LOS\n",
      "=============================INPUTEVENTS_CV.csv.gz==============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "CHARTTIME\n",
      "ITEMID\n",
      "AMOUNT\n",
      "AMOUNTUOM\n",
      "RATE\n",
      "RATEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "STOPPED\n",
      "NEWBOTTLE\n",
      "ORIGINALAMOUNT\n",
      "ORIGINALAMOUNTUOM\n",
      "ORIGINALROUTE\n",
      "ORIGINALRATE\n",
      "ORIGINALRATEUOM\n",
      "ORIGINALSITE\n",
      "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "STARTTIME\n",
      "ENDTIME\n",
      "ITEMID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "LOCATION\n",
      "LOCATIONCATEGORY\n",
      "STORETIME\n",
      "CGID\n",
      "ORDERID\n",
      "LINKORDERID\n",
      "ORDERCATEGORYNAME\n",
      "SECONDARYORDERCATEGORYNAME\n",
      "ORDERCATEGORYDESCRIPTION\n",
      "ISOPENBAG\n",
      "CONTINUEINNEXTDEPT\n",
      "CANCELREASON\n",
      "STATUSDESCRIPTION\n",
      "COMMENTS_EDITEDBY\n",
      "COMMENTS_CANCELEDBY\n",
      "COMMENTS_DATE\n",
      "===============================NOTEEVENTS.csv.gz================================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "CHARTDATE\n",
      "CHARTTIME\n",
      "STORETIME\n",
      "CATEGORY\n",
      "DESCRIPTION\n",
      "CGID\n",
      "ISERROR\n",
      "TEXT\n",
      "==============================OUTPUTEVENTS.csv.gz===============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "ICUSTAY_ID\n",
      "CHARTTIME\n",
      "ITEMID\n",
      "VALUE\n",
      "VALUEUOM\n",
      "STORETIME\n",
      "CGID\n",
      "STOPPED\n",
      "NEWBOTTLE\n",
      "ISERROR\n",
      "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
      "ROW_ID\n",
      "SUBJECT_ID\n",
      "HADM_ID\n",
      "CHARTDATE\n",
      "CHARTTIME\n",
      "SPEC_ITEMID\n",
      "SPEC_TYPE_DESC\n",
      "ORG_ITEMID\n",
      "ORG_NAME\n",
      "ISOLATE_NUM\n",
      "AB_ITEMID\n",
      "AB_NAME\n",
      "DILUTION_TEXT\n",
      "DILUTION_COMPARISON\n",
      "DILUTION_VALUE\n",
      "INTERPRETATION\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "===============================ADMISSIONS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ADMITTIME\n",
    "DISCHTIME\n",
    "DEATHTIME\n",
    "ADMISSION_TYPE\n",
    "ADMISSION_LOCATION\n",
    "DISCHARGE_LOCATION\n",
    "INSURANCE\n",
    "LANGUAGE\n",
    "RELIGION\n",
    "MARITAL_STATUS\n",
    "ETHNICITY\n",
    "EDREGTIME\n",
    "EDOUTTIME\n",
    "DIAGNOSIS\n",
    "HOSPITAL_EXPIRE_FLAG\n",
    "HAS_CHARTEVENTS_DATA\n",
    "=================================CALLOUT.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SUBMIT_WARDID\n",
    "SUBMIT_CAREUNIT\n",
    "CURR_WARDID\n",
    "CURR_CAREUNIT\n",
    "CALLOUT_WARDID\n",
    "CALLOUT_SERVICE\n",
    "REQUEST_TELE\n",
    "REQUEST_RESP\n",
    "REQUEST_CDIFF\n",
    "REQUEST_MRSA\n",
    "REQUEST_VRE\n",
    "CALLOUT_STATUS\n",
    "CALLOUT_OUTCOME\n",
    "DISCHARGE_WARDID\n",
    "ACKNOWLEDGE_STATUS\n",
    "CREATETIME\n",
    "UPDATETIME\n",
    "ACKNOWLEDGETIME\n",
    "OUTCOMETIME\n",
    "FIRSTRESERVATIONTIME\n",
    "CURRENTRESERVATIONTIME\n",
    "===============================CAREGIVERS.csv.gz================================\n",
    "ROW_ID\n",
    "CGID\n",
    "LABEL\n",
    "DESCRIPTION\n",
    "===============================CHARTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "================================CPTEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "COSTCENTER\n",
    "CHARTDATE\n",
    "CPT_CD\n",
    "CPT_NUMBER\n",
    "CPT_SUFFIX\n",
    "TICKET_ID_SEQ\n",
    "SECTIONHEADER\n",
    "SUBSECTIONHEADER\n",
    "DESCRIPTION\n",
    "=============================DATETIMEEVENTS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "==============================DIAGNOSES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================DRGCODES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "DRG_TYPE\n",
    "DRG_CODE\n",
    "DESCRIPTION\n",
    "DRG_SEVERITY\n",
    "DRG_MORTALITY\n",
    "==================================D_CPT.csv.gz==================================\n",
    "ROW_ID\n",
    "CATEGORY\n",
    "SECTIONRANGE\n",
    "SECTIONHEADER\n",
    "SUBSECTIONRANGE\n",
    "SUBSECTIONHEADER\n",
    "CODESUFFIX\n",
    "MINCODEINSUBSECTION\n",
    "MAXCODEINSUBSECTION\n",
    "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "============================D_ICD_PROCEDURES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "=================================D_ITEMS.csv.gz=================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "ABBREVIATION\n",
    "DBSOURCE\n",
    "LINKSTO\n",
    "CATEGORY\n",
    "UNITNAME\n",
    "PARAM_TYPE\n",
    "CONCEPTID\n",
    "===============================D_LABITEMS.csv.gz================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "FLUID\n",
    "CATEGORY\n",
    "LOINC_CODE\n",
    "================================ICUSTAYS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "FIRST_CAREUNIT\n",
    "LAST_CAREUNIT\n",
    "FIRST_WARDID\n",
    "LAST_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "=============================INPUTEVENTS_CV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALAMOUNTUOM\n",
    "ORIGINALROUTE\n",
    "ORIGINALRATE\n",
    "ORIGINALRATEUOM\n",
    "ORIGINALSITE\n",
    "=============================INPUTEVENTS_MV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCOMPONENTTYPEDESCRIPTION\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "PATIENTWEIGHT\n",
    "TOTALAMOUNT\n",
    "TOTALAMOUNTUOM\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALRATE\n",
    "================================LABEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "FLAG\n",
    "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "SPEC_ITEMID\n",
    "SPEC_TYPE_DESC\n",
    "ORG_ITEMID\n",
    "ORG_NAME\n",
    "ISOLATE_NUM\n",
    "AB_ITEMID\n",
    "AB_NAME\n",
    "DILUTION_TEXT\n",
    "DILUTION_COMPARISON\n",
    "DILUTION_VALUE\n",
    "INTERPRETATION\n",
    "===============================NOTEEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CATEGORY\n",
    "DESCRIPTION\n",
    "CGID\n",
    "ISERROR\n",
    "TEXT\n",
    "==============================OUTPUTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ISERROR\n",
    "================================PATIENTS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "GENDER\n",
    "DOB\n",
    "DOD\n",
    "DOD_HOSP\n",
    "DOD_SSN\n",
    "EXPIRE_FLAG\n",
    "==============================PRESCRIPTIONS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTDATE\n",
    "ENDDATE\n",
    "DRUG_TYPE\n",
    "DRUG\n",
    "DRUG_NAME_POE\n",
    "DRUG_NAME_GENERIC\n",
    "FORMULARY_DRUG_CD\n",
    "GSN\n",
    "NDC\n",
    "PROD_STRENGTH\n",
    "DOSE_VAL_RX\n",
    "DOSE_UNIT_RX\n",
    "FORM_VAL_DISP\n",
    "FORM_UNIT_DISP\n",
    "ROUTE\n",
    "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "LOCATION\n",
    "LOCATIONCATEGORY\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "=============================PROCEDURES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================SERVICES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "TRANSFERTIME\n",
    "PREV_SERVICE\n",
    "CURR_SERVICE\n",
    "================================TRANSFERS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "EVENTTYPE\n",
    "PREV_CAREUNIT\n",
    "CURR_CAREUNIT\n",
    "PREV_WARDID\n",
    "CURR_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================DATETIMEEVENTS.csv.gz==============================\n",
      "   ROW_ID  SUBJECT_ID   HADM_ID  ICUSTAY_ID  ITEMID            CHARTTIME  \\\n",
      "0     711        7657  121183.0    297945.0    3411  2172-03-14 11:00:00   \n",
      "\n",
      "             STORETIME   CGID VALUE VALUEUOM  WARNING  ERROR  RESULTSTATUS  \\\n",
      "0  2172-03-14 11:52:00  16446   NaN     Date      NaN    NaN           NaN   \n",
      "\n",
      "    STOPPED  \n",
      "0  NotStopd  \n",
      "================================CPTEVENTS.csv.gz================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID COSTCENTER  CHARTDATE CPT_CD  CPT_NUMBER  \\\n",
      "0     317       11743   129545        ICU        NaN  99232     99232.0   \n",
      "\n",
      "   CPT_SUFFIX  TICKET_ID_SEQ              SECTIONHEADER  \\\n",
      "0         NaN              6  Evaluation and management   \n",
      "\n",
      "              SUBSECTIONHEADER  DESCRIPTION  \n",
      "0  Hospital inpatient services          NaN  \n",
      "================================PATIENTS.csv.gz=================================\n",
      "   ROW_ID  SUBJECT_ID GENDER                  DOB  DOD DOD_HOSP DOD_SSN  \\\n",
      "0     234         249      F  2075-03-13 00:00:00  NaN      NaN     NaN   \n",
      "\n",
      "   EXPIRE_FLAG  \n",
      "0            0  \n",
      "==============================DIAGNOSES_ICD.csv.gz==============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
      "0    1297         109   172335      1.0     40301\n",
      "===============================CAREGIVERS.csv.gz================================\n",
      "   ROW_ID   CGID LABEL DESCRIPTION\n",
      "0    2228  16174    RO   Read Only\n",
      "==============================PRESCRIPTIONS.csv.gz==============================\n",
      "    ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID            STARTDATE  \\\n",
      "0  2214776           6   107064         NaN  2175-06-11 00:00:00   \n",
      "\n",
      "               ENDDATE DRUG_TYPE        DRUG DRUG_NAME_POE DRUG_NAME_GENERIC  \\\n",
      "0  2175-06-12 00:00:00      MAIN  Tacrolimus    Tacrolimus        Tacrolimus   \n",
      "\n",
      "  FORMULARY_DRUG_CD     GSN          NDC PROD_STRENGTH DOSE_VAL_RX  \\\n",
      "0             TACR1  021796  469061711.0   1mg Capsule           2   \n",
      "\n",
      "  DOSE_UNIT_RX FORM_VAL_DISP FORM_UNIT_DISP ROUTE  \n",
      "0           mg             2            CAP    PO  \n",
      "=============================INPUTEVENTS_MV.csv.gz==============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID            STARTTIME  \\\n",
      "0     241       27063   139787      223259  2133-02-05 06:29:00   \n",
      "\n",
      "               ENDTIME  ITEMID    AMOUNT AMOUNTUOM  RATE  ... TOTALAMOUNTUOM  \\\n",
      "0  2133-02-05 08:45:00  225166  6.774532       mEq   NaN  ...             ml   \n",
      "\n",
      "  ISOPENBAG  CONTINUEINNEXTDEPT  CANCELREASON  STATUSDESCRIPTION  \\\n",
      "0         0                   0             1          Rewritten   \n",
      "\n",
      "  COMMENTS_EDITEDBY COMMENTS_CANCELEDBY        COMMENTS_DATE ORIGINALAMOUNT  \\\n",
      "0               NaN                  RN  2133-02-05 12:52:00           10.0   \n",
      "\n",
      "   ORIGINALRATE  \n",
      "0          0.05  \n",
      "\n",
      "[1 rows x 31 columns]\n",
      "================================DRGCODES.csv.gz=================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID DRG_TYPE  DRG_CODE  \\\n",
      "0     342        2491   144486     HCFA        28   \n",
      "\n",
      "                                         DESCRIPTION  DRG_SEVERITY  \\\n",
      "0  TRAUMATIC STUPOR & COMA, COMA <1 HR AGE >17 WI...           NaN   \n",
      "\n",
      "   DRG_MORTALITY  \n",
      "0            NaN  \n",
      "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
      "   ROW_ID ICD9_CODE            SHORT_TITLE  \\\n",
      "0     174     01166  TB pneumonia-oth test   \n",
      "\n",
      "                                          LONG_TITLE  \n",
      "0  Tuberculous pneumonia [any form], tubercle bac...  \n",
      "===============================D_LABITEMS.csv.gz================================\n",
      "   ROW_ID  ITEMID   LABEL                      FLUID    CATEGORY LOINC_CODE\n",
      "0     546   51346  Blasts  Cerebrospinal Fluid (CSF)  Hematology    26447-3\n",
      "================================TRANSFERS.csv.gz================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID DBSOURCE EVENTTYPE PREV_CAREUNIT  \\\n",
      "0     657         111   192123    254245.0  carevue  transfer           CCU   \n",
      "\n",
      "  CURR_CAREUNIT  PREV_WARDID  CURR_WARDID               INTIME  \\\n",
      "0          MICU          7.0         23.0  2142-04-29 15:27:11   \n",
      "\n",
      "               OUTTIME     LOS  \n",
      "0  2142-05-04 20:38:33  125.19  \n",
      "===============================ADMISSIONS.csv.gz================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
      "0      21          22   165315  2196-04-09 12:26:00  2196-04-10 15:54:00   \n",
      "\n",
      "  DEATHTIME ADMISSION_TYPE    ADMISSION_LOCATION         DISCHARGE_LOCATION  \\\n",
      "0       NaN      EMERGENCY  EMERGENCY ROOM ADMIT  DISC-TRAN CANCER/CHLDRN H   \n",
      "\n",
      "  INSURANCE LANGUAGE      RELIGION MARITAL_STATUS ETHNICITY  \\\n",
      "0   Private      NaN  UNOBTAINABLE        MARRIED     WHITE   \n",
      "\n",
      "             EDREGTIME            EDOUTTIME                DIAGNOSIS  \\\n",
      "0  2196-04-09 10:06:00  2196-04-09 13:24:00  BENZODIAZEPINE OVERDOSE   \n",
      "\n",
      "   HOSPITAL_EXPIRE_FLAG  HAS_CHARTEVENTS_DATA  \n",
      "0                     0                     1  \n",
      "=================================D_ITEMS.csv.gz=================================\n",
      "   ROW_ID  ITEMID                                        LABEL ABBREVIATION  \\\n",
      "0     457     497  Patient controlled analgesia (PCA) [Inject]          NaN   \n",
      "\n",
      "  DBSOURCE      LINKSTO CATEGORY UNITNAME PARAM_TYPE  CONCEPTID  \n",
      "0  carevue  chartevents      NaN      NaN        NaN        NaN  \n",
      "=================================CALLOUT.csv.gz=================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  SUBMIT_WARDID SUBMIT_CAREUNIT  CURR_WARDID  \\\n",
      "0     402         854   175684           52.0             NaN         29.0   \n",
      "\n",
      "  CURR_CAREUNIT  CALLOUT_WARDID CALLOUT_SERVICE  REQUEST_TELE  ...  \\\n",
      "0          MICU               1             MED             0  ...   \n",
      "\n",
      "   CALLOUT_STATUS  CALLOUT_OUTCOME  DISCHARGE_WARDID  ACKNOWLEDGE_STATUS  \\\n",
      "0        Inactive       Discharged              29.0        Acknowledged   \n",
      "\n",
      "            CREATETIME           UPDATETIME      ACKNOWLEDGETIME  \\\n",
      "0  2146-10-05 13:16:55  2146-10-05 13:16:55  2146-10-05 13:24:00   \n",
      "\n",
      "           OUTCOMETIME FIRSTRESERVATIONTIME CURRENTRESERVATIONTIME  \n",
      "0  2146-10-05 18:55:22  2146-10-05 15:27:44                    NaN  \n",
      "\n",
      "[1 rows x 24 columns]\n",
      "==================================D_CPT.csv.gz==================================\n",
      "   ROW_ID  CATEGORY SECTIONRANGE              SECTIONHEADER SUBSECTIONRANGE  \\\n",
      "0       1         1  99201-99499  Evaluation and management     99201-99216   \n",
      "\n",
      "                   SUBSECTIONHEADER CODESUFFIX  MINCODEINSUBSECTION  \\\n",
      "0  Office/other outpatient services        NaN                99201   \n",
      "\n",
      "   MAXCODEINSUBSECTION  \n",
      "0                99216  \n",
      "================================LABEVENTS.csv.gz================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ITEMID            CHARTTIME VALUE  VALUENUM  \\\n",
      "0     281           3      NaN   50820  2101-10-12 16:07:00  7.39      7.39   \n",
      "\n",
      "  VALUEUOM FLAG  \n",
      "0    units  NaN  \n",
      "=============================PROCEDURES_ICD.csv.gz==============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM  ICD9_CODE\n",
      "0     944       62641   154460        3       3404\n",
      "===============================CHARTEVENTS.csv.gz===============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID  ITEMID            CHARTTIME  \\\n",
      "0     788          36   165660    241249.0  223834  2134-05-12 12:00:00   \n",
      "\n",
      "             STORETIME   CGID  VALUE  VALUENUM VALUEUOM  WARNING  ERROR  \\\n",
      "0  2134-05-12 13:56:00  17525   15.0      15.0    L/min        0      0   \n",
      "\n",
      "   RESULTSTATUS  STOPPED  \n",
      "0           NaN      NaN  \n",
      "================================SERVICES.csv.gz=================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID         TRANSFERTIME PREV_SERVICE CURR_SERVICE\n",
      "0     758         471   135879  2122-07-22 14:07:27        TSURG          MED\n",
      "============================D_ICD_PROCEDURES.csv.gz=============================\n",
      "   ROW_ID  ICD9_CODE SHORT_TITLE  LONG_TITLE\n",
      "0     264        851  Canthotomy  Canthotomy\n",
      "================================ICUSTAYS.csv.gz=================================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID DBSOURCE FIRST_CAREUNIT  \\\n",
      "0     365         268   110404      280836  carevue           MICU   \n",
      "\n",
      "  LAST_CAREUNIT  FIRST_WARDID  LAST_WARDID               INTIME  \\\n",
      "0          MICU            52           52  2198-02-14 23:27:38   \n",
      "\n",
      "               OUTTIME    LOS  \n",
      "0  2198-02-18 05:26:11  3.249  \n",
      "=============================INPUTEVENTS_CV.csv.gz==============================\n",
      "   ROW_ID  SUBJECT_ID   HADM_ID  ICUSTAY_ID            CHARTTIME  ITEMID  \\\n",
      "0     592       24457  184834.0    205776.0  2193-09-11 09:00:00   30056   \n",
      "\n",
      "   AMOUNT AMOUNTUOM  RATE  RATEUOM  ... ORDERID  LINKORDERID  STOPPED  \\\n",
      "0   100.0        ml   NaN      NaN  ...  756654      9359133      NaN   \n",
      "\n",
      "   NEWBOTTLE ORIGINALAMOUNT  ORIGINALAMOUNTUOM  ORIGINALROUTE ORIGINALRATE  \\\n",
      "0        NaN            NaN                 ml           Oral          NaN   \n",
      "\n",
      "  ORIGINALRATEUOM  ORIGINALSITE  \n",
      "0             NaN           NaN  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID            STARTTIME  \\\n",
      "0     379       29070   115071    232563.0  2145-03-12 23:04:00   \n",
      "\n",
      "               ENDTIME  ITEMID  VALUE VALUEUOM LOCATION  ...  \\\n",
      "0  2145-03-12 23:05:00  225401    1.0      NaN      NaN  ...   \n",
      "\n",
      "  ORDERCATEGORYNAME SECONDARYORDERCATEGORYNAME  ORDERCATEGORYDESCRIPTION  \\\n",
      "0        Procedures                        NaN              Electrolytes   \n",
      "\n",
      "   ISOPENBAG  CONTINUEINNEXTDEPT CANCELREASON  STATUSDESCRIPTION  \\\n",
      "0          0                   0            0    FinishedRunning   \n",
      "\n",
      "  COMMENTS_EDITEDBY  COMMENTS_CANCELEDBY  COMMENTS_DATE  \n",
      "0               NaN                  NaN            NaN  \n",
      "\n",
      "[1 rows x 25 columns]\n",
      "===============================NOTEEVENTS.csv.gz================================\n",
      "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE  CHARTTIME  STORETIME  \\\n",
      "0     174       22532  167853.0  2151-08-04        NaN        NaN   \n",
      "\n",
      "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
      "0  Discharge summary      Report   NaN      NaN   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
      "==============================OUTPUTEVENTS.csv.gz===============================\n",
      "   ROW_ID  SUBJECT_ID   HADM_ID  ICUSTAY_ID            CHARTTIME  ITEMID  \\\n",
      "0     344       21219  177991.0    225765.0  2142-09-08 10:00:00   40055   \n",
      "\n",
      "   VALUE VALUEUOM            STORETIME   CGID  STOPPED  NEWBOTTLE  ISERROR  \n",
      "0  200.0       ml  2142-09-08 12:08:00  17269      NaN        NaN      NaN  \n",
      "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
      "   ROW_ID  SUBJECT_ID  HADM_ID            CHARTDATE            CHARTTIME  \\\n",
      "0     744          96   170324  2156-04-13 00:00:00  2156-04-13 14:18:00   \n",
      "\n",
      "   SPEC_ITEMID          SPEC_TYPE_DESC  ORG_ITEMID                ORG_NAME  \\\n",
      "0      70021.0  BRONCHOALVEOLAR LAVAGE     80026.0  PSEUDOMONAS AERUGINOSA   \n",
      "\n",
      "   ISOLATE_NUM  AB_ITEMID AB_NAME DILUTION_TEXT DILUTION_COMPARISON  \\\n",
      "0          1.0        NaN     NaN           NaN                 NaN   \n",
      "\n",
      "   DILUTION_VALUE INTERPRETATION  \n",
      "0             NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=================================ADMISSIONS.csv=================================\n",
    "   row_id  subject_id  hadm_id            admittime            dischtime  \\\n",
    "0   12258       10006   142345  2164-10-23 21:09:00  2164-11-01 17:15:00   \n",
    "\n",
    "  deathtime admission_type    admission_location discharge_location insurance  \\\n",
    "0       NaN      EMERGENCY  EMERGENCY ROOM ADMIT   HOME HEALTH CARE  Medicare   \n",
    "\n",
    "  language  religion marital_status               ethnicity  \\\n",
    "0      NaN  CATHOLIC      SEPARATED  BLACK/AFRICAN AMERICAN   \n",
    "\n",
    "             edregtime            edouttime diagnosis  hospital_expire_flag  \\\n",
    "0  2164-10-23 16:43:00  2164-10-23 23:00:00    SEPSIS                     0   \n",
    "\n",
    "   has_chartevents_data  \n",
    "0                     1  \n",
    "==================================CALLOUT.csv===================================\n",
    "   row_id  subject_id  hadm_id  submit_wardid submit_careunit  curr_wardid  \\\n",
    "0    3917       10017   199207              7             NaN           45   \n",
    "\n",
    "  curr_careunit  callout_wardid callout_service  request_tele  ...  \\\n",
    "0           CCU               1             MED             1  ...   \n",
    "\n",
    "   callout_status  callout_outcome  discharge_wardid  acknowledge_status  \\\n",
    "0        Inactive       Discharged              45.0        Acknowledged   \n",
    "\n",
    "            createtime           updatetime      acknowledgetime  \\\n",
    "0  2149-05-31 10:44:34  2149-05-31 10:44:34  2149-05-31 15:08:04   \n",
    "\n",
    "           outcometime firstreservationtime currentreservationtime  \n",
    "0  2149-05-31 22:40:02                  NaN                    NaN  \n",
    "\n",
    "[1 rows x 24 columns]\n",
    "=================================CAREGIVERS.csv=================================\n",
    "   row_id   cgid label description\n",
    "0    2228  16174    RO   Read Only\n",
    "================================CHARTEVENTS.csv=================================\n",
    "    row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  5279021       40124   126179    279554.0  223761  2130-02-04 04:00:00   \n",
    "\n",
    "             storetime   cgid value  valuenum valueuom  warning  error  \\\n",
    "0  2130-02-04 04:35:00  19085  95.9      95.9       ?F      0.0    0.0   \n",
    "\n",
    "  resultstatus stopped  \n",
    "0          NaN     NaN  \n",
    "=================================CPTEVENTS.csv==================================\n",
    "   row_id  subject_id  hadm_id costcenter chartdate  cpt_cd  cpt_number  \\\n",
    "0    4615       10117   105150        ICU       NaN   99254       99254   \n",
    "\n",
    "   cpt_suffix  ticket_id_seq              sectionheader subsectionheader  \\\n",
    "0         NaN            1.0  Evaluation and management    Consultations   \n",
    "\n",
    "  description  \n",
    "0         NaN  \n",
    "===============================DATETIMEEVENTS.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  208474       10076   198503    201006.0    5684  2107-03-25 04:00:00   \n",
    "\n",
    "             storetime   cgid                value valueuom  warning  error  \\\n",
    "0  2107-03-25 04:34:00  20482  2107-03-24 00:00:00     Date      NaN    NaN   \n",
    "\n",
    "   resultstatus   stopped  \n",
    "0           NaN  NotStopd  \n",
    "===============================DIAGNOSES_ICD.csv================================\n",
    "   row_id  subject_id  hadm_id  seq_num icd9_code\n",
    "0  112344       10006   142345        1     99591\n",
    "==================================DRGCODES.csv==================================\n",
    "   row_id  subject_id  hadm_id drg_type  drg_code  \\\n",
    "0    1338       10130   156668     HCFA       148   \n",
    "\n",
    "                                         description  drg_severity  \\\n",
    "0  MAJOR SMALL & LARGE BOWEL PROCEDURES WITH COMP...           NaN   \n",
    "\n",
    "   drg_mortality  \n",
    "0            NaN  \n",
    "===================================D_CPT.csv====================================\n",
    "   row_id  category sectionrange              sectionheader subsectionrange  \\\n",
    "0       1         1  99201-99499  Evaluation and management     99201-99216   \n",
    "\n",
    "                   subsectionheader codesuffix  mincodeinsubsection  \\\n",
    "0  Office/other outpatient services        NaN                99201   \n",
    "\n",
    "   maxcodeinsubsection  \n",
    "0                99216  \n",
    "==============================D_ICD_DIAGNOSES.csv===============================\n",
    "   row_id icd9_code              short_title  \\\n",
    "0       1     01716  Erythem nod tb-oth test   \n",
    "\n",
    "                                          long_title  \n",
    "0  Erythema nodosum with hypersensitivity reactio...  \n",
    "==============================D_ICD_PROCEDURES.csv==============================\n",
    "   row_id  icd9_code               short_title  \\\n",
    "0       1       1423  Chorioret les xenon coag   \n",
    "\n",
    "                                          long_title  \n",
    "0  Destruction of chorioretinal lesion by xenon a...  \n",
    "==================================D_ITEMS.csv===================================\n",
    "   row_id  itemid               label abbreviation dbsource      linksto  \\\n",
    "0       1    1435  Sustained Nystamus          NaN  carevue  chartevents   \n",
    "\n",
    "  category unitname param_type  conceptid  \n",
    "0      NaN      NaN        NaN        NaN  \n",
    "=================================D_LABITEMS.csv=================================\n",
    "   row_id  itemid          label  fluid   category loinc_code\n",
    "0       1   50800  SPECIMEN TYPE  BLOOD  BLOOD GAS        NaN\n",
    "==================================ICUSTAYS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource first_careunit  \\\n",
    "0   12742       10006   142345      206504  carevue           MICU   \n",
    "\n",
    "  last_careunit  first_wardid  last_wardid               intime  \\\n",
    "0          MICU            52           52  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime     los  \n",
    "0  2164-10-25 12:21:07  1.6325  \n",
    "===============================INPUTEVENTS_CV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    1184       10114   167957      234989  2171-11-03 15:00:00   30056   \n",
    "\n",
    "   amount amountuom  rate rateuom  ...  orderid  linkorderid  stopped  \\\n",
    "0   400.0        ml   NaN     NaN  ...  2557279      2557279      NaN   \n",
    "\n",
    "   newbottle originalamount  originalamountuom  originalroute originalrate  \\\n",
    "0        NaN            NaN                 ml           Oral          NaN   \n",
    "\n",
    "  originalrateuom  originalsite  \n",
    "0             NaN           NaN  \n",
    "\n",
    "[1 rows x 22 columns]\n",
    "===============================INPUTEVENTS_MV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0  118897       42367   139932      250305  2147-10-29 16:45:00   \n",
    "\n",
    "               endtime  itemid  amount amountuom  rate  ... totalamountuom  \\\n",
    "0  2147-10-29 16:46:00  225799    60.0        ml   NaN  ...             ml   \n",
    "\n",
    "  isopenbag  continueinnextdept  cancelreason  statusdescription  \\\n",
    "0         0                   0             0    FinishedRunning   \n",
    "\n",
    "  comments_editedby comments_canceledby comments_date originalamount  \\\n",
    "0               NaN                 NaN           NaN           60.0   \n",
    "\n",
    "   originalrate  \n",
    "0          60.0  \n",
    "\n",
    "[1 rows x 31 columns]\n",
    "=================================LABEVENTS.csv==================================\n",
    "    row_id  subject_id  hadm_id  itemid            charttime value  valuenum  \\\n",
    "0  6244563       10006      NaN   50868  2164-09-24 20:21:00    19      19.0   \n",
    "\n",
    "  valueuom flag  \n",
    "0    mEq/L  NaN  \n",
    "=============================MICROBIOLOGYEVENTS.csv=============================\n",
    "   row_id  subject_id  hadm_id            chartdate            charttime  \\\n",
    "0  134694       10006   142345  2164-10-23 00:00:00  2164-10-23 15:30:00   \n",
    "\n",
    "   spec_itemid spec_type_desc  org_itemid                            org_name  \\\n",
    "0        70012  BLOOD CULTURE     80155.0  STAPHYLOCOCCUS, COAGULASE NEGATIVE   \n",
    "\n",
    "   isolate_num  ab_itemid ab_name dilution_text dilution_comparison  \\\n",
    "0          2.0        NaN     NaN           NaN                 NaN   \n",
    "\n",
    "   dilution_value interpretation  \n",
    "0             NaN            NaN  \n",
    "=================================NOTEEVENTS.csv=================================\n",
    "Empty DataFrame\n",
    "Columns: [row_id, subject_id, hadm_id, chartdate, charttime, storetime, category, description, cgid, iserror, text]\n",
    "Index: []\n",
    "================================OUTPUTEVENTS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    6540       10114   167957    234989.0  2171-10-30 20:00:00   40055   \n",
    "\n",
    "   value valueuom            storetime   cgid  stopped  newbottle  iserror  \n",
    "0   39.0       ml  2171-10-30 20:38:00  15029      NaN        NaN      NaN  \n",
    "==================================PATIENTS.csv==================================\n",
    "   row_id  subject_id gender                  dob                  dod  \\\n",
    "0    9467       10006      F  2094-03-05 00:00:00  2165-08-12 00:00:00   \n",
    "\n",
    "              dod_hosp              dod_ssn  expire_flag  \n",
    "0  2165-08-12 00:00:00  2165-08-12 00:00:00            1  \n",
    "===============================PRESCRIPTIONS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            startdate  \\\n",
    "0   32600       42458   159647         NaN  2146-07-21 00:00:00   \n",
    "\n",
    "               enddate drug_type                         drug  \\\n",
    "0  2146-07-22 00:00:00      MAIN  Pneumococcal Vac Polyvalent   \n",
    "\n",
    "                 drug_name_poe            drug_name_generic formulary_drug_cd  \\\n",
    "0  Pneumococcal Vac Polyvalent  PNEUMOcoccal Vac Polyvalent           PNEU25I   \n",
    "\n",
    "       gsn        ndc     prod_strength dose_val_rx dose_unit_rx  \\\n",
    "0  48548.0  6494300.0  25mcg/0.5mL Vial         0.5           mL   \n",
    "\n",
    "  form_val_disp form_unit_disp route  \n",
    "0             1           VIAL    IM  \n",
    "=============================PROCEDUREEVENTS_MV.csv=============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0    8641       42367   139932      250305  2147-10-03 16:40:00   \n",
    "\n",
    "               endtime  itemid  value valueuom        location  ...  \\\n",
    "0  2147-10-06 20:00:00  224263   4520      min  Right Femoral.  ...   \n",
    "\n",
    "  ordercategoryname secondaryordercategoryname  ordercategorydescription  \\\n",
    "0    Invasive Lines                        NaN                      Task   \n",
    "\n",
    "   isopenbag  continueinnextdept cancelreason  statusdescription  \\\n",
    "0          1                   0            0    FinishedRunning   \n",
    "\n",
    "  comments_editedby  comments_canceledby  comments_date  \n",
    "0               NaN                  NaN            NaN  \n",
    "\n",
    "[1 rows x 25 columns]\n",
    "===============================PROCEDURES_ICD.csv===============================\n",
    "   row_id  subject_id  hadm_id  seq_num  icd9_code\n",
    "0    3994       10114   167957        1       3605\n",
    "==================================SERVICES.csv==================================\n",
    "   row_id  subject_id  hadm_id         transfertime prev_service curr_service\n",
    "0   14974       10006   142345  2164-10-23 21:10:15          NaN          MED\n",
    "=================================TRANSFERS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource eventtype prev_careunit  \\\n",
    "0   54440       10006   142345    206504.0  carevue     admit           NaN   \n",
    "\n",
    "  curr_careunit  prev_wardid  curr_wardid               intime  \\\n",
    "0          MICU          NaN         52.0  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime    los  \n",
    "0  2164-10-25 12:21:07  39.18  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données et les packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axJrX0R--kky"
   },
   "source": [
    "Dans cette section, nous incluons les bibliothèques nécessaires et chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures.\n",
    "\n",
    "Voici le but d'utilisation de modules:\n",
    "* Pandas - le module pour gérer facilement les tables (dataframes)\n",
    "* Numpy - le module de base pour la plupart de modules scientifiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW_ID                                                       174\n",
      "SUBJECT_ID                                                 22532\n",
      "HADM_ID                                                 167853.0\n",
      "CHARTDATE                                             2151-08-04\n",
      "CHARTTIME                                                    NaN\n",
      "STORETIME                                                    NaN\n",
      "CATEGORY                                       Discharge summary\n",
      "DESCRIPTION                                               Report\n",
      "CGID                                                         NaN\n",
      "ISERROR                                                      NaN\n",
      "TEXT           Admission Date:  [**2151-7-16**]       Dischar...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission Date:  [**2151-7-16**]       Discharge Date:  [**2151-8-4**]\n",
      "\n",
      "\n",
      "Service:\n",
      "ADDENDUM:\n",
      "\n",
      "RADIOLOGIC STUDIES:  Radiologic studies also included a chest\n",
      "CT, which confirmed cavitary lesions in the left lung apex\n",
      "consistent with infectious process/tuberculosis.  This also\n",
      "moderate-sized left pleural effusion.\n",
      "\n",
      "HEAD CT:  Head CT showed no intracranial hemorrhage or mass\n",
      "effect, but old infarction consistent with past medical\n",
      "history.\n",
      "\n",
      "ABDOMINAL CT:  Abdominal CT showed lesions of\n",
      "T10 and sacrum most likely secondary to osteoporosis. These can\n",
      "be followed by repeat imaging as an outpatient.\n",
      "\n",
      "\n",
      "\n",
      "                            [**First Name8 (NamePattern2) **] [**First Name4 (NamePattern1) 1775**] [**Last Name (NamePattern1) **], M.D.  [**MD Number(1) 1776**]\n",
      "\n",
      "Dictated By:[**Hospital 1807**]\n",
      "MEDQUIST36\n",
      "\n",
      "D:  [**2151-8-5**]  12:11\n",
      "T:  [**2151-8-5**]  12:21\n",
      "JOB#:  [**Job Number 1808**]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0,:]['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vtaj1bVu-kky"
   },
   "outputs": [],
   "source": [
    "# Lire les données à partir d'un fichier CSV dans un DataFrame Pandas\n",
    "#data = pd.read_csv(\"/kaggle/input/clean-data/clean_data.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "# Convertir la colonne 'TIME' en entiers\n",
    "# data['TIME'] = data['TIME'].astype(int)\n",
    "data['TIME'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58976"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(adm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>165315</td>\n",
       "      <td>2196-04-09 12:26:00</td>\n",
       "      <td>2196-04-10 15:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DISC-TRAN CANCER/CHLDRN H</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2196-04-09 10:06:00</td>\n",
       "      <td>2196-04-09 13:24:00</td>\n",
       "      <td>BENZODIAZEPINE OVERDOSE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>152223</td>\n",
       "      <td>2153-09-03 07:15:00</td>\n",
       "      <td>2153-09-08 19:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>124321</td>\n",
       "      <td>2157-10-18 19:34:00</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>161859</td>\n",
       "      <td>2139-06-06 16:14:00</td>\n",
       "      <td>2139-06-09 12:48:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR MYOCARDIAL INFARCTION</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>129635</td>\n",
       "      <td>2160-11-02 02:06:00</td>\n",
       "      <td>2160-11-05 14:55:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2160-11-02 01:01:00</td>\n",
       "      <td>2160-11-02 04:27:00</td>\n",
       "      <td>ACUTE CORONARY SYNDROME</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58971</th>\n",
       "      <td>58594</td>\n",
       "      <td>98800</td>\n",
       "      <td>191113</td>\n",
       "      <td>2131-03-30 21:13:00</td>\n",
       "      <td>2131-04-02 15:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-03-30 19:44:00</td>\n",
       "      <td>2131-03-30 22:41:00</td>\n",
       "      <td>TRAUMA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58972</th>\n",
       "      <td>58595</td>\n",
       "      <td>98802</td>\n",
       "      <td>101071</td>\n",
       "      <td>2151-03-05 20:00:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2151-03-05 17:23:00</td>\n",
       "      <td>2151-03-05 21:06:00</td>\n",
       "      <td>SAH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58973</th>\n",
       "      <td>58596</td>\n",
       "      <td>98805</td>\n",
       "      <td>122631</td>\n",
       "      <td>2200-09-12 07:15:00</td>\n",
       "      <td>2200-09-20 12:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENAL CANCER/SDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58974</th>\n",
       "      <td>58597</td>\n",
       "      <td>98813</td>\n",
       "      <td>170407</td>\n",
       "      <td>2128-11-11 02:29:00</td>\n",
       "      <td>2128-12-22 13:11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>SNF</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2128-11-10 23:48:00</td>\n",
       "      <td>2128-11-11 03:16:00</td>\n",
       "      <td>S/P FALL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58975</th>\n",
       "      <td>58598</td>\n",
       "      <td>98813</td>\n",
       "      <td>190264</td>\n",
       "      <td>2131-10-25 03:09:00</td>\n",
       "      <td>2131-10-26 17:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-10-25 00:08:00</td>\n",
       "      <td>2131-10-25 04:35:00</td>\n",
       "      <td>INTRACRANIAL HEMORRHAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58976 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
       "0          21          22   165315  2196-04-09 12:26:00  2196-04-10 15:54:00   \n",
       "1          22          23   152223  2153-09-03 07:15:00  2153-09-08 19:10:00   \n",
       "2          23          23   124321  2157-10-18 19:34:00  2157-10-25 14:00:00   \n",
       "3          24          24   161859  2139-06-06 16:14:00  2139-06-09 12:48:00   \n",
       "4          25          25   129635  2160-11-02 02:06:00  2160-11-05 14:55:00   \n",
       "...       ...         ...      ...                  ...                  ...   \n",
       "58971   58594       98800   191113  2131-03-30 21:13:00  2131-04-02 15:02:00   \n",
       "58972   58595       98802   101071  2151-03-05 20:00:00  2151-03-06 09:10:00   \n",
       "58973   58596       98805   122631  2200-09-12 07:15:00  2200-09-20 12:08:00   \n",
       "58974   58597       98813   170407  2128-11-11 02:29:00  2128-12-22 13:11:00   \n",
       "58975   58598       98813   190264  2131-10-25 03:09:00  2131-10-26 17:44:00   \n",
       "\n",
       "                 DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
       "0                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "1                      NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "2                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "3                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "4                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "...                    ...            ...                        ...   \n",
       "58971                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58972  2151-03-06 09:10:00      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58973                  NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "58974                  NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "58975                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "\n",
       "              DISCHARGE_LOCATION INSURANCE LANGUAGE           RELIGION  \\\n",
       "0      DISC-TRAN CANCER/CHLDRN H   Private      NaN       UNOBTAINABLE   \n",
       "1               HOME HEALTH CARE  Medicare      NaN           CATHOLIC   \n",
       "2               HOME HEALTH CARE  Medicare     ENGL           CATHOLIC   \n",
       "3                           HOME   Private      NaN  PROTESTANT QUAKER   \n",
       "4                           HOME   Private      NaN       UNOBTAINABLE   \n",
       "...                          ...       ...      ...                ...   \n",
       "58971                       HOME   Private     ENGL      NOT SPECIFIED   \n",
       "58972               DEAD/EXPIRED  Medicare     ENGL           CATHOLIC   \n",
       "58973           HOME HEALTH CARE   Private     ENGL      NOT SPECIFIED   \n",
       "58974                        SNF   Private     ENGL           CATHOLIC   \n",
       "58975                       HOME   Private     ENGL           CATHOLIC   \n",
       "\n",
       "      MARITAL_STATUS ETHNICITY            EDREGTIME            EDOUTTIME  \\\n",
       "0            MARRIED     WHITE  2196-04-09 10:06:00  2196-04-09 13:24:00   \n",
       "1            MARRIED     WHITE                  NaN                  NaN   \n",
       "2            MARRIED     WHITE                  NaN                  NaN   \n",
       "3             SINGLE     WHITE                  NaN                  NaN   \n",
       "4            MARRIED     WHITE  2160-11-02 01:01:00  2160-11-02 04:27:00   \n",
       "...              ...       ...                  ...                  ...   \n",
       "58971         SINGLE     WHITE  2131-03-30 19:44:00  2131-03-30 22:41:00   \n",
       "58972        WIDOWED     WHITE  2151-03-05 17:23:00  2151-03-05 21:06:00   \n",
       "58973        MARRIED     WHITE                  NaN                  NaN   \n",
       "58974        MARRIED     WHITE  2128-11-10 23:48:00  2128-11-11 03:16:00   \n",
       "58975        MARRIED     WHITE  2131-10-25 00:08:00  2131-10-25 04:35:00   \n",
       "\n",
       "                                               DIAGNOSIS  \\\n",
       "0                                BENZODIAZEPINE OVERDOSE   \n",
       "1      CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...   \n",
       "2                                             BRAIN MASS   \n",
       "3                         INTERIOR MYOCARDIAL INFARCTION   \n",
       "4                                ACUTE CORONARY SYNDROME   \n",
       "...                                                  ...   \n",
       "58971                                             TRAUMA   \n",
       "58972                                                SAH   \n",
       "58973                                   RENAL CANCER/SDA   \n",
       "58974                                           S/P FALL   \n",
       "58975                            INTRACRANIAL HEMORRHAGE   \n",
       "\n",
       "       HOSPITAL_EXPIRE_FLAG  HAS_CHARTEVENTS_DATA  \n",
       "0                         0                     1  \n",
       "1                         0                     1  \n",
       "2                         0                     1  \n",
       "3                         0                     1  \n",
       "4                         0                     1  \n",
       "...                     ...                   ...  \n",
       "58971                     0                     1  \n",
       "58972                     1                     1  \n",
       "58973                     0                     1  \n",
       "58974                     0                     0  \n",
       "58975                     0                     1  \n",
       "\n",
       "[58976 rows x 19 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire les données d'admission à partir d'un autre fichier CSV dans un DataFrame Pandas\n",
    "# adm = pd.read_csv(\"/kaggle/input/clean-data/clean_adm.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITTIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMTTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"SUBJECT_ID\"].isin(data[\"SUBJECT_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHigEHe_-kk1"
   },
   "source": [
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T13:22:27.799253Z",
     "iopub.status.busy": "2023-08-27T13:22:27.798779Z",
     "iopub.status.idle": "2023-08-27T13:22:41.546351Z",
     "shell.execute_reply": "2023-08-27T13:22:41.544957Z",
     "shell.execute_reply.started": "2023-08-27T13:22:27.799212Z"
    },
    "id": "f3I6vt3q-kk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46139\n"
     ]
    }
   ],
   "source": [
    "# Créer un dataframe vide avec deux colonnes pour le nouveau dataframe\n",
    "new_data = pd.DataFrame(columns=[\"SUBJECT_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
    "\n",
    "# Grouper les données d'admission par \"SUBJECT_ID\"\n",
    "grouped_adm = adm.groupby(\"SUBJECT_ID\")\n",
    "print(len(grouped_adm))\n",
    "\n",
    "# Parcourir chaque groupe de données associées à un \"SUBJECT_ID\"\n",
    "for subject_id, group in grouped_adm:\n",
    "    # Vérifier si le groupe contient au moins un enregistrement avec la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    if group[\"HOSPITAL_EXPIRE_FLAG\"].eq(1).any():\n",
    "        # Si oui, ajouter le \"SUBJECT_ID\" et la valeur 1 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [1]})], ignore_index=True)\n",
    "    else:\n",
    "        # Sinon, ajouter le \"SUBJECT_ID\" et la valeur 0 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [0]})], ignore_index=True)\n",
    "\n",
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0\n",
    "label_0 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "# Fonction pour diviser les documents en plus petits morceaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVkUXJXk-kk1"
   },
   "source": [
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/cuda/__init__.py:264\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    262\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7I4Tdbb-kk2"
   },
   "source": [
    "# Jeux de données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QDtdhfB-kk2"
   },
   "source": [
    "Cette section prépare l'ensemble de données d'apprentissage en extrayant et traitant les embeddings ClinicalBERT à partir des informations relatives aux patients. Ces embeddings sont ensuite organisés en vue d'analyses ultérieures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## [TRAIN] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36wqGMV3-kk3"
   },
   "source": [
    "Dans cette partie, nous chargeons le modèle ClinicalBERT à partir du référentiel Hugging Face et sélectionnons un échantillon spécifique de patients pour l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat([label_1\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m), label_0\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m78\u001b[39m)])\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\u001b[39;00m\n\u001b[1;32m     14\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\n",
    "sample = pd.concat([label_1.sample(n=100, random_state=56), label_0.sample(n=100, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\n",
    "filtered_data = data[data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'SUBJECT_ID' en agrégeant les listes de 'TEXT' et 'TIME'\n",
    "grouped_sample = filtered_data.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## [TRAIN] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgSkdofN-kk3"
   },
   "source": [
    "Dans cette partie, le code traite les données textuelles en les divisant en segments plus petits et extrait les embeddings ClinicalBERT correspondants à ces segments. Le résultat est un dictionnaire qui associe chaque patient à ses embeddings ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(partie, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Effectuer l'inférence pour obtenir les résultats du modèle\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Récupérer l'embedding du token [CLS] pour chaque partie\u001b[39;00m\n\u001b[1;32m     37\u001b[0m cls_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:436\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[0;32m--> 436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:386\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 386\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    388\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ENSAI/Projet-stat-2A/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/projet/.conda/lib/python3.8/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "time_list = []\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Parcourir les patients et leurs données associées\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer la liste des documents\n",
    "    times = values['TIME']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "    embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "        # Diviser le texte en parties égales\n",
    "        encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "        n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "        n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "        parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "        # Stocker les embeddings des différentes parties du document\n",
    "        cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for partie in parties:\n",
    "            # Convertir la partie dans un format compatible avec le modèle\n",
    "            inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # Stocker l'embedding dans la liste\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "            # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "            time_list.append(time)\n",
    "\n",
    "        # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "    embeddings_dict[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## [TRAIN] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yid4ozu9-kk4"
   },
   "source": [
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### [TRAIN] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.4 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-macosx_12_0_arm64.whl (28.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.8 MB 140.8 MB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 10000, Embedding shape: (24, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "# Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings pour créer une matrice unique\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "# Diviser les embeddings réduits pour chaque patient\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### [TRAIN] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QjMOhW3-kk4"
   },
   "source": [
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliquée par chaque composante principale: [0.15687051 0.12194031 0.08685473 0.05406694 0.04830988 0.04441428\n",
      " 0.03371466 0.02746247 0.02233062 0.01886391 0.01859464 0.01683133\n",
      " 0.01429177 0.01289333 0.01203107 0.01050083 0.00956827 0.00914414\n",
      " 0.00852762 0.00787063 0.00720696 0.00691318 0.00658497 0.00617374\n",
      " 0.00557866 0.005488   0.00484132 0.00476844 0.00455266 0.00443011\n",
      " 0.00433773 0.00399491 0.0037172  0.00357913 0.00346258 0.00339894\n",
      " 0.00329882 0.0030952  0.00297395 0.00268589 0.00266932 0.00257588\n",
      " 0.00249187 0.00243291 0.00227161 0.00217559 0.00210547 0.00206206\n",
      " 0.00199798 0.00190873 0.0018846  0.00185075 0.00180102 0.00175915\n",
      " 0.00174075 0.00172576 0.00167542 0.001614   0.00156624 0.00150878\n",
      " 0.00146477 0.00143411 0.00140291 0.0013794  0.00133722 0.00130369\n",
      " 0.00129745 0.00128346 0.00126033 0.0012087  0.00117082 0.00114967\n",
      " 0.00112559 0.00112096 0.00109772 0.00105876 0.0010284  0.00102636\n",
      " 0.00100847 0.00098992 0.00097796 0.00095227 0.00094002 0.00092038\n",
      " 0.00088646 0.00087764 0.00086764 0.00085527 0.00084452 0.00081647\n",
      " 0.00081117 0.00080269 0.00079432 0.00077793 0.00075168 0.00072707\n",
      " 0.00071722 0.00070577 0.00069008 0.00067312]\n",
      "Variance totale expliquée: 0.906520561198704\n",
      "ID du patient : 10000, Forme des embeddings : (24, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings en une seule matrice\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Afficher la variance totale expliquée par toutes les composantes principales\n",
    "print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## [TRAIN] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHA3C_Ib-kk5"
   },
   "source": [
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'signatory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importer les bibliothèques nécessaires\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import torch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msignatory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Ordre de la signature tronquée\u001b[39;00m\n\u001b[1;32m      6\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'signatory'"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronquée\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "for key, value in reduced_embeddings_dict.items():\n",
    "    # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "    tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "\n",
    "    # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "    # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "\n",
    "    # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "    log_signature_dict[key] = log_signature\n",
    "\n",
    "# À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10000': tensor([ 1.5807,  4.1867,  3.0287,  ..., -0.0092,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '10286': tensor([ 2.5318, -0.2515,  1.8915,  ...,  0.0343,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '10532': tensor([-0.4030,  3.5097, -0.5323,  ...,  0.0430,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11271': tensor([ 3.6321,  0.2937, -0.7424,  ...,  0.0546,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1136': tensor([ 3.3834,  0.7540,  0.6607,  ..., -0.0326,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11766': tensor([ 4.4018,  4.1407,  0.0046,  ..., -0.1344,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11873': tensor([-0.9014, -1.5977, -0.9957,  ..., -0.0239,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '11960': tensor([1.2511, 4.1776, 3.0700,  ..., 0.0474, 0.0000, 0.0000], device='cuda:0'), '12': tensor([5.4604, 0.9422, 1.4187,  ..., 0.1017, 0.0000, 0.0000], device='cuda:0'), '12183': tensor([3.5300, 4.5022, 4.1352,  ..., 0.0863, 0.0000, 0.0000], device='cuda:0'), '12598': tensor([ 3.3212, -2.9122,  0.9391,  ...,  0.0751,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '12712': tensor([3.8651, 5.8274, 1.6218,  ..., 0.1687, 0.0000, 0.0000], device='cuda:0'), '12759': tensor([ 4.3779,  0.5389,  1.0512,  ..., -0.0607,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '12824': tensor([ 5.0613,  1.5975,  2.9631,  ..., -0.0220,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1333': tensor([ 3.3052, -1.1528,  1.2735,  ...,  0.0062,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1339': tensor([ 0.5142, -1.7884, -0.0713,  ...,  0.7048,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1351': tensor([ 1.5992, -1.1741,  0.7032,  ...,  0.0459,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '13695': tensor([1.9971, 0.8767, 0.6699,  ..., 0.0395, 0.0000, 0.0000], device='cuda:0'), '13744': tensor([ 4.9923,  1.0971,  1.7721,  ..., -0.0245,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14036': tensor([ 5.9998,  0.4681,  1.0532,  ..., -0.0535,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14080': tensor([3.1026, 3.5456, 2.5602,  ..., 0.0515, 0.0000, 0.0000], device='cuda:0'), '14218': tensor([ 5.0178,  1.5802,  2.1028,  ..., -0.1063,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1426': tensor([2.2174, 2.5090, 2.6471,  ..., 0.0435, 0.0000, 0.0000], device='cuda:0'), '14338': tensor([ 4.3874, -1.4642, -2.3702,  ..., -0.0815,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14366': tensor([ 4.7942, -0.2571,  0.8159,  ...,  0.0539,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1465': tensor([ 2.4285, -1.3811,  3.3521,  ..., -0.0297,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14847': tensor([ 3.1676, -1.2690,  2.3175,  ..., -0.0299,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '14937': tensor([4.7316, 5.4671, 1.0546,  ..., 0.0318, 0.0000, 0.0000], device='cuda:0'), '1503': tensor([ 5.0051,  0.2059,  3.5410,  ..., -0.0178,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '15185': tensor([ 3.6136,  0.6481,  2.7206,  ..., -0.2585,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '15590': tensor([ 3.0548,  1.1607,  1.7483,  ..., -0.0206,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16026': tensor([ 3.5928,  0.7963,  1.3735,  ..., -0.0897,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16102': tensor([ 2.6173,  3.1008,  2.1598,  ..., -0.0534,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16176': tensor([4.8479, 0.4483, 2.1987,  ..., 0.0909, 0.0000, 0.0000], device='cuda:0'), '16189': tensor([ 2.7253,  1.0001,  2.2216,  ..., -0.0617,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16283': tensor([4.6229, 0.2980, 0.2789,  ..., 0.0304, 0.0000, 0.0000], device='cuda:0'), '164': tensor([ 2.9157,  0.0965,  1.6099,  ..., -0.0615,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '16874': tensor([5.1412, 3.1146, 4.4286,  ..., 0.0927, 0.0000, 0.0000], device='cuda:0'), '17489': tensor([4.3073, 1.0093, 0.0560,  ..., 0.0552, 0.0000, 0.0000], device='cuda:0'), '17798': tensor([ 1.5020, -1.8774, -2.2531,  ...,  0.1496,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '17805': tensor([ 4.3211,  2.8190,  2.9103,  ..., -0.0359,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1787': tensor([4.3230, 0.8931, 3.0771,  ..., 0.0577, 0.0000, 0.0000], device='cuda:0'), '17907': tensor([ 2.4556,  2.1029,  1.5636,  ..., -0.0478,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '17924': tensor([3.2823, 4.4991, 2.8658,  ..., 0.3125, 0.0000, 0.0000], device='cuda:0'), '17972': tensor([ 2.2049, -1.1618,  3.6418,  ...,  0.1447,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18094': tensor([ 3.3648,  1.6715,  1.8700,  ..., -0.2297,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1855': tensor([ 1.3345, -1.0260,  0.5199,  ...,  0.0822,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18566': tensor([-0.2963,  0.0276,  0.4130,  ...,  0.0587,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '18873': tensor([2.5414, 0.1039, 4.2670,  ..., 0.0237, 0.0000, 0.0000], device='cuda:0'), '19271': tensor([-0.7159, -2.7118, -1.2443,  ..., -0.0190,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '1944': tensor([ 4.4603,  3.3799,  3.5925,  ..., -0.1350,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '19499': tensor([ 5.6195,  1.6633,  1.1754,  ..., -0.0522,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '19603': tensor([ 2.4078, -0.9668,  1.6626,  ...,  0.0392,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '20023': tensor([ 5.1323e+00, -3.2783e-04,  1.1528e+00,  ..., -4.8847e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '20062': tensor([ 4.2657e+00, -4.9050e-01,  9.3489e-01,  ...,  3.3600e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '20244': tensor([ 4.1657,  0.5081,  2.1965,  ..., -0.0243,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '20651': tensor([ 3.8625,  0.1595,  1.7197,  ..., -0.0466,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2121': tensor([4.2433, 2.7482, 1.7740,  ..., 0.0717, 0.0000, 0.0000], device='cuda:0'), '21402': tensor([ 1.4062, -0.5293,  1.0387,  ..., -0.0412,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '21842': tensor([ 5.5954,  3.3177,  5.3091,  ..., -0.1651,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '21862': tensor([ 4.5231,  0.9569,  1.8266,  ..., -0.0370,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22016': tensor([ 4.2063, -0.7874,  1.6900,  ..., -0.0096,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22220': tensor([ 3.0740, -0.0798,  3.4349,  ...,  1.8974,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22241': tensor([4.5646, 0.1156, 1.8938,  ..., 0.2013, 0.0000, 0.0000], device='cuda:0'), '22263': tensor([ 0.8448, -0.5012, -0.6397,  ...,  0.0643,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '22660': tensor([2.1509, 4.4302, 2.9270,  ..., 0.0562, 0.0000, 0.0000], device='cuda:0'), '22732': tensor([3.9721, 0.5102, 1.0695,  ..., 0.1763, 0.0000, 0.0000], device='cuda:0'), '23527': tensor([ 2.3986,  2.0178,  1.4587,  ..., -0.0338,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '23610': tensor([ 1.5854, -0.2549, -0.1105,  ...,  0.0146,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2362': tensor([2.6044, 3.1822, 3.0983,  ..., 0.0953, 0.0000, 0.0000], device='cuda:0'), '23637': tensor([3.6533, 0.5286, 0.7543,  ..., 0.0675, 0.0000, 0.0000], device='cuda:0'), '23977': tensor([ 4.0364,  0.4551,  0.9694,  ..., -0.0992,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '24018': tensor([4.1902, 0.4768, 1.8847,  ..., 0.2919, 0.0000, 0.0000], device='cuda:0'), '24134': tensor([5.0070, 0.4500, 2.5564,  ..., 0.0161, 0.0000, 0.0000], device='cuda:0'), '2456': tensor([ 6.9571, -2.3057, -1.2135,  ...,  0.0512,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '24636': tensor([5.6836, 4.9166, 0.9738,  ..., 0.0320, 0.0000, 0.0000], device='cuda:0'), '24750': tensor([ 3.2951, -1.0787,  2.7410,  ..., -0.0892,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25165': tensor([ 5.1171,  1.5629,  0.9361,  ..., -0.2034,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25415': tensor([ 2.4478,  0.1037,  0.9744,  ..., -0.0449,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25447': tensor([ 1.6223, -0.0747, -0.2615,  ..., -0.0942,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25525': tensor([ 3.4420,  0.6419,  2.7697,  ..., -0.0221,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '25671': tensor([3.7699, 5.0196, 1.9640,  ..., 0.0627, 0.0000, 0.0000], device='cuda:0'), '25894': tensor([ 2.7019, -0.3878,  1.3227,  ...,  0.0763,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26201': tensor([4.8523, 1.7473, 3.8526,  ..., 0.0876, 0.0000, 0.0000], device='cuda:0'), '26690': tensor([ 4.0022,  5.0103,  0.9514,  ..., -0.0158,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26815': tensor([ 3.0119, -2.9179, -0.2219,  ...,  0.0282,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26868': tensor([-1.4908, -1.1615,  0.5418,  ...,  0.3835,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '26979': tensor([ 3.0688,  0.6516,  2.0855,  ..., -0.1884,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27378': tensor([ 3.7843,  4.0319,  3.8294,  ..., -0.2252,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27404': tensor([ 0.5549, -0.7247, -1.1293,  ...,  0.1193,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27421': tensor([ 1.6301,  2.7149, -0.3492,  ..., -0.0268,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27742': tensor([ 2.8850,  4.1987,  2.2821,  ..., -0.0621,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '27884': tensor([ 2.9637,  5.9713,  3.2426,  ..., -0.0367,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '2800': tensor([ 2.8816e+00,  3.5547e+00,  3.2019e+00,  ..., -3.0521e-04,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '28327': tensor([3.9285, 1.1768, 1.2186,  ..., 0.0905, 0.0000, 0.0000], device='cuda:0'), '28742': tensor([ 1.9628, -0.6619, -0.6313,  ...,  0.3263,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '28889': tensor([2.5639, 0.3373, 2.1202,  ..., 0.0756, 0.0000, 0.0000], device='cuda:0'), '29092': tensor([2.8482, 3.4629, 2.2126,  ..., 0.0684, 0.0000, 0.0000], device='cuda:0'), '29886': tensor([2.9235, 5.3285, 1.0225,  ..., 0.0605, 0.0000, 0.0000], device='cuda:0'), '29954': tensor([-2.8982,  3.4002, -0.1161,  ..., -0.0245,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30126': tensor([4.6065, 0.6217, 1.4491,  ..., 0.0398, 0.0000, 0.0000], device='cuda:0'), '30128': tensor([ 2.8725,  1.3262,  2.3913,  ..., -0.0702,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30184': tensor([ 0.8319, -0.3694,  0.5321,  ...,  0.3685,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '30810': tensor([4.2399, 1.1161, 0.5793,  ..., 0.0474, 0.0000, 0.0000], device='cuda:0'), '30864': tensor([4.1791, 1.6684, 0.2752,  ..., 0.1240, 0.0000, 0.0000], device='cuda:0'), '31523': tensor([3.9500, 4.5470, 2.9779,  ..., 0.0427, 0.0000, 0.0000], device='cuda:0'), '31811': tensor([ 3.0876, -0.1670,  3.2383,  ..., -0.0060,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32050': tensor([3.8398, 0.1889, 1.1030,  ..., 0.0207, 0.0000, 0.0000], device='cuda:0'), '32246': tensor([ 1.6479,  2.2763,  1.6646,  ..., -0.0137,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32260': tensor([ 2.4066, -0.4634,  0.9954,  ..., -0.0440,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32489': tensor([1.1485, 5.4836, 3.6875,  ..., 0.0858, 0.0000, 0.0000], device='cuda:0'), '32606': tensor([ 3.4955, -0.3211,  2.5009,  ..., -0.1589,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32684': tensor([ 3.5830,  4.5982,  4.7143,  ..., -0.2980,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32711': tensor([-0.5785,  4.6630,  2.0929,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '32804': tensor([2.4984, 5.7234, 3.8989,  ..., 0.0294, 0.0000, 0.0000], device='cuda:0'), '3590': tensor([ 4.8593,  0.9575,  1.4337,  ..., -0.0368,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '40310': tensor([ 0.5448, -0.5174,  0.2076,  ...,  0.8636,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '40440': tensor([-3.1391e+00,  3.3111e+00,  6.2716e-02,  ..., -1.8520e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '41203': tensor([-0.2010,  2.6495, -2.0458,  ...,  0.2844,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4158': tensor([ 2.1549,  0.4830,  1.5915,  ..., -0.0024,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '41696': tensor([-2.4719,  3.7148, -0.3909,  ...,  0.0773,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '42255': tensor([-2.9380,  2.4086,  0.1275,  ...,  0.1183,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '43276': tensor([-8.3479e-01,  5.9808e+00,  2.2052e-01,  ..., -5.5369e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '43323': tensor([-2.0659,  2.8380,  0.1549,  ...,  0.0140,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '43742': tensor([-0.9482,  1.7565, -0.0366,  ...,  0.1332,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '44265': tensor([-3.4389,  1.8984, -0.5269,  ...,  0.1985,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '44851': tensor([ 2.0915, -1.0396, -0.1240,  ...,  0.9133,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4552': tensor([ 2.9731, -0.3913,  0.3997,  ..., -0.0083,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '4836': tensor([ 3.0618,  4.8347,  3.3022,  ..., -0.1432,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '49139': tensor([-0.4250,  1.8164, -0.6867,  ...,  0.0134,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5021': tensor([4.2630, 1.1796, 0.4083,  ..., 0.0335, 0.0000, 0.0000], device='cuda:0'), '50417': tensor([-4.6274,  2.3038, -0.4651,  ..., -0.0552,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '50486': tensor([-1.7454,  2.8284, -0.4835,  ...,  0.0554,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '50664': tensor([-2.2097,  3.7292, -1.9190,  ..., -0.2473,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51226': tensor([-1.9932,  1.3771,  0.1823,  ...,  0.0390,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51805': tensor([-2.5755,  7.1158,  4.5629,  ...,  0.1441,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '51914': tensor([ 1.0817, -0.7826,  0.4629,  ..., -0.0432,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '52010': tensor([-2.2183,  2.8052, -0.4466,  ...,  0.0327,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5205': tensor([ 4.1909,  1.3073,  1.8316,  ..., -0.4203,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5453': tensor([ 2.7680,  5.1448,  2.9389,  ..., -0.0407,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '54979': tensor([-2.5607,  6.0574,  2.2624,  ...,  0.0663,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5506': tensor([ 3.0384,  4.6712,  1.4055,  ..., -0.1041,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '5544': tensor([4.5185, 0.1265, 2.9002,  ..., 0.2294, 0.0000, 0.0000], device='cuda:0'), '55639': tensor([-1.7714,  4.3228, -0.3350,  ..., -0.0335,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '55728': tensor([-3.7171,  2.5824,  0.1835,  ...,  0.0377,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '56309': tensor([2.9358, 1.5061, 0.6421,  ..., 0.0170, 0.0000, 0.0000], device='cuda:0'), '5670': tensor([3.6826, 1.5115, 2.1010,  ..., 0.0746, 0.0000, 0.0000], device='cuda:0'), '57579': tensor([-2.1949,  3.5413,  0.2187,  ...,  0.0606,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '58154': tensor([-3.0650,  0.8027, -0.3258,  ..., -0.1228,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '58757': tensor([ 0.1753, -1.9894, -0.7190,  ..., -0.2196,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59067': tensor([-2.0403,  2.8158,  0.3716,  ...,  0.6985,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59133': tensor([-1.7572,  2.9731, -0.2037,  ...,  0.1074,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '59725': tensor([-1.4323,  3.8671, -1.5763,  ...,  0.0433,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '61654': tensor([-0.1125,  0.6155, -0.5477,  ...,  0.1373,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '62157': tensor([-1.0018, -1.0633,  2.6217,  ..., -0.0906,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '62186': tensor([-1.5757, -0.7536,  5.8599,  ...,  0.9183,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '63492': tensor([-3.0908,  4.7231, -2.0699,  ..., -0.0346,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '63686': tensor([-2.0899,  3.3583, -0.8176,  ...,  0.0295,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '65665': tensor([-1.4744,  0.0220,  2.0382,  ...,  0.2239,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6638': tensor([ 5.0021,  1.5169,  2.9086,  ..., -0.0067,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6686': tensor([ 5.2556,  0.1199,  0.9969,  ..., -0.0189,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6702': tensor([ 5.5944,  3.8344,  4.1577,  ..., -0.5121,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6735': tensor([2.9176, 4.2295, 1.7984,  ..., 0.0683, 0.0000, 0.0000], device='cuda:0'), '69232': tensor([-1.8802,  5.7050,  2.7983,  ...,  0.1075,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '6977': tensor([-2.6302,  1.9815, -0.9851,  ..., -0.0519,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '71233': tensor([-2.8294,  3.3073, -0.2915,  ..., -0.0220,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72154': tensor([-0.5969,  2.4439, -0.1137,  ..., -0.0430,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7230': tensor([ 3.2242, -0.3937,  0.3488,  ...,  0.1502,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72931': tensor([-1.5331,  1.7055, -1.0648,  ...,  0.4048,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '72942': tensor([ 3.9158,  1.3077, -0.8563,  ...,  0.0093,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7343': tensor([ 2.8699,  1.5970,  0.7792,  ..., -0.0299,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '7529': tensor([ 4.2342,  0.5296,  1.1309,  ..., -0.0088,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '77046': tensor([-2.6141e+00,  1.7258e+00, -2.4619e-01,  ...,  2.2737e-03,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), '7900': tensor([3.1430, 3.9256, 4.6933,  ..., 0.1929, 0.0000, 0.0000], device='cuda:0'), '7904': tensor([4.2094, 4.2963, 4.3769,  ..., 0.0216, 0.0000, 0.0000], device='cuda:0'), '7919': tensor([0.8806, 0.7279, 1.2024,  ..., 0.5148, 0.0000, 0.0000], device='cuda:0'), '80262': tensor([ 3.0174,  0.5617, -1.3250,  ...,  0.0499,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '80342': tensor([ 2.2279, -1.4361,  0.3224,  ..., -0.2012,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '81068': tensor([-3.2153,  3.4400, -0.4063,  ..., -0.0060,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '81866': tensor([-1.7349,  3.7383, -1.3181,  ...,  0.0236,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '819': tensor([3.5189, 0.9145, 1.5460,  ..., 0.0151, 0.0000, 0.0000], device='cuda:0'), '82393': tensor([-3.1009,  3.1278, -0.1004,  ...,  0.1783,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '82469': tensor([-0.7850,  3.6829, -1.1029,  ...,  0.0250,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83691': tensor([-2.8123,  2.8351, -0.8451,  ...,  0.1913,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83773': tensor([-0.8334,  3.5582, -1.8846,  ...,  0.0187,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '83982': tensor([ 1.4270, -0.6556, -0.2883,  ..., -0.6180,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '8549': tensor([ 3.3002,  4.5928,  2.6465,  ..., -0.0206,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '86006': tensor([-1.9847,  2.8347, -1.2519,  ..., -0.0525,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '86702': tensor([-0.1583,  3.7829, -2.3965,  ...,  0.2357,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '87572': tensor([-2.1732,  3.2132,  0.1725,  ...,  0.1653,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '89840': tensor([-2.5655,  3.9363,  0.5281,  ...,  0.2233,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '91633': tensor([-3.1653,  4.3211,  0.2646,  ...,  0.0341,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '93528': tensor([-0.6939,  1.1882,  2.3405,  ...,  0.0809,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '94415': tensor([-1.4016,  3.6618, -1.2995,  ...,  0.0933,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '9444': tensor([ 4.9233, -1.7062,  1.4326,  ...,  0.0887,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '94617': tensor([-2.7639,  1.8104, -0.8737,  ...,  0.0426,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '9467': tensor([ 4.2338,  2.5515,  2.0898,  ..., -0.1350,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '975': tensor([-3.5765, -1.3259,  4.3683,  ..., -0.0769,  0.0000,  0.0000],\n",
      "       device='cuda:0'), '99865': tensor([-0.4875,  1.1588, -0.3823,  ...,  0.1548,  0.0000,  0.0000],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(log_signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiJE3_iL-kk5"
   },
   "source": [
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.877945Z",
     "iopub.status.busy": "2023-08-27T15:50:54.877545Z",
     "iopub.status.idle": "2023-08-27T15:50:54.888896Z",
     "shell.execute_reply": "2023-08-27T15:50:54.887365Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.877912Z"
    },
    "id": "XZaS5YUq-kk6",
    "outputId": "470eeea0-678e-4f82-8b87-e5ab52c4a3f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 5153)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>5142</th>\n",
       "      <th>5143</th>\n",
       "      <th>5144</th>\n",
       "      <th>5145</th>\n",
       "      <th>5146</th>\n",
       "      <th>5147</th>\n",
       "      <th>5148</th>\n",
       "      <th>5149</th>\n",
       "      <th>5150</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>1.580732</td>\n",
       "      <td>4.186687</td>\n",
       "      <td>3.028702</td>\n",
       "      <td>-2.008340</td>\n",
       "      <td>-0.220417</td>\n",
       "      <td>2.378833</td>\n",
       "      <td>-1.820687</td>\n",
       "      <td>-2.836124</td>\n",
       "      <td>2.407619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034118</td>\n",
       "      <td>0.024424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037972</td>\n",
       "      <td>-0.091475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10286</td>\n",
       "      <td>2.531790</td>\n",
       "      <td>-0.251507</td>\n",
       "      <td>1.891476</td>\n",
       "      <td>-4.173205</td>\n",
       "      <td>0.223488</td>\n",
       "      <td>-0.381999</td>\n",
       "      <td>-1.663747</td>\n",
       "      <td>-2.654674</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250363</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.087456</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10532</td>\n",
       "      <td>-0.402976</td>\n",
       "      <td>3.509747</td>\n",
       "      <td>-0.532292</td>\n",
       "      <td>-0.727622</td>\n",
       "      <td>-0.759668</td>\n",
       "      <td>1.675333</td>\n",
       "      <td>0.747812</td>\n",
       "      <td>-0.011958</td>\n",
       "      <td>-1.794389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>0.043533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11271</td>\n",
       "      <td>3.632075</td>\n",
       "      <td>0.293749</td>\n",
       "      <td>-0.742413</td>\n",
       "      <td>-1.989862</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.957444</td>\n",
       "      <td>1.632290</td>\n",
       "      <td>2.513831</td>\n",
       "      <td>-2.261577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038107</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>0.049178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1136</td>\n",
       "      <td>3.383447</td>\n",
       "      <td>0.753994</td>\n",
       "      <td>0.660684</td>\n",
       "      <td>-3.736158</td>\n",
       "      <td>-0.761842</td>\n",
       "      <td>-0.661879</td>\n",
       "      <td>-0.313948</td>\n",
       "      <td>-2.516666</td>\n",
       "      <td>-1.984024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121096</td>\n",
       "      <td>-0.043274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>-0.037597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.032573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SUBJECT_ID         0         1         2         3         4         5  \\\n",
       "0      10000  1.580732  4.186687  3.028702 -2.008340 -0.220417  2.378833   \n",
       "1      10286  2.531790 -0.251507  1.891476 -4.173205  0.223488 -0.381999   \n",
       "2      10532 -0.402976  3.509747 -0.532292 -0.727622 -0.759668  1.675333   \n",
       "3      11271  3.632075  0.293749 -0.742413 -1.989862  0.867802  0.957444   \n",
       "4       1136  3.383447  0.753994  0.660684 -3.736158 -0.761842 -0.661879   \n",
       "\n",
       "          6         7         8  ...      5142      5143  5144      5145  \\\n",
       "0 -1.820687 -2.836124  2.407619  ... -0.034118  0.024424   0.0  0.037972   \n",
       "1 -1.663747 -2.654674  0.047086  ...  0.250363  0.162344   0.0 -0.087456   \n",
       "2  0.747812 -0.011958 -1.794389  ...  0.008459  0.020197   0.0 -0.025810   \n",
       "3  1.632290  2.513831 -2.261577  ... -0.038107  0.008433   0.0 -0.030156   \n",
       "4 -0.313948 -2.516666 -1.984024  ...  0.121096 -0.043274   0.0 -0.042306   \n",
       "\n",
       "       5146  5147      5148  5149  5150  HOSPITAL_EXPIRE_FLAG  \n",
       "0 -0.091475   0.0 -0.009156   0.0   0.0                     0  \n",
       "1  0.114900   0.0  0.034347   0.0   0.0                     0  \n",
       "2  0.043533   0.0  0.043025   0.0   0.0                     0  \n",
       "3  0.049178   0.0  0.054637   0.0   0.0                     1  \n",
       "4 -0.037597   0.0 -0.032573   0.0   0.0                     0  \n",
       "\n",
       "[5 rows x 5153 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXfOmn2w-kk6"
   },
   "source": [
    "# Jeux de données test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7D4ONXb-kk6"
   },
   "source": [
    "Cette partie prépare un ensemble de données distinct pour les tests en choisissant au hasard un groupe de patients avec des étiquettes équilibrées de décès à l'hôpital. Ces données seront utilisées pour évaluer les performances du modèle sur de nouvelles observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI4KlKLo-kk6"
   },
   "source": [
    "## [TEST] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_bJh3DE-kk6"
   },
   "source": [
    "Dans cette partie, le code charge le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients dans l'ensemble de données de test. Cela permet d'avoir accès au modèle pré-entraîné afin de traiter les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.891762Z",
     "iopub.status.busy": "2023-08-27T15:50:54.891334Z",
     "iopub.status.idle": "2023-08-27T15:50:57.431020Z",
     "shell.execute_reply": "2023-08-27T15:50:57.429444Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.891728Z"
    },
    "id": "ytlaOQLJ-kk6",
    "outputId": "9964e28b-6a66-47e5-e26a-e7e73547bf55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle pré-entraîné Bio_ClinicalBERT et son tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Filtrer les données test (new_data) pour ne conserver que les patients absents dans le dictionnaire des embeddings (flattened_embeddings_dict)\n",
    "new_data_test = new_data[~new_data[\"SUBJECT_ID\"].isin(flattened_embeddings_dict.keys())]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1.sample(n=15, random_state=56), label_0.sample(n=85, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"SUBJECT_ID\"].isin(sample_test[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kQsnSe8-kk7"
   },
   "source": [
    "## [TEST] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV77RZCc-kk7"
   },
   "source": [
    "Cette partie du code se focalise sur l'extraction des embeddings ClinicalBERT à partir du texte des patients présents dans le jeu de données de test. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Les embeddings de chaque patient sont sauvegardés pour une utilisation ultérieure lors de l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:35:17.753028Z",
     "iopub.status.busy": "2023-08-09T12:35:17.752625Z",
     "iopub.status.idle": "2023-08-09T15:35:47.773903Z",
     "shell.execute_reply": "2023-08-09T15:35:47.771299Z",
     "shell.execute_reply.started": "2023-08-09T12:35:17.752990Z"
    },
    "id": "ReL_-53L-kk7"
   },
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample_test.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Créer une liste pour stocker les valeurs de 'TIME' de chaque partie d'un document\n",
    "time_list_test = []\n",
    "\n",
    "# Créer un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict_test = {}\n",
    "\n",
    "# Parcourir les patients et leurs données textuelles\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer les documents textuels\n",
    "    times = values['TIME']  # Récupérer les valeurs de 'TIME' associées\n",
    "\n",
    "    embeddings_list = []  # Stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "\n",
    "        # Diviser le texte en parties de longueur appropriée\n",
    "        encoded_text = tokenizer.encode(text)\n",
    "        n_tokens = len(encoded_text)\n",
    "        n_chunks = max(1, n_tokens // 512)\n",
    "        windows = split_text(text, n_chunks)\n",
    "\n",
    "        cls_embeddings_list = []  # Stocker les embeddings CLS de chaque partie\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for window in windows:\n",
    "            # Convertir la fenêtre en un format compatible avec le modèle\n",
    "            inputs = tokenizer(window, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence sur le modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque fenêtre\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "\n",
    "            # Ajouter la valeur 'TIME' correspondante à la liste\n",
    "            time_list_test.append(time)\n",
    "\n",
    "        # Moyenne des embeddings CLS pour chaque fenêtre\n",
    "        #avg_cls_embedding = torch.mean(torch.stack(cls_embeddings_list), dim=0)\n",
    "        #embeddings_list.append(avg_cls_embedding)\n",
    "        # Ajouter la liste des embeddings CLS à la liste des embeddings pour ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans le dictionnaire avec le n° du patient comme clé\n",
    "    embeddings_dict_test[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS0t1T5G-kk8"
   },
   "source": [
    "## [TEST] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HDmvGSq-kk8"
   },
   "source": [
    "### [TEST] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyIj1Zak-kk8"
   },
   "source": [
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:57.434945Z",
     "iopub.status.busy": "2023-08-27T15:50:57.434509Z",
     "iopub.status.idle": "2023-08-27T15:50:58.953013Z",
     "shell.execute_reply": "2023-08-27T15:50:58.951571Z",
     "shell.execute_reply.started": "2023-08-27T15:50:57.434911Z"
    },
    "id": "eKOGB5UY-kk8",
    "outputId": "66191fbd-a498-45a8-d3f8-8f1a11663b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 10289, Embedding shape: (21, 101)\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe GaussianRandomProjection de la bibliothèque scikit-learn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque sujet\n",
    "lengths_test = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Appliquer la projection gaussienne aléatoire sur les embeddings\n",
    "reduced_embeddings_np_test = transformer.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths_test)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 101)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 101)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKwKl1V-kk8"
   },
   "source": [
    "### [TEST] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEl9Ashr-kk8"
   },
   "source": [
    "La technique de l'Analyse en Composantes Principales (ACP) est utilisée pour réduire la dimensionnalité des plongements dans le jeu de données de test, tout en maintenant les informations pertinentes. De plus, cette méthode permet d'afficher la variance expliquée par chaque composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-27T14:35:06.528046Z",
     "iopub.status.busy": "2023-08-27T14:35:06.527090Z",
     "iopub.status.idle": "2023-08-27T14:35:10.238543Z",
     "shell.execute_reply": "2023-08-27T14:35:10.237285Z",
     "shell.execute_reply.started": "2023-08-27T14:35:06.528009Z"
    },
    "id": "Ptm55O3q-kk9",
    "outputId": "28717a71-974b-4b8c-e5e5-87a83a4c65d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each component: [0.17085595 0.12655854 0.07180104 0.05270758 0.04825578 0.04018151\n",
      " 0.03466578 0.02513221 0.02216924 0.02000224 0.01804894 0.01541976\n",
      " 0.01398654 0.01324861 0.0114102  0.01096654 0.01021872 0.00904915\n",
      " 0.00886639 0.00855818 0.00707977 0.0069404  0.00655178 0.00637795\n",
      " 0.00564005 0.00540953 0.00507004 0.00493546 0.00477063 0.00450308\n",
      " 0.00421699 0.00401662 0.0039828  0.00371767 0.00362881 0.0034303\n",
      " 0.00341119 0.00304949 0.00292737 0.00276166 0.00267746 0.00260731\n",
      " 0.00258352 0.00247502 0.00236587 0.00228805 0.00218283 0.0021599\n",
      " 0.00211235 0.00209489 0.00198502 0.00196362 0.00186125 0.00185165\n",
      " 0.00180456 0.00174533 0.00166968 0.00161585 0.00156355 0.00154717\n",
      " 0.00153203 0.00145485 0.00142228 0.00140095 0.00136783 0.00135508\n",
      " 0.00133627 0.00131552 0.00126198 0.00125601 0.00123491 0.00120362\n",
      " 0.00118936 0.00115279 0.00113946 0.0011192  0.00108613 0.00107388\n",
      " 0.00104935 0.001023   0.00101481 0.00101169 0.00099267 0.00096795\n",
      " 0.00092423 0.00091578 0.00089908 0.00088673 0.00087788 0.00086313\n",
      " 0.0008411  0.00082344 0.00081373 0.00079497 0.00077749 0.00076954\n",
      " 0.00074712 0.00074211 0.00072948 0.00070163]\n",
      "Total variance explained: 0.9077483667060733\n",
      "Subject ID: 10289, Embedding shape: (21, 101)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice numpy\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser l'analyse en composantes principales (ACP) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle ACP sur les données et les transformer\n",
    "reduced_embeddings_np_test = pca.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 100)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    # Créer un tableau vide pour stocker les embeddings et le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        # Concaténer l'embedding réduit avec la valeur 'time'\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETKJU6AP-kk9"
   },
   "source": [
    "## [TEST] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGvVJ66Q-kk9"
   },
   "source": [
    "Ici, le programme calcule les signatures logarithmiques pour les embeddings des données de test. Ces signatures contiennent des informations plus avancées qui peuvent être bénéfiques pour l'entraînement du modèle. Le résultat est un dictionnaire où les embeddings de chaque patient sont représentés sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:58.954909Z",
     "iopub.status.busy": "2023-08-27T15:50:58.954544Z",
     "iopub.status.idle": "2023-08-27T15:50:59.778955Z",
     "shell.execute_reply": "2023-08-27T15:50:59.777505Z",
     "shell.execute_reply.started": "2023-08-27T15:50:58.954877Z"
    },
    "id": "fcqBX74m-kk9"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronqué\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict_test = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits pour chaque patient\n",
    "for key, value in reduced_embeddings_dict_test.items():\n",
    "    # Convertir le numpy array en un torch tensor et spécifier le type comme float\n",
    "    tensor = torch.from_numpy(value).float()\n",
    "    # Alternative : utiliser directement torch.tensor(value, dtype=torch.float)\n",
    "\n",
    "    # Ajouter une dimension supplémentaire pour correspondre à l'exigence du modèle (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Calculer la log signature en utilisant le modèle signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension batch que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0)\n",
    "\n",
    "    # Ajouter le résultat au dictionnaire avec le numéro du patient comme clé\n",
    "    log_signature_dict_test[key] = log_signature\n",
    "\n",
    "# Maintenant, log_signature_dict est un dictionnaire où chaque clé correspond à un patient et chaque valeur est la log signature de ce sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgoeroMB-kk-"
   },
   "source": [
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:09:07.874069Z",
     "iopub.status.busy": "2023-08-27T16:09:07.873428Z",
     "iopub.status.idle": "2023-08-27T16:09:07.883912Z",
     "shell.execute_reply": "2023-08-27T16:09:07.882548Z",
     "shell.execute_reply.started": "2023-08-27T16:09:07.874025Z"
    },
    "id": "b74xdF2K-kk-",
    "outputId": "a38e34db-3786-4170-ea4c-4ddb26ba7da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5153"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 1 pour la colonne 'HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation données pré-traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "193    0\n",
      "194    0\n",
      "195    1\n",
      "196    1\n",
      "197    1\n",
      "Name: HOSPITAL_EXPIRE_FLAG, Length: 198, dtype: int64\n",
      "1\n",
      "         0         1         2         3         4         5         6     \\\n",
      "0    1.580720  4.185603  3.028620 -2.008586 -0.222012  2.379665 -1.819452   \n",
      "1    2.531985 -0.251433  1.891096 -4.173684  0.221094 -0.379147 -1.663596   \n",
      "2   -0.402699  3.509899 -0.532081 -0.726203 -0.759488  1.675094  0.747753   \n",
      "3    3.631563  0.292610 -0.743517 -1.990374  0.866655  0.956446  1.635095   \n",
      "4    3.383871  0.753373  0.661267 -3.734596 -0.763109 -0.662084 -0.313718   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "194  4.922768 -1.707666  1.432679 -4.734762  0.131282 -0.784450 -1.184537   \n",
      "195 -2.763147  1.812479 -0.873362 -3.612413 -0.319842  0.420706 -1.704986   \n",
      "196  4.234116  2.549886  2.089611 -3.569138  2.195544  3.792971 -2.411561   \n",
      "197 -3.576301 -1.324811  4.367753 -5.521049  2.431141 -0.538935 -1.571341   \n",
      "198 -0.488449  1.159878 -0.381321 -4.396064 -0.087925  0.447312 -0.893212   \n",
      "\n",
      "         7         8         9     ...      5141      5142      5143  5144  \\\n",
      "0   -2.835571  2.409048  1.910630  ... -0.110682 -0.027158  0.209085   0.0   \n",
      "1   -2.655509  0.047621  0.152332  ... -0.220794  0.074353 -0.168205   0.0   \n",
      "2   -0.011716 -1.794618 -0.950105  ... -0.080861  0.001826 -0.022156   0.0   \n",
      "3    2.515238 -2.263048 -0.496285  ... -0.000131 -0.026833 -0.002450   0.0   \n",
      "4   -2.516659 -1.983624  1.076588  ...  0.076034  0.105143  0.149733   0.0   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   ...   \n",
      "194 -2.091893 -0.318641 -0.511706  ... -0.050212  0.251884 -0.405348   0.0   \n",
      "195 -1.942868 -0.684134 -0.437581  ...  0.050158  0.047346 -0.034698   0.0   \n",
      "196 -0.309529 -1.436482 -0.053828  ... -0.276468  0.014451  0.394986   0.0   \n",
      "197 -1.462438 -0.609520  0.077332  ... -0.112016 -0.157616  0.087052   0.0   \n",
      "198  0.050359 -1.651188  0.209203  ...  0.102886 -0.099615 -0.092541   0.0   \n",
      "\n",
      "         5145      5146  5147      5148  5149  5150  \n",
      "0   -0.039699 -0.004927   0.0  0.048453   0.0   0.0  \n",
      "1    0.216969  0.040551   0.0 -0.105154   0.0   0.0  \n",
      "2    0.136290  0.053633   0.0  0.067649   0.0   0.0  \n",
      "3   -0.031397 -0.002259   0.0  0.015904   0.0   0.0  \n",
      "4    0.108327 -0.112366   0.0  0.057479   0.0   0.0  \n",
      "..        ...       ...   ...       ...   ...   ...  \n",
      "194  0.113633 -0.086874   0.0 -0.065135   0.0   0.0  \n",
      "195  0.055473  0.006402   0.0  0.042133   0.0   0.0  \n",
      "196 -0.088317  0.025413   0.0  0.092453   0.0   0.0  \n",
      "197  0.355021  0.303645   0.0 -0.206883   0.0   0.0  \n",
      "198  0.092437 -0.074347   0.0 -0.014179   0.0   0.0  \n",
      "\n",
      "[199 rows x 5151 columns]\n",
      "0       3.430311\n",
      "1       0.186055\n",
      "2       1.499623\n",
      "3      -2.751001\n",
      "4       0.487114\n",
      "          ...   \n",
      "5146    0.027808\n",
      "5147    0.000000\n",
      "5148   -0.060000\n",
      "5149    0.000000\n",
      "5150    0.000000\n",
      "Name: 0, Length: 5151, dtype: float64\n",
      "200\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/traite/x_train', 'rb') as f1:\n",
    "    X_train = pickle.load(f1)\n",
    "with open('data/traite/x_test', 'rb') as f1:\n",
    "    X_test = pickle.load(f1)\n",
    "with open('data/traite/y_train', 'rb') as f1:\n",
    "    y_train = pickle.load(f1).astype(int)\n",
    "with open('data/traite/y_test', 'rb') as f1:\n",
    "    y_test = pickle.load(f1).astype(int)\n",
    "\n",
    "print(y_train.iloc[:-1])\n",
    "print(y_train.iloc[-1])\n",
    "print(X_train)\n",
    "print(X_test.iloc[0])\n",
    "X = pd.concat([X_train, X_test.iloc[:1]])\n",
    "print(len(X))\n",
    "print(len(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "# Reg Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VFYF2nn-kk-"
   },
   "source": [
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 65.00%\n",
      "Rappel: 66.67%\n",
      "F1-score: 36.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "\"\"\"X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\"\"\"\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + small sets pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical coverage is: 0.92\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 50 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = model.predict_proba(X_test)\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + meilleur méthode ? pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(50, 2)\n",
      "The empirical coverage is: 0.92\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/ipykernel_17957/3375603569.py:32: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 50 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = model.predict_proba(X_test)\n",
    "\n",
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode full conformal + small sets pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000)\n",
    "\n",
    "def full_conformal(k):\n",
    "    alpha = 0.1 # 1-alpha is the desired coverage\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(5):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAFUB5vy-kk_"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGeOedqw-kk_"
   },
   "source": [
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T21:31:54.195444Z",
     "iopub.status.busy": "2023-08-26T21:31:54.194905Z",
     "iopub.status.idle": "2023-08-26T21:34:30.027397Z",
     "shell.execute_reply": "2023-08-26T21:34:30.025943Z",
     "shell.execute_reply.started": "2023-08-26T21:31:54.195399Z"
    },
    "id": "AYUwBEIa-kk_",
    "outputId": "8ba3b6c2-e274-4acd-d3a0-899e27ed97d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 53.00%\n",
      "Rappel: 73.33%\n",
      "F1-score: 31.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "\"\"\"# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\"\"\"\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "best_model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + small sets pour random  forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical coverage is: 0.98\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 50 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = best_model.predict_proba(X_test)\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + meilleur méthode ? pour random  forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "The empirical coverage is: 0.88\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/js67_bvd59xfmgr535vjhyzh0000gn/T/ipykernel_17957/3822215827.py:33: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 50 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = best_model.predict_proba(X_test)\n",
    "\n",
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode full conformal + small sets pour random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "\n",
    "def full_conformal(k):\n",
    "    alpha = 0.1 # 1-alpha is the desired coverage\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB3vSRvc-kk_"
   },
   "source": [
    "# Sauvegarder les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KdnmRvR-kk_"
   },
   "source": [
    "Nous procédons à une sauvegarde des données en enregistrant les plongements traités ainsi que les valeurs temporelles dans des fichiers JSON et Pickle, afin de pouvoir les utiliser ou les analyser ultérieurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:12:12.863066Z",
     "iopub.status.busy": "2023-08-09T18:12:12.861870Z",
     "iopub.status.idle": "2023-08-09T18:12:37.434603Z",
     "shell.execute_reply": "2023-08-09T18:12:37.432844Z",
     "shell.execute_reply.started": "2023-08-09T18:12:12.863011Z"
    },
    "id": "5oTPx7uX-kk_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis convertis en listes\n",
    "flattened_embeddings_dict_converted = {}\n",
    "\n",
    "# Parcourir le dictionnaire flattened_embeddings_dict_test\n",
    "for clé, valeur in flattened_embeddings_dict_test.items():\n",
    "    # Convertir chaque valeur (numpy array) en liste et stocker dans le nouveau dictionnaire\n",
    "    flattened_embeddings_dict_converted[clé] = valeur.tolist()\n",
    "\n",
    "# Sauvegarder le dictionnaire converti en tant que fichier JSON\n",
    "\n",
    "# Spécifier le chemin du fichier JSON où vous souhaitez enregistrer les données\n",
    "with open('embeddings_dict_test.json', 'w') as fichier:\n",
    "    # Utiliser la bibliothèque JSON pour écrire le dictionnaire converti dans le fichier JSON\n",
    "    json.dump(flattened_embeddings_dict_converted, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:13:16.081552Z",
     "iopub.status.busy": "2023-08-09T18:13:16.081068Z",
     "iopub.status.idle": "2023-08-09T18:13:16.091682Z",
     "shell.execute_reply": "2023-08-09T18:13:16.090132Z",
     "shell.execute_reply.started": "2023-08-09T18:13:16.081516Z"
    },
    "id": "OZs6btSi-klA"
   },
   "outputs": [],
   "source": [
    "# Importer le module pour la serialization des objets\n",
    "import pickle\n",
    "\n",
    "# Sauvegarder la liste dans un fichier binaire (pickle)\n",
    "with open('time_list_test.pkl', 'wb') as f:\n",
    "    # Utiliser la bibliothèque pickle pour écrire la liste dans le fichier binaire\n",
    "    pickle.dump(time_list_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "id": "TLL-YYePVIg6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (7.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6; python_version < \"3.10\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (6.7.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.7.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (4.12.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (4.12.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (6.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (3.1.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (5.8.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (2.1.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (2.17.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (5.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (23.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (0.2.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=3.6; python_version < \"3.10\"->nbconvert) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=3.6; python_version < \"3.10\"->nbconvert) (4.7.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (7.4.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.4.1)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (306)\n",
      "Requirement already satisfied: webencodings in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbformat>=5.7->nbconvert) (2.19.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nbformat>=5.7->nbconvert) (4.17.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.5.8)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (0.4)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.19.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (5.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (23.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nbconvert "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
