{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Prédiction conforme pour les données textuelles analysées par Transformers : application sur rapports médicaux\n",
    "\n",
    "Benoliel Stuart\n",
    "\\\n",
    "Carrere Charles\n",
    "\\\n",
    "Thomas Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version\n",
    "\n",
    "# Importer les bibliothèques nécessaires\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 1\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seulement si besoin d'entrainer le Bert\n",
    "%pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas transformers\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install quantile-forest\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"# Répertoire actuel\n",
    "current_directory = os.getcwd()\n",
    "print(\"Vous êtes dans le répertoire:\", current_directory)\n",
    "\n",
    "# Accéder au répertoire parent (A)\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Changer de répertoire de travail vers le répertoire parent\n",
    "os.chdir(parent_directory)\n",
    "# Vérifier le nouveau répertoire\n",
    "new_directory = os.getcwd()\n",
    "print(\"Vous êtes maintenant dans le répertoire:\", new_directory)\"\"\"\n",
    "\n",
    "pth = 'data/mimiciii'\n",
    "files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données\n",
    "Dans cette section, nous chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(data)\n",
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,:]['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtaj1bVu-kky"
   },
   "outputs": [],
   "source": [
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "# Convertir la colonne 'TIME' en entiers\n",
    "# data['TIME'] = data['TIME'].astype(int)\n",
    "data['TIME'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(adm)\n",
    "adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITTIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMTTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DELTATIME'] = (adm['DISCHTIME'] - adm['ADMITTIME']).dt.total_seconds() / 3600\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm.drop(adm[adm['DELTATIME'] < 0].index)\n",
    "adm = adm[adm[\"SUBJECT_ID\"].isin(data[\"SUBJECT_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement\n",
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T13:22:27.799253Z",
     "iopub.status.busy": "2023-08-27T13:22:27.798779Z",
     "iopub.status.idle": "2023-08-27T13:22:41.546351Z",
     "shell.execute_reply": "2023-08-27T13:22:41.544957Z",
     "shell.execute_reply.started": "2023-08-27T13:22:27.799212Z"
    },
    "id": "f3I6vt3q-kk1"
   },
   "outputs": [],
   "source": [
    "# Créer un dataframe vide avec deux colonnes pour le nouveau dataframe\n",
    "new_data = pd.DataFrame(columns=[\"SUBJECT_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
    "\n",
    "# Grouper les données d'admission par \"SUBJECT_ID\"\n",
    "grouped_adm = adm.groupby(\"SUBJECT_ID\")\n",
    "print(len(grouped_adm))\n",
    "\n",
    "# Parcourir chaque groupe de données associées à un \"SUBJECT_ID\"\n",
    "for subject_id, group in grouped_adm:\n",
    "    # Vérifier si le groupe contient au moins un enregistrement avec la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    if group[\"HOSPITAL_EXPIRE_FLAG\"].eq(1).any():\n",
    "        # Si oui, ajouter le \"SUBJECT_ID\" et la valeur 1 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [1]})], ignore_index=True)\n",
    "    else:\n",
    "        # Sinon, ajouter le \"SUBJECT_ID\" et la valeur 0 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [0]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "## Fonction pour diviser les documents en plus petits morceaux\n",
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## Charger le modèle ClinicalBERT depuis Hugging Face\n",
    "Dans cette partie, nous chargeons le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [],
   "source": [
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0 ???\n",
    "label_0 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\n",
    "sample = pd.concat([label_1.sample(n=1000, random_state=seed), label_0.sample(n=1000, random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\n",
    "filtered_data = data[data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'SUBJECT_ID' en agrégeant les listes de 'TEXT' et 'TIME'\n",
    "grouped_sample = filtered_data.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les données test (new_data) pour ne conserver que les patients absents\n",
    "new_data_test = new_data[~new_data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1.sample(n=150, random_state=seed), label_0.sample(n=850, random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"SUBJECT_ID\"].isin(sample_test[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## Extraire les tokens CLS\n",
    "Dans cette partie, le code se focalise sur l'extraction des embeddings ClinicalBERT. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Le résultat est un dictionnaire qui associe chaque patient à ses embeddings ClinicalBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token(grouped_sample):\n",
    "    # Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "    grouped_texts_dict = grouped_sample.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "    # Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "    time_list = []\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les embeddings\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    # Parcourir les patients et leurs données associées\n",
    "    for subject_id, values in grouped_texts_dict.items():\n",
    "        texts = values['TEXT']  # Récupérer la liste des documents\n",
    "        times = values['TIME']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "        embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "        # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "        for text, time in zip(texts, times):\n",
    "            # Diviser le texte en parties égales\n",
    "            encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "            n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "            n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "            parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "            # Stocker les embeddings des différentes parties du document\n",
    "            cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "\n",
    "            # Parcourir les parties du document\n",
    "            for partie in parties:\n",
    "                # Convertir la partie dans un format compatible avec le modèle\n",
    "                inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                    outputs = model(**inputs)\n",
    "\n",
    "                # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                # Stocker l'embedding dans la liste\n",
    "                cls_embeddings_list.append(cls_embeddings)\n",
    "                # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "                time_list.append(time)\n",
    "\n",
    "            # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "            embeddings_list += cls_embeddings_list\n",
    "\n",
    "        # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "        embeddings_dict[subject_id] = torch.stack(embeddings_list)\n",
    "    return embeddings_dict , time_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [],
   "source": [
    "embeddings_dict, time_list = extract_token(grouped_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_test, time_list_test = extract_token(grouped_sample_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## Réduction de dimension\n",
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### Projection gaussienne aléatoire\n",
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "def project_gaussian(embeddings_dict, time_list):\n",
    "\n",
    "    # ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "    # Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings pour créer une matrice unique\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "    transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "    # Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "    reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "    # Diviser les embeddings réduits pour chaque patient\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    return reduced_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = project_gaussian(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = project_gaussian(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### ACP\n",
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def ACP(embeddings_dict, time_list):\n",
    "    # Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings en une seule matrice\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "    pca = PCA(n_components=100)\n",
    "\n",
    "    # Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "    reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    # Séparer les embeddings transformés pour chaque sujet\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    # Afficher la variance expliquée par chaque composante principale\n",
    "    print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "    # Afficher la variance totale expliquée par toutes les composantes principales\n",
    "    print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "    return reduced_embeddings_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = ACP(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = ACP(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## Calculer les signatures\n",
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import signatory\n",
    "\n",
    "def log_signa(reduced_embeddings_dict):\n",
    "\n",
    "    # Ordre de la signature tronquée\n",
    "    depth = 2\n",
    "\n",
    "    # Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "    log_signature_dict = {}\n",
    "\n",
    "    # Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "    for key, value in reduced_embeddings_dict.items():\n",
    "        # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "        tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "\n",
    "        # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "        tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "        log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "        # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "        log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "\n",
    "        # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "        log_signature_dict[key] = log_signature\n",
    "    return log_signature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [],
   "source": [
    "log_signature_dict = log_signa(reduced_embeddings_dict)\n",
    "# À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n",
    "print(log_signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_signature_dict_test = log_signa(reduced_embeddings_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement\n",
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "df_final.shape\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test\n",
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 1 pour la colonne 'HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement données traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'wb') as f1:\n",
    "    data_train = pickle.dump(df_final, f1)\n",
    "with open('data/traite/data_test', 'wb') as f1:\n",
    "    data_test = pickle.dump(df_final_test, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation données pré-traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'rb') as f1:\n",
    "    df_final = pickle.load(f1)\n",
    "with open('data/traite/data_test', 'rb') as f1:\n",
    "    df_final_test = pickle.load(f1)\n",
    "\n",
    "print(\"Dataset: %s\" % (df_final.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (df_final_test.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_cqr', 'rb') as f1:\n",
    "    pred_cqr = pickle.load(f1)\n",
    "with open('data/traite/pred_rf', 'rb') as f1:\n",
    "    pred_rf = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cqr', 'rb') as f1:\n",
    "    y_lower_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cqr', 'rb') as f1:\n",
    "    y_upper_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cvp', 'rb') as f1:\n",
    "    y_lower_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cvp', 'rb') as f1:\n",
    "    y_upper_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_lower_split', 'rb') as f1:\n",
    "    y_lower_split = pickle.load(f1)\n",
    "with open('data/traite/y_upper_split', 'rb') as f1:\n",
    "    y_upper_split = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "# Reg Logistique\n",
    "\n",
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000, random_state = seed)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_lr = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs prédit (Vrai Positif + Faux Positif).\n",
    "# Plus elle est élevé, plus le modèle de Machine Learning minimise le nombre de Faux Positif.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle = Sensibilité\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "# Quand le recall est haut, cela veut plutôt dire qu’il ne ratera aucun positif.\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(matrix)\n",
    "\n",
    "# 3min\n",
    "# Précision: 59.42%\n",
    "# Rappel: 58.67%\n",
    "# F1-score: 30.29%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, random_state = seed)\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features, random_state = seed)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_rf = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "# 14 min\n",
    "# Précision: 46.09%\n",
    "# Rappel: 71.33%\n",
    "# F1-score: 28.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison performances des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer le taux de faux positifs (FPR), le taux de vrais positifs (TPR) et les seuils\n",
    "fpr_lr, tpr_lr, thresholds = roc_curve(y_test, smx_lr[:, 1])\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(y_test, smx_rf[:, 1])\n",
    "# Calculer l'aire sous la courbe ROC (AUC)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label='Logistic regression (AUC = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_rf, tpr_rf, color='lightblue', lw=2, label='Random forest (AUC = %0.2f)' % roc_auc_rf)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate = 1 - Spécificité')\n",
    "plt.ylabel('True Positive Rate = Sensibilité')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"fig/classi/roc.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des seuils de classification pour lesquels on va calculer\n",
    "# les f1 scores\n",
    "threshold_array = np.linspace(0, 1, 100)\n",
    "f1_list_lr = []\n",
    "f1_list_rf = []\n",
    "\n",
    "# Boucle pour calculer le f1 score pour chaque seuil\n",
    "for threshold in threshold_array:\n",
    "    # Prédiction des étiquettes pour un seuil donné\n",
    "    pred_threshold_lr = (smx_lr[:, 1] > threshold).astype(int)\n",
    "    pred_threshold_rf = (smx_rf[:, 1] > threshold).astype(int)\n",
    "    \n",
    "    # Calcul du f1 score pour un seuil donné\n",
    "    f1_threshold_lr = f1_score(y_true=y_test, y_pred=pred_threshold_lr)\n",
    "    f1_threshold_rf = f1_score(y_true=y_test, y_pred=pred_threshold_rf)\n",
    "    \n",
    "    # Ajout du f1 score à la liste\n",
    "    f1_list_lr.append(f1_threshold_lr)\n",
    "    f1_list_rf.append(f1_threshold_rf)\n",
    "\n",
    "# Tracer la courbe des f1 scores en fonction des seuils\n",
    "plt.plot(threshold_array, f1_list_lr, color='darkorange', lw=2,label='Logistic regression')\n",
    "plt.plot(threshold_array, f1_list_rf, color='lightblue', lw=2,label='Random forest')\n",
    "plt.xlabel('Seuil score')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"fig/classi/f1_score.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction conforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = model.predict_proba(X_test)\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_small_sets_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + meilleur RAPS (Random Adaptative Prediction Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_raps_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def full_conformal(k):\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(5):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remplacement de la variable 0-1 par temps de séjour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join('data/mimiciii', 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train = df_final.iloc[:, 0:-1]\n",
    "X_train = pd.merge(X_train, adm, on='SUBJECT_ID', how='left')\n",
    "X_train['ADMITTIME'] = pd.to_datetime(X_train['ADMITTIME'])\n",
    "X_train['DISCHTIME'] = pd.to_datetime(X_train['DISCHTIME'])\n",
    "X_train['DELTATIME'] = (X_train['DISCHTIME'] - X_train['ADMITTIME']).dt.total_seconds() / 3600\n",
    "X_train = X_train.drop(X_train[X_train['DELTATIME'] < 0].index)\n",
    "\n",
    "y_train = X_train['DELTATIME']\n",
    "X_train = X_train.iloc[:, 1:5152]\n",
    "\n",
    "X_test = df_final_test.iloc[:, 0:-1]\n",
    "X_test = pd.merge(X_test, adm, on='SUBJECT_ID', how='left')\n",
    "X_test['ADMITTIME'] = pd.to_datetime(X_test['ADMITTIME'])\n",
    "X_test['DISCHTIME'] = pd.to_datetime(X_test['DISCHTIME'])\n",
    "X_test['DELTATIME'] = (X_test['DISCHTIME'] - X_test['ADMITTIME']).dt.total_seconds() / 3600\n",
    "X_test = X_test.drop(X_test[X_test['DELTATIME'] < 0].index)\n",
    "\n",
    "y_test = X_test['DELTATIME']\n",
    "X_test = X_test.iloc[:, 1:5152]\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "\n",
    "# Split the data into calibration and validation sets (save the shuffling)\n",
    "idx = np.array([1] * n + [0] * (X_test.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=seed)\n",
    "# divide the data into proper training set and calibration set\n",
    "# idx = np.random.permutation(n_train)\n",
    "# n_half = int(np.floor(n_train/2))\n",
    "# idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(y,\n",
    "              y_u=None,\n",
    "              y_l=None,\n",
    "              pred=None,\n",
    "              shade_color=\"\",\n",
    "              method_name=\"\",\n",
    "              title=\"\",\n",
    "              filename=None,\n",
    "              save_figures=False,\n",
    "              max_show=100):\n",
    "    \n",
    "    \"\"\" Scatter plot of (x,y) points along with the constructed prediction interval \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array, target response variable (length n)\n",
    "    pred : numpy array, the estimated prediction. It may be the conditional mean,\n",
    "           or low and high conditional quantiles.\n",
    "    shade_color : string, desired color of the prediciton interval\n",
    "    method_name : string, name of the method\n",
    "    title : string, the title of the figure\n",
    "    filename : sting, name of the file to save the figure\n",
    "    save_figures : boolean, save the figure (True) or not (False)\n",
    "    \n",
    "    \"\"\"\n",
    "    y_ = y[:max_show]\n",
    "    x_ = np.arange(1, len(y_) + 1)\n",
    "\n",
    "    if y_u is not None:\n",
    "        y_u_ = y_u[:max_show]\n",
    "    if y_l is not None:\n",
    "        y_l_ = y_l[:max_show]\n",
    "    if pred is not None:\n",
    "        pred_ = pred[:max_show]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x_, y_, 'k.', alpha=.2, markersize=10,\n",
    "             fillstyle='none', label=u'Observations')\n",
    "    \n",
    "    if (y_u is not None) and (y_l is not None):\n",
    "        plt.fill(np.concatenate([x_, x_[::-1]]),\n",
    "                 np.concatenate([y_u_, y_l_[::-1]]),\n",
    "                 alpha=.3, fc=shade_color, ec='None',\n",
    "                 label = method_name + ' prediction interval')\n",
    "    \n",
    "    if pred is not None:\n",
    "        if pred_.ndim == 2:\n",
    "            plt.plot(x_, pred_[:,0], 'k', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted low and high quantiles')\n",
    "            plt.plot(x_, pred_[:,1], 'k', lw=2, alpha=0.9)\n",
    "        else:\n",
    "            plt.plot(x_, pred_, 'k--', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted value')\n",
    "    \n",
    "    plt.xlabel('$X$')\n",
    "    plt.ylabel('$Y$')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_hist(length,\n",
    "              in_the_range,\n",
    "              x_name=\"\",\n",
    "              dec_x_quant=[0,0,0],\n",
    "              draw_quant=True,\n",
    "              filename=None,\n",
    "              save_figures=False):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.histplot(data=df, x='length', hue='in_the_range', multiple=\"stack\", stat='percent',\n",
    "        palette={True: 'green', False: 'red'}, bins=20, hue_order=[True, False])\n",
    "    sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=2, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "    if draw_quant:\n",
    "        # Calcul des quantiles\n",
    "        quantiles = [np.percentile(length, i) for i in range(25, 100, 25)]\n",
    "\n",
    "        # Calcul de la hauteur maximale pour les barres verticales\n",
    "        ymax = plt.ylim()[1] \n",
    "\n",
    "        # Tracé des lignes verticales pour les quantiles et ajout des étiquettes\n",
    "        for i, quantile in enumerate(quantiles, 1):\n",
    "            plt.vlines(x=quantile, ymin=0, ymax=ymax*9/10 , linestyle='--', linewidth=1.5, color='black')\n",
    "            plt.text(quantile+dec_x_quant[i-1], ymax*9/10, f'Q{i}:\\n{round(quantile)}', va='bottom', ha='center', color='black', fontsize=9, rotation=0)\n",
    "\n",
    "    plt.xlabel(x_name)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Notice that the random forests regressor **estimates the conditional mean** of $Y_i$ given $X_i=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import math\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, random_state = seed)\n",
    "\n",
    "# Définir la métrique MSE comme score\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=mse_scorer)\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, max_features=best_max_features, random_state = seed)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE sur les données de test:\", mse)\n",
    "\n",
    "# MSE sur les données de test: 85268.35571631312\n",
    "# 42 min / 70 min machine perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import AbsErrorErrFunc\n",
    "\n",
    "# define a conformal prediction object \n",
    "nc = RegressorNc(model, AbsErrorErrFunc())\n",
    "\n",
    "# build a regualr split conformal prediction object \n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# fit the conditional mean regression to the proper training data\n",
    "icp.fit(X_train, y_train)\n",
    "\n",
    "# compute the absolute residual error on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower = np.maximum(predictions[:, 0], 0)\n",
    "y_upper = predictions[:,1]\n",
    "\n",
    "# compute the conditional mean estimation\n",
    "pred_rf = model.predict(X_test[~idx])\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_split = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"Random Forests: Percentage in the range (expecting \" + str(100*(1-alpha)) + \"%):\",\n",
    "      np.sum(in_the_range_split) / len(y_test[~idx]) * 100)\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_split = y_upper - y_lower\n",
    "\n",
    "# compute and display the average length\n",
    "print(\"Random Forests: Average length:\", np.mean(length_split))\n",
    "\n",
    "# 1min\n",
    "# Random Forests: Percentage in the range (expecting 90.0%): 90.53333333333333\n",
    "# Random Forests: Average length: 703.0612197403849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_rf,shade_color='tomato',\n",
    "          method_name=\"Split:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_split.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_split, in_the_range = in_the_range_split, x_name=\"Length Split\",\n",
    "      filename=\"fig/regres/histo_length_split.png\", save_figures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pred_rf = pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_split', 'wb') as f1:\n",
    "    y_upper_split = pickle.dump(y_upper, f1)\n",
    "with open('data/traite/y_lower_split', 'wb') as f1:\n",
    "    y_lower_split = pickle.dump(y_lower, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV+ for K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "X_train_K = np.concatenate((X_train, X_test[idx]), axis=0)\n",
    "y_train_K = np.concatenate((y_train, y_test[idx]), axis=0)\n",
    "\n",
    "# Initialisez l'objet KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Initialisez une liste pour stocker les index des échantillons d'entraînement\n",
    "residus = np.zeros(len(X_train_K))\n",
    "pred = np.zeros((len(X_train_K),len(X_test[~idx])))\n",
    "\n",
    "# Parcourez les splits et stockez les index des échantillons d'entraînement\n",
    "for train_indices, test_indices in kf.split(X_train_K):\n",
    "    model.fit(X_train_K[train_indices], y_train_K[train_indices])\n",
    "    residus[test_indices] = abs(y_train_K[test_indices]-model.predict(X_train_K[test_indices]))\n",
    "    pred[test_indices] = np.repeat(model.predict(X_test[~idx]).reshape(1, -1), len(test_indices), axis=0)\n",
    "\n",
    "# display the results\n",
    "\n",
    "y_upper = np.quantile(pred+residus.reshape(-1, 1), q_level, method='higher', axis=0)\n",
    "y_lower = np.maximum(- np.quantile(-pred+residus.reshape(-1, 1), q_level, method='higher', axis=0), 0)\n",
    "\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, max_features=best_max_features, random_state = seed)\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "pred_rf = model.predict(X_test[~idx])\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_cvp = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"Random Forests: Percentage in the range (expecting \" + str(100*(1-alpha)) + \"%):\",\n",
    "      np.sum(in_the_range_cvp) / len(y_test[~idx]) * 100)\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_cvp = y_upper - y_lower\n",
    "\n",
    "# compute and display the average length\n",
    "print(\"Random Forests: Average length:\", np.mean(length_cvp))\n",
    "\n",
    "# 2 min\n",
    "# Random Forests: Percentage in the range (expecting 90.0%): 91.86666666666666\n",
    "# Random Forests: Average length: 622.246617064222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_rf,shade_color='gray',\n",
    "          method_name=\"CV+ K folder:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_cvp.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cvp, in_the_range = in_the_range_cvp, x_name=\"Length CV+ K folder\", dec_x_quant= [-15,0,0],\n",
    "        filename=\"fig/regres/histo_length_cvp.png\", save_figures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pred_rf = pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_cvp', 'wb') as f1:\n",
    "    y_upper_cvp = pickle.dump(y_upper, f1)\n",
    "with open('data/traite/y_lower_cvp', 'wb') as f1:\n",
    "    y_lower_cvp = pickle.dump(y_lower, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQR Random Forests\n",
    "\n",
    "Given any quantile regression algorithm $\\mathcal{A}$ (the function `QuantileForestRegressorAdapter` in the code below), we then fit two conditional quantile functions $\\hat{q}_{\\alpha_{lo}}$ and $\\hat{q}_{\\alpha_{hi}}$ on the proper training set: $$ \\{ \\hat{q}_{\\alpha_{lo}}, \\hat{q}_{\\alpha_{hi}} \\} \\leftarrow \\mathcal{A}(\\left\\lbrace (X_i, Y_i): i \\in I_1 \\right\\rbrace). $$\n",
    "This is done by calling the function `icp.fit`.\n",
    "\n",
    "In the essential next step, the function `icp.calibrate` computes conformity scores (using `QuantileRegErrFunc`) that quantify the error made by the plug-in prediction interval $ \\hat{C}(x) = [\\hat{q}_{\\alpha_{lo}}(x), \\ \\hat{q}_{\\alpha_{hi}}(x)]  $. The scores are evaluated on the calibration set as\n",
    "$$\n",
    "\tE_i := \\max\\{\\hat{q}_{\\alpha_{lo}}(X_i) - Y_i, Y_i - \\hat{q}_{\\alpha_{hi}}(X_i)\\},\n",
    "$$\n",
    "for each $i \\in I_2$. The conformity score $E_i$ has the following interpretation. If $Y_i$ is below the lower endpoint of the interval, $Y_i < \\hat{q}_{\\alpha_{lo}}(X_i)$, then $E_i = |Y_i - \\hat{q}_{\\alpha_{lo}}(X_i)|$ is the magnitude of the error incurred by this mistake. Similarly, if $Y_i$ is above the upper endpoint of the interval, $Y_i > \\hat{q}_{\\alpha_{hi}}(X_i)$, then $E_i = |Y_i - \\hat{q}_{\\alpha_{hi}}(X_i)|$. Finally, if $Y_i$ correctly belongs to the interval, $\\hat{q}_{\\alpha_{lo}}(X_i) \\leq Y_i \\leq \\hat{q}_{\\alpha_{hi}}(X_i)$, then $E_i$ is the larger of the two non-positive numbers $\\hat{q}_{\\alpha_{lo}}(X_i) - Y_i$ and $Y_i - \\hat{q}_{\\alpha_{hi}}(X_i)$ and so is itself non-positive. The conformity score thus accounts for both undercoverage and overcoverage.\n",
    "\n",
    "Finally, given new input data $X_{n+1}$, we construct the prediction interval for $Y_{n+1}$ as\n",
    "$$\n",
    "C(X_{n+1}) = \\left[ \\hat{q}_{\\alpha_{lo}}(X_{n+1}) - Q_{1-\\alpha}(E, I_2) , \\ \\hat{q}_{\\alpha_{hi}}(X_{n+1}) + Q_{1-\\alpha}(E, I_2) \\right],\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Q_{1-\\alpha}(E, I_2) :=  (1-\\alpha)(1+1/|I_2|)\\text{-th empirical quantile of} \\left\\{E_i : i \\in I_2\\right\\}\n",
    "$$\n",
    "conformalizes the plug-in prediction interval. This is done by calling the function `icp.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install setuptools numpy scipy scikit-learn cython\n",
    "# pip3 install scikit-garden python version 3.7 (code à modifier)\n",
    "# OU pip3 install quantile_forest\n",
    "\n",
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import QuantileRegErrFunc, QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 150\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = X_train.shape[1]\n",
    "\n",
    "# target quantile levels\n",
    "# desired quanitile levels\n",
    "quantiles = [5, 95]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 0.80 #0.85\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = seed\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 10\n",
    "\n",
    "# define quantile random forests (QRF) parameters\n",
    "params_qforest = dict()\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"max_features\"] = max_features\n",
    "params_qforest[\"CV\"] = cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"] = cv_test_ratio\n",
    "params_qforest[\"random_state\"] = cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals\n",
    "\n",
    "# define the QRF model\n",
    "model = helper.QuantileForestRegressorAdapter(model=None,fit_params=None,\n",
    "                                                quantiles=quantiles,\n",
    "                                                params=params_qforest)\n",
    "\n",
    "# define the CQR object, computing the absolute residual error of points \n",
    "# located outside the estimated QRF band \n",
    "nc = RegressorNc(model, QuantileRegErrFunc()) # ou QuantileRegAsymmetricErrFunc / QuantileRegErrFunc\n",
    "\n",
    "# build the split CQR object\n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# fit the conditional quantile regression to the proper training data\n",
    "icp.fit(X_train, y_train)\n",
    "# 77 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the absolute errors on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower = np.maximum(predictions[:, 0], 0)\n",
    "y_upper = predictions[:,1]\n",
    "\n",
    "# compute the low and high conditional quantile estimation\n",
    "pred_cqr = model.predict(X_test[~idx])\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_cqr = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"CQR Random Forests: Percentage in the range (expecting \" + str(100*(1-alpha)) + \"%):\",\n",
    "      np.sum(in_the_range_cqr) / len(y_test[~idx]) * 100)\n",
    "\n",
    "# compute length of the conformal interval per each test point\n",
    "length_cqr = y_upper - y_lower\n",
    "\n",
    "# compute and display the average length\n",
    "print(\"CQR Random Forests: Average length:\", np.mean(length_cqr))\n",
    "\n",
    "# CQR Random Forests: Percentage in the range (expecting 90.0%): 91.2\n",
    "# CQR Random Forests: Average length: 775.5206499999995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_cqr,shade_color='lightblue',\n",
    "          method_name=\"CQR:\",title=\"CQR Random Forests (quantile regression)\",\n",
    "          filename=\"fig/regres/lineplot_cqr.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cqr, in_the_range = in_the_range_cqr, x_name=\"Length CQR\", dec_x_quant= [-75,70,0],\n",
    "        filename=\"fig/regres/histo_length_cqr.png\", save_figures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_cqr', 'wb') as f1:\n",
    "    pred_cqr = pickle.dump(pred_cqr, f1)\n",
    "with open('data/traite/y_upper_cqr', 'wb') as f1:\n",
    "    y_upper_cqr = pickle.dump(y_upper, f1)\n",
    "with open('data/traite/y_lower_cqr', 'wb') as f1:\n",
    "    y_lower_cqr = pickle.dump(y_lower, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration du meilleur intervalle de confiance (approximativement symmétrique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower = y_lower_split.copy()\n",
    "y_upper = y_upper_split.copy()\n",
    "\n",
    "for i in range(len(y_test[~idx])):\n",
    "    if pred_rf[i] < y_test[~idx].iloc[i]:\n",
    "        y_lower[i] = max(pred_rf[i] - y_test[~idx].iloc[i]/2,0)\n",
    "        y_upper[i] = y_test[~idx].iloc[i]\n",
    "    else:\n",
    "        y_lower[i] = max(y_test[~idx].iloc[i],0)\n",
    "        y_upper[i] = pred_rf[i] + y_test[~idx].iloc[i]/2\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"Random Forests: Percentage in the range (expecting \" + str(100*(1-alpha)) + \"%):\",\n",
    "      np.sum(in_the_range) / len(y_test[~idx]) * 100)\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length = y_upper - y_lower\n",
    "# compute and display the average length\n",
    "print(\"Random Forests: Average length:\", np.mean(length))\n",
    "\n",
    "# Random Forests: Percentage in the range (expecting 90.0%): 100.0\n",
    "# Random Forests: Average length: 292.23292521108556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_rf,shade_color='green',\n",
    "          method_name=\"Best:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_best.png\",save_figures=True)\n",
    "          \n",
    "plot_hist(length = length, in_the_range = in_the_range, x_name=\"Ideal length\",draw_quant=False,\n",
    "        filename=\"fig/regres/histo_length_best.png\", save_figures=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des trois méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via lineplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, len(length[:100]) + 1), length[:100], color='green', lw=1.5, alpha=0.4, label='Best')\n",
    "plt.plot(np.arange(1, len(length_split[:100]) + 1), length_split[:100], color='tomato', lw=1.5, alpha=0.8, label='Split')\n",
    "plt.plot(np.arange(1, len(length_cvp[:100]) + 1), length_cvp[:100], color='gray', lw=1.5, alpha=0.8, label='CV+ K folder')\n",
    "plt.plot(np.arange(1, len(length_cqr[:100]) + 1), length_cqr[:100], color='lightblue', lw=1.5, label='CQR')\n",
    "\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper center', ncol=4, frameon=False, bbox_to_anchor=(0.5, 1.1))\n",
    "plt.savefig(\"fig/regres/lineplot_length.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.DataFrame({\n",
    "        'length': length_split,\n",
    "        'in_the_range': in_the_range_split\n",
    "})\n",
    "\n",
    "df_cvp = pd.DataFrame({\n",
    "        'length': length_cvp,\n",
    "        'in_the_range': in_the_range_cvp\n",
    "})\n",
    "\n",
    "df_cqr = pd.DataFrame({\n",
    "        'length': length_cqr,\n",
    "        'in_the_range': in_the_range_cqr\n",
    "})\n",
    "\n",
    "df_best = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "})\n",
    "\n",
    "\n",
    "# Ajout de la colonne \"méthode\"\n",
    "df_split['méthode'] = 'Split'\n",
    "df_cvp['méthode'] = 'CV+ K folder'\n",
    "df_cqr['méthode'] = 'CQR'\n",
    "df_best['méthode'] = 'Best'\n",
    "\n",
    "# Concaténation des dataframes\n",
    "df_concat = pd.concat([df_split, df_cvp, df_cqr, df_best], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(data=df_concat, x=\"méthode\", y='length', hue='in_the_range', alpha=0.8, palette={True: 'green', False: 'red'},\n",
    "    split=True, gap=0.05, inner=\"quart\", hue_order=[True, False])\n",
    "plt.ylim(0,df_concat['length'].max()/2)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper left', title=\"Vrai valeur dans l'intervalle de prédiction:\")\n",
    "plt.savefig(\"fig/regres/violin_length_zoom.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
