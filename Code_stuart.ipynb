{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Prédiction conforme pour les données textuelles analysées par Transformers : application sur rapports médicaux\n",
    "\n",
    "Benoliel Stuart\n",
    "\\\n",
    "Carrere Charles\n",
    "\\\n",
    "Thomas Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.version\n",
    "\n",
    "# Importer les bibliothèques nécessaires\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 1\n",
    "pth = 'data/mimiciii'\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[39m=\u001b[39m pexpect\u001b[39m.\u001b[39;49mspawn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msh, args\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m-c\u001b[39;49m\u001b[39m'\u001b[39;49m, cmd])  \u001b[39m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawn(command, args, preexec_fn, dimensions)\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_poll \u001b[39m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m [a \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m a\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawnpty(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    304\u001b[0m                              cwd\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcwd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc\u001b[39m.\u001b[39mpid\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m ptyprocess\u001b[39m.\u001b[39;49mPtyProcess\u001b[39m.\u001b[39;49mspawn(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[39m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mread(exec_err_pipe_read, \u001b[39m4096\u001b[39;49m)\n\u001b[1;32m    316\u001b[0m os\u001b[39m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/Code_stuart.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://user-stuartbenoliel-767693-0.user.lab.sspcloud.fr/home/onyxia/work/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/Code_stuart.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Seulement si besoin d'appliquer le Bert\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-stuartbenoliel-767693-0.user.lab.sspcloud.fr/home/onyxia/work/Prediction-conforme-pour-les-donnees-textuelles-analysee-par-Transformers/Code_stuart.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mpip\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39minstall signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2482\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[39m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/IPython/core/magics/packaging.py:92\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     python \u001b[39m=\u001b[39m shlex\u001b[39m.\u001b[39mquote(python)\n\u001b[0;32m---> 92\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49msystem(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([python, \u001b[39m\"\u001b[39;49m\u001b[39m-m\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpip\u001b[39;49m\u001b[39m\"\u001b[39;49m, line]))\n\u001b[1;32m     94\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_expand(cmd, depth\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(child\u001b[39m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[39m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     child\u001b[39m.\u001b[39msendline(\u001b[39mchr\u001b[39m(\u001b[39m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[39m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Seulement si besoin d'appliquer le Bert\n",
    "%pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas transformers\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install quantile-forest\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Répertoire actuel\n",
    "current_directory = os.getcwd()\n",
    "print(\"Vous êtes dans le répertoire:\", current_directory)\n",
    "\n",
    "# Accéder au répertoire parent (A)\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Changer de répertoire de travail vers le répertoire parent\n",
    "os.chdir(parent_directory)\n",
    "# Vérifier le nouveau répertoire\n",
    "new_directory = os.getcwd()\n",
    "print(\"Vous êtes maintenant dans le répertoire:\", new_directory)\"\"\"\n",
    "\n",
    "files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données + Stat descriptives\n",
    "Dans cette section, nous chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "# But : enlever le mauvais encodage du type '.0' pour chacune des valeurs de la colonne\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "print(len(data))\n",
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,:]['TEXT'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMITTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DELTATIME'] = (adm['DISCHTIME'] - adm['ADMITTIME']).dt.total_seconds() / 3600\n",
    "adm = adm.drop(adm[adm['DELTATIME'] < 0].index)\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'HADM_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"HADM_ID\"].isin(data[\"HADM_ID\"].unique())]\n",
    "# Enlever les lignes correspondant aux donneurs d'organes dans la colonne DIAGNOSIS\n",
    "adm = adm[~adm[\"DIAGNOSIS\"].isin([\"ORGAN DONOR\", \"ORGAN DONOR ACCOUNT\", \"DONOR ACCOUNT\"])]\n",
    "\n",
    "# Filtrer les lignes correspondant au SUBJECT_ID \"3369\" car ligne doublon étrange\n",
    "subject_3369 = adm[adm[\"SUBJECT_ID\"] == \"3369\"]\n",
    "# Identifier l'index de la ligne avec la valeur de DELTATIME la plus faible\n",
    "index_to_remove = subject_3369[\"DELTATIME\"].idxmin()\n",
    "# Supprimer la ligne correspondante\n",
    "adm = adm.drop(index_to_remove)\n",
    "\n",
    "print(\"Le nombre d'admissions est de :\", len(adm))\n",
    "print(\"Le nombre d'individus est de :\", len(adm.groupby(\"SUBJECT_ID\")))\n",
    "print(adm.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'PATIENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(pat)\n",
    "pat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "df['DELTATIME'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('HOSPITAL_EXPIRE_FLAG')['DELTATIME'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les quantiles\n",
    "def quantile_plot(data, quantiles):\n",
    "    q_values = data.quantile(quantiles)\n",
    "    plt.plot(quantiles, q_values, marker='.', linestyle='-')\n",
    "\n",
    "df =  adm\n",
    "# Calcul des quantiles à afficher (par exemple, de 0 à 1 avec un pas de 0.01)\n",
    "quantiles = np.arange(0.01, 1.01, 0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 0\n",
    "data_0 = df[df['HOSPITAL_EXPIRE_FLAG'] == 0]['DELTATIME']\n",
    "quantile_plot(data_0, quantiles)\n",
    "    \n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 1\n",
    "data_1 = df[df['HOSPITAL_EXPIRE_FLAG'] == 1]['DELTATIME']\n",
    "quantile_plot(data_1, quantiles)\n",
    "\n",
    "plt.xlabel('Quantiles')\n",
    "plt.ylabel('DELTATIME')\n",
    "plt.title('Graphique de Quantiles pour DELTATIME par HOSPITAL_EXPIRE_FLAG')\n",
    "plt.legend(['Survivants (Flag=0)', 'Décédés (Flag=1)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 0\n",
    "data_0 = df[df['HOSPITAL_EXPIRE_FLAG'] == 0]['DELTATIME']\n",
    "quantile_plot(data_0, quantiles)\n",
    "    \n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 1\n",
    "data_1 = df[df['HOSPITAL_EXPIRE_FLAG'] == 1]['DELTATIME']\n",
    "quantile_plot(data_1, quantiles)\n",
    "\n",
    "plt.xlabel('Quantiles')\n",
    "plt.xlim(left = 0, right = 1)\n",
    "plt.ylabel('DELTATIME')\n",
    "plt.ylim(0, 1000)\n",
    "plt.title('Graphique de Quantiles pour DELTATIME par HOSPITAL_EXPIRE_FLAG')\n",
    "plt.legend(['Survivants (Flag=0)', 'Décédés (Flag=1)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "# Réglage du style (optionnel)\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=50, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme empilé de DELTATIME par HOSPITAL_EXPIRE_FLAG ')\n",
    "plt.legend(['Décédés (Flag=1)', 'Survivants (Flag=0)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "# Réglage du style (optionnel)\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "histogram = sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme empilé de DELTATIME par HOSPITAL_EXPIRE_FLAG ')\n",
    "plt.legend(['Décédés (Flag=1)', 'Survivants (Flag=0)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==1]\n",
    "sns.set(style='ticks')\n",
    "palette = sns.color_palette(\"deep\")\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", color = palette[1])\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les décès')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==0]\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les survivants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==0]\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=1, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 20, right=60)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les survivants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement\n",
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "## Fonction pour diviser les documents en plus petits morceaux\n",
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts\n",
    "\n",
    "def calculate_days_since_earliest_date(dates_list):\n",
    "    earliest_date = min(dates_list)\n",
    "    days_since_earliest = [(date - earliest_date).days for date in dates_list]\n",
    "    \n",
    "    epsilon = 1e-2  # Petit nombre pour ajouter du bruit\n",
    "    adjusted_days = []\n",
    "    prev_days = []  # Pour stocker les jours précédents\n",
    "    \n",
    "    for day in days_since_earliest:\n",
    "        noise = random.uniform(0, epsilon) # Générer un bruit aléatoire positif\n",
    "        if day not in prev_days:  # Si le jour n'est pas identique à ceux rencontrés précédemment\n",
    "            prev_days.append(day)\n",
    "            noise = 0\n",
    "        adjusted_days.append(day + noise)\n",
    "    \n",
    "    return adjusted_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## Charger le modèle ClinicalBERT depuis Hugging Face\n",
    "Dans cette partie, nous chargeons le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "# torch.cuda.set_device(0)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [],
   "source": [
    "# Créer un nouveau dataframe avec les \"HADM_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = adm[adm[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "print(\"Le nombre d'admissions avec mort du patient est de :\", len(label_1))\n",
    "\n",
    "# Créer un nouveau dataframe avec les \"HADM_ID\" ayant la valeur 0 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_0 = adm[adm[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "print(\"Le nombre d'admission avec survie du patient est de :\", len(label_0))\n",
    "# Pour l'échangeabilité des données, on ne considère pas les individus étant mort mais également vivant au cours d'une admission\n",
    "label_0 = label_0[~label_0[\"SUBJECT_ID\"].isin(label_1[\"SUBJECT_ID\"])].reset_index(drop=True)\n",
    "\n",
    "# Sélectionner aléatoirement 2500 individus de chaque classe (label_1 et label_0)\n",
    "n_train = 2500\n",
    "\n",
    "sample = pd.concat([label_1.sample(n=n_train, random_state=seed), label_0.sample(n=n_train, random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les admissions sélectionnées précédemment\n",
    "filtered_data = data[data[\"HADM_ID\"].isin(sample[\"HADM_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'HADM_ID' en agrégeant les listes de 'TEXT' et 'CHARTDATE'\n",
    "grouped_sample = filtered_data.groupby('HADM_ID').agg({'TEXT': list, 'CHARTDATE': list}).reset_index()\n",
    "\n",
    "# Appliquer cette fonction à chaque ligne de la colonne CHARTDATE\n",
    "grouped_sample['CHARTDATE'] = grouped_sample['CHARTDATE'].apply(calculate_days_since_earliest_date)\n",
    "\n",
    "grouped_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les données test (new_data) pour ne conserver que les admissions absentes\n",
    "new_data_test = adm[~adm[\"HADM_ID\"].isin(sample[\"HADM_ID\"].values)]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "label_0 = label_0[~label_0[\"SUBJECT_ID\"].isin(label_1[\"SUBJECT_ID\"])].reset_index(drop=True)\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (200 patients) et non décédées (1800 patients)\n",
    "n_test = 2000\n",
    "\n",
    "sample_test = pd.concat([label_1.sample(n=int(n_test/10), random_state=seed), label_0.sample(n=int(n_test*9/10), random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"HADM_ID\"].isin(sample_test[\"HADM_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"HADM_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('HADM_ID').agg({'TEXT': list, 'CHARTDATE': list}).reset_index()\n",
    "\n",
    "# Appliquer cette fonction à chaque ligne de la colonne CHARTDATE\n",
    "grouped_sample_test['CHARTDATE'] = grouped_sample_test['CHARTDATE'].apply(calculate_days_since_earliest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## Extraire les tokens CLS\n",
    "Dans cette partie, le code se focalise sur l'extraction des embeddings ClinicalBERT. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Le résultat est un dictionnaire qui associe chaque patient (admission) à ses embeddings ClinicalBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token(grouped_sample):\n",
    "    # Créer un dictionnaire de la forme {n° admission: [liste des textes, valeurs CHARTDATE]} pour faciliter l'itération\n",
    "    grouped_texts_dict = grouped_sample.set_index('HADM_ID')[['TEXT', 'CHARTDATE']].to_dict(orient='index')\n",
    "\n",
    "    # Initialiser une liste pour stocker les valeurs 'CHARTDATE' de chaque partie d'un document\n",
    "    time_list = []\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les embeddings\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    # Parcourir les patients et leurs données associées\n",
    "    for hadm_id, values in grouped_texts_dict.items():\n",
    "        texts = values['TEXT']  # Récupérer la liste des documents\n",
    "        times = values['CHARTDATE']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "        embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "        # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "        for text, time in zip(texts, times):\n",
    "            # Diviser le texte en parties égales\n",
    "            encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "            n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "            n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "            parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "            # Stocker les embeddings des différentes parties du document\n",
    "            cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "            # Parcourir les parties du document\n",
    "            for partie in parties:\n",
    "                # Convertir la partie dans un format compatible avec le modèle\n",
    "                inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                    outputs = model(**inputs)\n",
    "\n",
    "                # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                # Stocker l'embedding dans la liste\n",
    "                cls_embeddings_list.append(cls_embeddings)\n",
    "                # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "                time_list.append(time)\n",
    "\n",
    "            # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "            embeddings_list += cls_embeddings_list\n",
    "\n",
    "        # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "        embeddings_dict[hadm_id] = torch.stack(embeddings_list)\n",
    "    return embeddings_dict , time_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [],
   "source": [
    "embeddings_dict, time_list = extract_token(grouped_sample)\n",
    "# 165 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_dict)\n",
    "print(len(time_list))\n",
    "# 255273 len(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_test, time_list_test = extract_token(grouped_sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## Réduction de dimension\n",
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### Projection gaussienne aléatoire\n",
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "def project_gaussian(embeddings_dict, time_list):\n",
    "\n",
    "    # ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "    # Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings pour créer une matrice unique\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "    transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "    # Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "    reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "    # Diviser les embeddings réduits pour chaque patient\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    return reduced_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = project_gaussian(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = project_gaussian(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### ACP\n",
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def ACP(embeddings_dict, time_list):\n",
    "    # Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings en une seule matrice\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "    pca = PCA(n_components=200)\n",
    "\n",
    "    # Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "    reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    # Séparer les embeddings transformés pour chaque sujet\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 200)\n",
    "    # ValueError: Argument 'path' must have stream dimension of size at least 2. (Need at least this many points to define a path.)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 200)}\n",
    "\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 201))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    # Afficher la variance expliquée par chaque composante principale\n",
    "    print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "    # Afficher la variance totale expliquée par toutes les composantes principales\n",
    "    print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "    return reduced_embeddings_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = ACP(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID de l'admission : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = ACP(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"HADM ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## Calculer les signatures\n",
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import signatory\n",
    "\n",
    "def signa(reduced_embeddings_dict):\n",
    "\n",
    "    # Ordre de la signature tronquée\n",
    "    depth = 2\n",
    "\n",
    "    # Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "    signature_dict = {}\n",
    "\n",
    "    # Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "    for key, value in reduced_embeddings_dict.items():\n",
    "        # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "        tensor = torch.from_numpy(value).float().to(device)\n",
    "\n",
    "        # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "        tensor = tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "        signature = signatory.signature(path=tensor, depth=depth)\n",
    "\n",
    "        # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "        signature = signature.squeeze(0).to(device)\n",
    "\n",
    "        # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "        signature_dict[key] = signature\n",
    "    return signature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [],
   "source": [
    "signature_dict = signa(reduced_embeddings_dict)\n",
    "# À ce stade, signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n",
    "print(signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_dict_test = signa(reduced_embeddings_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement\n",
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'HADM_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'HADM_ID'\n",
    "df_features.rename(columns={'index':'HADM_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'HADM_ID'\n",
    "df_final = pd.merge(df_features, adm[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG', 'DELTATIME']], on='HADM_ID', how='inner')\n",
    "\n",
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test\n",
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que 'HADM_ID' devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'HADM_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'HADM_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne 'HADM_ID' en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, adm[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG', 'DELTATIME']], on='HADM_ID', how='inner')\n",
    "\n",
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 2)\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement données traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'wb') as f1:\n",
    "    pickle.dump(df_final, f1)\n",
    "with open('data/traite/data_test', 'wb') as f1:\n",
    "    pickle.dump(df_final_test, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation données pré-traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'rb') as f1:\n",
    "    df_final = pickle.load(f1)\n",
    "with open('data/traite/data_test', 'rb') as f1:\n",
    "    df_final_test = pickle.load(f1)\n",
    "\n",
    "print(\"Dataset: %s\" % (df_final.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (df_final_test.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_qr', 'rb') as f1:\n",
    "    pred_qr = pickle.load(f1)\n",
    "with open('data/traite/pred_rf', 'rb') as f1:\n",
    "    pred_rf = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cqr', 'rb') as f1:\n",
    "    y_lower_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cqr', 'rb') as f1:\n",
    "    y_upper_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cvp', 'rb') as f1:\n",
    "    y_lower_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cvp', 'rb') as f1:\n",
    "    y_upper_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_lower_split', 'rb') as f1:\n",
    "    y_lower_split = pickle.load(f1)\n",
    "with open('data/traite/y_upper_split', 'rb') as f1:\n",
    "    y_upper_split = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(y,\n",
    "              y_u=None,\n",
    "              y_l=None,\n",
    "              pred=None,\n",
    "              y_max=None,\n",
    "              shade_color=\"\",\n",
    "              method_name=\"\",\n",
    "              title=\"\",\n",
    "              filename=None,\n",
    "              save_figures=False,\n",
    "              max_show=100):\n",
    "    \n",
    "    \"\"\" Scatter plot of (x,y) points along with the constructed prediction interval \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array, target response variable (length n)\n",
    "    pred : numpy array, the estimated prediction. It may be the conditional mean,\n",
    "           or low and high conditional quantiles.\n",
    "    shade_color : string, desired color of the prediciton interval\n",
    "    method_name : string, name of the method\n",
    "    title : string, the title of the figure\n",
    "    filename : sting, name of the file to save the figure\n",
    "    save_figures : boolean, save the figure (True) or not (False)\n",
    "    \n",
    "    \"\"\"\n",
    "    y_ = y[:max_show]\n",
    "    x_ = np.arange(1, len(y_) + 1)\n",
    "\n",
    "    if y_u is not None:\n",
    "        y_u_ = y_u[:max_show]\n",
    "    if y_l is not None:\n",
    "        y_l_ = y_l[:max_show]\n",
    "    if pred is not None:\n",
    "        pred_ = pred[:max_show]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x_, y_, 'k.', alpha=.2, markersize=10,\n",
    "             fillstyle='none', label=u'Observations')\n",
    "    \n",
    "    if (y_u is not None) and (y_l is not None):\n",
    "        plt.fill(np.concatenate([x_, x_[::-1]]),\n",
    "                 np.concatenate([y_u_, y_l_[::-1]]),\n",
    "                 alpha=.3, fc=shade_color, ec='None',\n",
    "                 label = method_name + ' prediction interval')\n",
    "    \n",
    "    if pred is not None:\n",
    "        if pred_.ndim == 2:\n",
    "            plt.plot(x_, pred_[:,0], 'k', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted low and high quantiles')\n",
    "            plt.plot(x_, pred_[:,1], 'k', lw=2, alpha=0.9)\n",
    "        else:\n",
    "            plt.plot(x_, pred_, 'k--', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted value')\n",
    "\n",
    "    if y_max is not None:\n",
    "        plt.ylim(0,y_max)\n",
    "        \n",
    "    plt.xlabel('$X$')\n",
    "    plt.ylabel('$Y$')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_hist(length,\n",
    "              in_the_range,\n",
    "              x_name=\"\",\n",
    "              dec_x_quant=[0,0,0],\n",
    "              draw_quant=True,\n",
    "              filename=None,\n",
    "              save_figures=False):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.histplot(data=df, x='length', hue='in_the_range', multiple=\"stack\", stat='percent',\n",
    "        palette={True: 'green', False: 'red'}, bins=20, hue_order=[True, False])\n",
    "    sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=2, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "    if draw_quant:\n",
    "        # Calcul des quantiles\n",
    "        quantiles = [np.percentile(length, i) for i in range(25, 100, 25)]\n",
    "\n",
    "        # Calcul de la hauteur maximale pour les barres verticales\n",
    "        ymax = plt.ylim()[1] \n",
    "\n",
    "        # Tracé des lignes verticales pour les quantiles et ajout des étiquettes\n",
    "        for i, quantile in enumerate(quantiles, 1):\n",
    "            plt.vlines(x=quantile, ymin=0, ymax=ymax*9/10 , linestyle='--', linewidth=1.5, color='black')\n",
    "            plt.text(quantile+dec_x_quant[i-1], ymax*9/10, f'Q{i}:\\n{round(quantile)}', va='bottom', ha='center', color='black', fontsize=9, rotation=0)\n",
    "\n",
    "    plt.xlabel(x_name)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout variables + Changement var d'intérêt si souhaité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression = True # Choix d'aller sur regression ou rester sur binaire\n",
    "X_train = df_final.iloc[:, 1:-2]\n",
    "X_test = df_final_test.iloc[:, 1:-2]\n",
    "\n",
    "if Regression :\n",
    "    y_train = df_final['DELTATIME']\n",
    "    y_test = df_final_test['DELTATIME']\n",
    "else :\n",
    "    y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "    y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "print(\"Dataset: %s\" % (X_train.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (X_test.shape,))\n",
    "\n",
    "# Problem setup\n",
    "n = 650 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "\n",
    "# Split the data into calibration and validation sets (save the shuffling)\n",
    "idx = np.array([1] * n + [0] * (X_test.shape[0]-n)) > 0\n",
    "np.random.RandomState(seed).shuffle(idx)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si ajout variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "Regression = False # Choix d'aller sur regression ou rester sur binaire\n",
    "\n",
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join('data/mimiciii', 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "pat = pd.concat([chunk for chunk in pd.read_csv(os.path.join('data/mimiciii', 'PATIENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' en chaînes de caractères\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "pat['HADM_ID'] = pat['HADM_ID'].astype(str)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train = df_final.iloc[:,:]\n",
    "X_train = pd.merge(X_train, adm, on='HADM_ID', how='left')\n",
    "X_train = pd.merge(X_train, pat, on='HADM_ID', how='left')\n",
    "\n",
    "# Extraction des noms de colonnes des variables numériques\n",
    "numerical_columns = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Conversion des variables catégorielles en valeurs encodées\n",
    "x_categorical = X_train.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Création du DataFrame des variables numériques avec les noms de colonnes\n",
    "x_numerical = X_train[numerical_columns]\n",
    "\n",
    "# Concaténation des variables numériques et catégorielles\n",
    "X_train = pd.concat([x_numerical, x_categorical], axis=1)\n",
    "\n",
    "# Supprimez les doublons conservés dans 'X_train' en conservant uniquement la première occurrence (la plus tardive)\n",
    "X_train = X_train.drop_duplicates(subset='HADM_ID', keep='first')\n",
    "\n",
    "if Regression :\n",
    "    y_train = X_train['DELTATIME']\n",
    "else :\n",
    "    y_train = X_train[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "X_train = X_train.iloc[:, list(range(1, 5151)) ] # + [5162, 5165, 5166, 5173] \n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "X_test = df_final_test.iloc[:, 0:-1]\n",
    "X_test = pd.merge(X_test, adm, on='HADM_ID', how='left')\n",
    "X_test = pd.merge(X_test, pat, on='HADM_ID', how='left')\n",
    "\n",
    "# Extraction des noms de colonnes des variables numériques\n",
    "numerical_columns = X_test.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Conversion des variables catégorielles en valeurs encodées\n",
    "x_categorical = X_test.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Création du DataFrame des variables numériques avec les noms de colonnes\n",
    "x_numerical = X_test[numerical_columns]\n",
    "\n",
    "# Concaténation des variables numériques et catégorielles\n",
    "X_test = pd.concat([x_numerical, x_categorical], axis=1)\n",
    "\n",
    "if Regression :\n",
    "    y_test = X_test['DELTATIME']\n",
    "else :\n",
    "    y_test = X_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "    \n",
    "X_test = X_test.iloc[:, list(range(1, 5151)) ] # + [5162, 5165, 5166, 5173] <=> 'ADMISSION_TYPE', 'INSURANCE', 'LANGUAGE', 'GENDER'\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "print(\"Dataset: %s\" % (X_train.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (X_test.shape,))\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "\n",
    "# Split the data into calibration and validation sets (save the shuffling)\n",
    "idx = np.array([1] * n + [0] * (X_test.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=seed)\n",
    "# divide the data into proper training set and calibration set\n",
    "# idx = np.random.permutation(n_train)\n",
    "# n_half = int(np.floor(n_train/2))\n",
    "# idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles ML Survie des patients (binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "## Reg Logistique\n",
    "\n",
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignorer les warnings de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés et tuning du paramètres Cs\n",
    "# inverse of regularization strength -> smaller values specify stronger regularization\n",
    "model = LogisticRegressionCV(penalty='l1', solver='saga', max_iter=1000, class_weight = 'balanced', random_state = seed, cv=5)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_lr = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle (!= precision)\n",
    "# Taux de bien classés\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle = Sensibilité\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "# Quand le recall est haut, cela veut plutôt dire qu’il ne ratera aucun positif.\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(matrix)\n",
    "\n",
    "# 136min\n",
    "# Précision: 48.12%\n",
    "# Rappel: 46.00%\n",
    "# F1-score: 15.09%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, class_weight='balanced',random_state = seed)\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, class_weight='balanced',\n",
    "    max_features=best_max_features, oob_score =True, random_state = seed)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_rf = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "oob_score = model.oob_score_\n",
    "print(\"Out-of-Bag Score: %.2f\" % (oob_score))\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle (!= precision)\n",
    "# Taux de bien classés\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle = Sensibilité\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "# estime la probabilité de bien détecter un positif\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "# 13 min\n",
    "# OOB : 0.81\n",
    "# Précision: 34.29%\n",
    "# Rappel: 81.00%\n",
    "# F1-score: 19.82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison performances des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer le taux de faux positifs (FPR), le taux de vrais positifs (TPR) et les seuils\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, smx_lr[:, 1])\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, smx_rf[:, 1])\n",
    "# Calculer l'aire sous la courbe ROC (AUC)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label='Logistic regression (AUC = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_rf, tpr_rf, color='lightblue', lw=2, label='Random forest (AUC = %0.2f)' % roc_auc_rf)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "\n",
    "# Afficher les points correspondant aux seuils avec un pas de 0.1\n",
    "threshold_indices_lr = [i for i, threshold in enumerate(thresholds_lr) if i % 20 == 0]\n",
    "threshold_indices_rf = [i for i, threshold in enumerate(thresholds_rf) if i % 10 == 0]\n",
    "\n",
    "plt.scatter(fpr_lr[threshold_indices_lr], tpr_lr[threshold_indices_lr], marker='o', color='darkorange', label='Thresholds (LR)')\n",
    "plt.scatter(fpr_rf[threshold_indices_rf], tpr_rf[threshold_indices_rf], marker='o', color='lightblue', label='Thresholds (RF)')\n",
    "\n",
    "# Ajouter des annotations indiquant la valeur des seuils\n",
    "for i in threshold_indices_lr:\n",
    "    plt.text(fpr_lr[i], tpr_lr[i], f'{thresholds_lr[i]:.2f}', fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "for i in threshold_indices_rf:\n",
    "    plt.text(fpr_rf[i], tpr_rf[i], f'{thresholds_rf[i]:.2f}', fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('TFP = 1 - Spécificité')\n",
    "plt.ylabel('TVP = Sensibilité')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"fig/classi/roc.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des seuils de classification pour lesquels on va calculer\n",
    "# les f1 scores\n",
    "threshold_array = np.linspace(0, 1, 100)\n",
    "f1_list_lr = []\n",
    "f1_list_rf = []\n",
    "\n",
    "# Boucle pour calculer le f1 score pour chaque seuil\n",
    "for threshold in threshold_array:\n",
    "    # Prédiction des étiquettes pour un seuil donné\n",
    "    pred_threshold_lr = (smx_lr[:, 1] > threshold).astype(int)\n",
    "    pred_threshold_rf = (smx_rf[:, 1] > threshold).astype(int)\n",
    "    \n",
    "    # Calcul du f1 score pour un seuil donné\n",
    "    f1_threshold_lr = f1_score(y_true=y_test, y_pred=pred_threshold_lr)\n",
    "    f1_threshold_rf = f1_score(y_true=y_test, y_pred=pred_threshold_rf)\n",
    "    \n",
    "    # Ajout du f1 score à la liste\n",
    "    f1_list_lr.append(f1_threshold_lr)\n",
    "    f1_list_rf.append(f1_threshold_rf)\n",
    "\n",
    "# Tracer la courbe des f1 scores en fonction des seuils\n",
    "plt.plot(threshold_array, f1_list_lr, color='darkorange', lw=2,label='Logistic regression')\n",
    "plt.plot(threshold_array, f1_list_rf, color='lightblue', lw=2,label='Random forest')\n",
    "plt.xlabel('Seuil score')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"fig/classi/f1_score.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'score': smx_lr[:, 1],\n",
    "    'true_label': y_test\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.histplot(data=df, x='score', hue='true_label', multiple=\"stack\", stat='percent',\n",
    "    palette={1: 'darkorange', 0: 'lightblue'}, bins=20, hue_order=[0, 1])\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\",\n",
    "    bbox_to_anchor=(.5, 1), ncol=2, frameon=False,\n",
    ")\n",
    "ax.legend(['Mort', 'Vivant'])\n",
    "plt.xlabel(\"Score prédits logistic regression\")\n",
    "plt.savefig(\"fig/classi/histo_score_lr.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'score': smx_rf[:, 1],\n",
    "    'true_label': y_test\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.histplot(data=df, x='score', hue='true_label', multiple=\"stack\", stat='percent',\n",
    "    palette={1: 'darkorange', 0: 'lightblue'}, bins=20, hue_order=[0, 1])\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\",\n",
    "    bbox_to_anchor=(.5, 1), ncol=2, frameon=False,\n",
    ")\n",
    "ax.legend(['Mort', 'Vivant'])\n",
    "plt.xlabel(\"Score prédits random forest\")\n",
    "plt.savefig(\"fig/classi/histo_score_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction conforme (binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n = 650 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "# CHANGER smx_rf ou smx_lr pour appliquer la prédiction conforme soit sur la random forest / soit logistic regression\n",
    "smx = smx_rf\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.RandomState(seed).shuffle(idx)\n",
    "\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "q = 0.6\n",
    "\n",
    "# Calcul des scores de conformité Vi pour les données d'étalonnage\n",
    "cal_scores = cal_labels - cal_smx[:, 1]\n",
    "cal_scores_bis = 100*cal_labels - cal_smx[:, 1]\n",
    "# Calcul des scores de conformité Vn+j pour les données de test\n",
    "test_scores = c - val_smx[:, 1]\n",
    "test_scores_bis = 100 - val_smx[:, 1]\n",
    "\n",
    "def BH(calib_scores, test_scores, q = 0.1):\n",
    "    ntest = len(test_scores)\n",
    "    ncalib = len(calib_scores)\n",
    "    pvals = np.zeros(ntest)\n",
    "    \n",
    "    for j in range(ntest):\n",
    "        pvals[j] = (np.sum(calib_scores < test_scores[j]) + 1) / (ncalib+1)\n",
    "         \n",
    "    # BH(q) \n",
    "    df_test = pd.DataFrame({\"id\": range(ntest), \"pval\": pvals}).sort_values(by='pval')\n",
    "    \n",
    "    df_test['threshold'] = q * np.linspace(1, ntest, num=ntest) / ntest \n",
    "    idx_smaller = [j for j in range(ntest) if df_test.iloc[j,1] <= df_test.iloc[j,2]]\n",
    "    \n",
    "    if len(idx_smaller) == 0:\n",
    "        return(np.array([]))\n",
    "    else:\n",
    "        idx_sel = np.array(df_test.index[range(np.max(idx_smaller)+1)])\n",
    "        return(idx_sel)\n",
    "\n",
    "# BH using residuals\n",
    "BH_res= BH(cal_scores, test_scores, q)\n",
    "# The FDR (FDP) is a natural measure of type-I error for binary classification\n",
    "# Power = Rappel\n",
    "if len(BH_res) == 0:\n",
    "    BH_res_fdp = 0\n",
    "    BH_res_power = 0\n",
    "else:\n",
    "    BH_res_fdp = np.sum(val_labels.reset_index(drop=True)[BH_res] <= c) / len(BH_res)\n",
    "    BH_res_power = np.sum(val_labels.reset_index(drop=True)[BH_res] > c) / sum(val_labels > c)\n",
    "        \n",
    "print(BH_res_fdp,BH_res_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des p-valeurs conformes\n",
    "p_values = np.zeros(len(test_scores))\n",
    "p_values_bis = np.zeros(len(test_scores))\n",
    "# Calcul des p-valeurs conformes pour chaque score de test\n",
    "for j in range(len(test_scores)):\n",
    "    count_V_less_than_Vnj = np.sum(cal_scores < test_scores[j])\n",
    "    count_V_equal_to_Vnj = np.sum(cal_scores == test_scores[j])\n",
    "    U_j = np.random.uniform()\n",
    "    p_values[j] = (count_V_less_than_Vnj + (1 + count_V_equal_to_Vnj) * U_j) / (len(cal_scores) + 1)\n",
    "    p_values_bis[j] = (count_V_less_than_Vnj + 1) / (len(cal_scores) + 1)\n",
    "\n",
    "# Affichage des p-valeurs conformes\n",
    "print(\"P-values conformes :\", p_values, p_values_bis)\n",
    "\n",
    "# Initialiser k* à 0\n",
    "k_star = 0\n",
    "\n",
    "# Parcourir les p-valeurs conformes dans l'ordre décroissant\n",
    "for k in range(len(test_scores)):\n",
    "    # Calculer la proportion de p-valeurs conformes inférieures ou égales à qk/m\n",
    "    proportion = np.sum(p_values <=  q*k/ m)\n",
    "    \n",
    "    # Vérifier si la proportion est supérieure ou égale à k\n",
    "    if proportion >= k:\n",
    "        # Mettre à jour k* avec la valeur actuelle de k\n",
    "        k_star = k + 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores\n",
    "cal_scores = cal_smx[:, 1]\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx[:, 1] > qhat\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets == val_labels\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Trouver les cas où la prédiction est négative mais la vraie classe est positive\n",
    "erreur_type_2 = (prediction_sets == False) & (val_labels == 1)\n",
    "\n",
    "# Calculer le pourcentage d'erreurs de type 2 par rapport au total des prédictions positives\n",
    "erreur_type_2_rate = erreur_type_2.sum() / val_labels.sum()\n",
    "\n",
    "print(\"Taux d'erreur de type 2 :\", erreur_type_2_rate)\n",
    "\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'], 1, 0)\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage.reset_index(drop=True)\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.xlabel(\"Valeur prédite random forest\")\n",
    "plt.savefig(\"fig/classi/outlier_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# LR\n",
    "# The empirical coverage is: 0.7991967871485943\n",
    "# Taux d'erreur de type 2 : 0.8260869565217391\n",
    "\n",
    "# RF \n",
    "# The empirical coverage is: 0.8052208835341366\n",
    "# Taux d'erreur de type 2 : 0.825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_small_sets_lr.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + meilleur RAPS (Random Adaptative Prediction Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_raps_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def full_conformal(k):\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(5):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles ML Durée de séjour des patients (régression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Notice that the random forests regressor **estimates the conditional mean** of $Y_i$ given $X_i=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "import math\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, random_state = seed)\n",
    "\n",
    "# Définir la métrique MSE comme score\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=mse_scorer)\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, max_features=best_max_features, \n",
    "    random_state = seed, oob_score=True)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "oob_score = model.oob_score_\n",
    "print(\"Out-of-Bag Score: %.2f\" % (oob_score))\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", round(mse))\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared: %.2f\" % (r2))\n",
    "\n",
    "# OOB : 0.35\n",
    "# MSE : 56650\n",
    "# R-squared: 0.29\n",
    "# 66 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import AbsErrorErrFunc\n",
    "\n",
    "# define a conformal prediction object \n",
    "nc = RegressorNc(model, AbsErrorErrFunc())\n",
    "\n",
    "# build a regualr split conformal prediction object \n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# fit the conditional mean regression to the proper training data\n",
    "icp.fit(X_train, y_train)\n",
    "\n",
    "# compute the absolute residual error on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower_split = np.maximum(predictions[:, 0], 0)\n",
    "y_upper_split = predictions[:,1]\n",
    "\n",
    "# compute the conditional mean estimation\n",
    "pred_rf = model.predict(X_test[~idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display the average coverage\n",
    "in_the_range_split = (y_test[~idx] >= y_lower_split) & (y_test[~idx]<= y_upper_split)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_split) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_split = y_upper_split - y_lower_split\n",
    "print(\"Average length:\", round(np.mean(length_split)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_split)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_split,y_l=y_lower_split,pred=pred_rf,shade_color='tomato',\n",
    "          method_name=\"Split:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_split.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_split, in_the_range = in_the_range_split, x_name=\"Length Split\", dec_x_quant= [0,0,0],\n",
    "      filename=\"fig/regres/histo_length_split.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 88.25%\n",
    "# Average length: 467\n",
    "# Standard deviation of length: 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_split', 'wb') as f1:\n",
    "    pickle.dump(y_upper_split, f1)\n",
    "with open('data/traite/y_lower_split', 'wb') as f1:\n",
    "    pickle.dump(y_lower_split, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV+ for K-fold cross-validation (Jacknife + if K=len(X_train_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train_K = np.concatenate((X_train, X_test[idx]), axis=0)\n",
    "y_train_K = np.concatenate((y_train, y_test[idx]), axis=0)\n",
    "\n",
    "q_level = np.ceil((X_train_K.shape[0]+1)*(1-alpha))/X_train_K.shape[0]\n",
    "\n",
    "# Initialisez l'objet KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Initialisez une liste pour stocker les index des échantillons d'entraînement\n",
    "residus = np.zeros(len(X_train_K))\n",
    "pred = np.zeros((len(X_train_K),len(X_test[~idx])))\n",
    "\n",
    "# Parcourez les splits et stockez les index des échantillons d'entraînement\n",
    "for train_indices, test_indices in kf.split(X_train_K):\n",
    "    model.fit(X_train_K[train_indices], y_train_K[train_indices])\n",
    "    residus[test_indices] = abs(y_train_K[test_indices]-model.predict(X_train_K[test_indices]))\n",
    "    pred[test_indices] = np.repeat(model.predict(X_test[~idx]).reshape(1, -1), len(test_indices), axis=0)\n",
    "\n",
    "# display the results\n",
    "\n",
    "y_upper_cvp = np.quantile(pred+residus.reshape(-1, 1), q_level, method='higher', axis=0)\n",
    "y_lower_cvp = np.maximum(- np.quantile(-pred+residus.reshape(-1, 1), q_level, method='higher', axis=0), 0)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "pred_rf = model.predict(X_test[~idx])\n",
    "\n",
    "# 11 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display the average coverage\n",
    "in_the_range_cvp = (y_test[~idx] >= y_lower_cvp) & (y_test[~idx]<= y_upper_cvp)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_cvp) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_cvp = y_upper_cvp - y_lower_cvp\n",
    "print(\"Average length:\", round(np.mean(length_cvp)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_cvp)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_cvp,y_l=y_lower_cvp,pred=pred_rf,shade_color='gray',\n",
    "          method_name=\"CV+ K folder:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_cvp.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cvp, in_the_range = in_the_range_cvp, x_name=\"Length CV+ K folder\", dec_x_quant= [0,0,0],\n",
    "        filename=\"fig/regres/histo_length_cvp.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 90.11%\n",
    "# Average length: 505\n",
    "# Standard deviation of length: 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_cvp', 'wb') as f1:\n",
    "    pickle.dump(y_upper_cvp, f1)\n",
    "with open('data/traite/y_lower_cvp', 'wb') as f1:\n",
    "    pickle.dump(y_lower_cvp, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison en variant le nombre de folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Définir les constantes\n",
    "n_splits_range = range(2, 16)\n",
    "length_cvp_means = []\n",
    "length_cvp_med = []\n",
    "\n",
    "X_train_K = np.concatenate((X_train, X_test[idx]), axis=0)\n",
    "y_train_K = np.concatenate((y_train, y_test[idx]), axis=0)\n",
    "\n",
    "q_level = np.ceil((X_train_K.shape[0]+1)*(1-alpha))/X_train_K.shape[0]\n",
    "\n",
    "# Initialisez une liste pour stocker les index des échantillons d'entraînement\n",
    "residus = np.zeros(len(X_train_K))\n",
    "pred = np.zeros((len(X_train_K),len(X_test[~idx])))\n",
    "\n",
    "# Boucle sur le nombre de splits\n",
    "for n_splits in n_splits_range:\n",
    "    # Initialisez l'objet KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Parcourez les splits et exécutez le processus de validation croisée\n",
    "    for train_indices, test_indices in kf.split(X_train_K):\n",
    "        model.fit(X_train_K[train_indices], y_train_K[train_indices])\n",
    "        residus[test_indices] = abs(y_train_K[test_indices] - model.predict(X_train_K[test_indices]))\n",
    "        pred[test_indices] = np.repeat(model.predict(X_test[~idx]).reshape(1, -1), len(test_indices), axis=0)\n",
    "\n",
    "    # Calcul de length_cvp pour chaque test point\n",
    "    y_upper = np.quantile(pred + residus.reshape(-1, 1), q_level, method='higher', axis=0)\n",
    "    y_lower = np.maximum(- np.quantile(-pred + residus.reshape(-1, 1), q_level, method='higher', axis=0), 0)\n",
    "    length_cvp = y_upper - y_lower\n",
    "    length_cvp_means.append(np.mean(length_cvp))\n",
    "    length_cvp_med.append(np.quantile(length_cvp, 0.5))\n",
    "    print(n_splits)\n",
    "    print(\"Random Forests: Average length:\", np.mean(length_cvp), np.quantile(length_cvp, 0.5))\n",
    "\n",
    "# Tracer l'évolution de length_cvp en fonction du nombre de splits\n",
    "plt.plot(n_splits_range, length_cvp_means, color='lightblue', label='Moyenne')\n",
    "plt.plot(n_splits_range, length_cvp_med, color ='darkorange', label='Médiane')\n",
    "plt.xlabel('Nombre de splits')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Évolution de length en fonction du nombre de splits')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"fig/regres/lineplot_k_folder.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 147 min pour K jusqu'à 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQR Random Forests\n",
    "\n",
    "Given any quantile regression algorithm $\\mathcal{A}$ (the function `QuantileForestRegressorAdapter` in the code below), we then fit two conditional quantile functions $\\hat{q}_{\\alpha_{lo}}$ and $\\hat{q}_{\\alpha_{hi}}$ on the proper training set: $$ \\{ \\hat{q}_{\\alpha_{lo}}, \\hat{q}_{\\alpha_{hi}} \\} \\leftarrow \\mathcal{A}(\\left\\lbrace (X_i, Y_i): i \\in I_1 \\right\\rbrace). $$\n",
    "This is done by calling the function `icp.fit`.\n",
    "\n",
    "In the essential next step, the function `icp.calibrate` computes conformity scores (using `QuantileRegErrFunc`) that quantify the error made by the plug-in prediction interval $ \\hat{C}(x) = [\\hat{q}_{\\alpha_{lo}}(x), \\ \\hat{q}_{\\alpha_{hi}}(x)]  $. The scores are evaluated on the calibration set as\n",
    "$$\n",
    "\tE_i := \\max\\{\\hat{q}_{\\alpha_{lo}}(X_i) - Y_i, Y_i - \\hat{q}_{\\alpha_{hi}}(X_i)\\},\n",
    "$$\n",
    "for each $i \\in I_2$. The conformity score accounts for both undercoverage and overcoverage.\n",
    "\n",
    "Finally, given new input data $X_{n+1}$, we construct the prediction interval for $Y_{n+1}$ as\n",
    "$$\n",
    "C(X_{n+1}) = \\left[ \\hat{q}_{\\alpha_{lo}}(X_{n+1}) - Q_{1-\\alpha}(E, I_2) , \\ \\hat{q}_{\\alpha_{hi}}(X_{n+1}) + Q_{1-\\alpha}(E, I_2) \\right],\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Q_{1-\\alpha}(E, I_2) :=  (1-\\alpha)(1+1/|I_2|)\\text{-th empirical quantile of} \\left\\{E_i : i \\in I_2\\right\\}\n",
    "$$\n",
    "conformalizes the plug-in prediction interval. This is done by calling the function `icp.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install setuptools numpy scipy scikit-learn cython\n",
    "# pip3 install scikit-garden python version 3.7 (code à modifier)\n",
    "# OU pip3 install quantile_forest\n",
    "\n",
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import QuantileRegErrFunc, QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 150\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = X_train.shape[1]\n",
    "\n",
    "# target quantile levels\n",
    "# desired quanitile levels\n",
    "quantiles = [5, 95]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 1 #0.80\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = seed\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 10\n",
    "\n",
    "# define quantile random forests (QRF) parameters\n",
    "params_qforest = dict()\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"max_features\"] = max_features\n",
    "params_qforest[\"CV\"] = cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"] = cv_test_ratio\n",
    "params_qforest[\"random_state\"] = cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals\n",
    "\n",
    "# define the QRF model\n",
    "model = helper.QuantileForestRegressorAdapter(model=None,fit_params=None,\n",
    "                                                quantiles=quantiles,\n",
    "                                                params=params_qforest)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 206 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install setuptools numpy scipy scikit-learn cython\n",
    "# pip3 install scikit-garden python version 3.7 (code à modifier)\n",
    "# OU pip3 install quantile_forest\n",
    "\n",
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import QuantileRegErrFunc, QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# define the CQR object, computing the absolute residual error of points \n",
    "# located outside the estimated QRF band \n",
    "nc = RegressorNc(model, QuantileRegErrFunc()) # ou QuantileRegAsymmetricErrFunc / QuantileRegErrFunc\n",
    "\n",
    "# build the split CQR object\n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# compute the absolute errors on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower_cqr = np.maximum(predictions[:, 0], 0)\n",
    "y_upper_cqr = predictions[:,1]\n",
    "\n",
    "# compute the low and high conditional quantile estimation\n",
    "pred_qr = model.predict(X_test[~idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower_qr = np.maximum(pred_qr[:, 0], 0)\n",
    "y_upper_qr = pred_qr[:,1]\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_qr = (y_test[~idx] >= y_lower_qr) & (y_test[~idx]<= y_upper_qr)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_qr) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the conformal interval per each test point\n",
    "length_qr = y_upper_qr - y_lower_qr\n",
    "print(\"Average length:\", round(np.mean(length_qr)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_qr)))\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_cqr = (y_test[~idx] >= y_lower_cqr) & (y_test[~idx]<= y_upper_cqr)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_cqr) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the conformal interval per each test point\n",
    "length_cqr = y_upper_cqr - y_lower_cqr\n",
    "print(\"Average length:\", round(np.mean(length_cqr)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_cqr)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_cqr,y_l=y_lower_cqr,pred=pred_qr,shade_color='lightblue',\n",
    "          method_name=\"CQR:\",title=\"CQR Random Forests (quantile regression)\",\n",
    "          filename=\"fig/regres/lineplot_cqr.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cqr, in_the_range = in_the_range_cqr, x_name=\"Length CQR\", dec_x_quant= [-150,110,70],\n",
    "        filename=\"fig/regres/histo_length_cqr.png\", save_figures=True)\n",
    "\n",
    "plot_hist(length = length_qr, in_the_range = in_the_range_qr, x_name=\"Length QR\", dec_x_quant= [-150,110,70],\n",
    "        filename=\"fig/regres/histo_length_cqr.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 92.42%\n",
    "# Average length: 549\n",
    "# Standard deviation of length: 465\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 88.77%\n",
    "# Average length: 524\n",
    "# Standard deviation of length: 465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_qr', 'wb') as f1:\n",
    "    pickle.dump(pred_qr, f1)\n",
    "with open('data/traite/y_upper_cqr', 'wb') as f1:\n",
    "    pickle.dump(y_upper_cqr, f1)\n",
    "with open('data/traite/y_lower_cqr', 'wb') as f1:\n",
    "    pickle.dump(y_lower_cqr, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best (approximativement symmétrique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower = y_lower_split.copy()\n",
    "y_upper = y_upper_split.copy()\n",
    "\n",
    "for i in range(len(y_test[~idx])):\n",
    "    if pred_rf[i] < y_test[~idx].iloc[i]:\n",
    "        y_lower[i] = max(pred_rf[i] - y_test[~idx].iloc[i]/2,0)\n",
    "        y_upper[i] = y_test[~idx].iloc[i]\n",
    "    else:\n",
    "        y_lower[i] = max(y_test[~idx].iloc[i],0)\n",
    "        y_upper[i] = pred_rf[i] + y_test[~idx].iloc[i]/2\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length = y_upper - y_lower\n",
    "print(\"Average length:\", round(np.mean(length)))\n",
    "print(\"Median length:\", round(np.median(length)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_rf,shade_color='green',\n",
    "          method_name=\"Best:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_best.png\",save_figures=True)\n",
    "          \n",
    "plot_hist(length = length, in_the_range = in_the_range, x_name=\"Ideal length\", draw_quant=False,\n",
    "        filename=\"fig/regres/histo_length_best.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 100.0\n",
    "# Average length: 241\n",
    "# Median length: 150\n",
    "# Standard deviation of length: 275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des méthodes (régression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via lineplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, len(length[:100]) + 1), length[:100], color='green', lw=1.5, alpha=0.4, label='Best')\n",
    "plt.plot(np.arange(1, len(length_split[:100]) + 1), length_split[:100], color='tomato', lw=1.5, alpha=0.8, label='Split')\n",
    "plt.plot(np.arange(1, len(length_cvp[:100]) + 1), length_cvp[:100], color='gray', lw=1.5, alpha=0.8, label='CV+ K folder')\n",
    "plt.plot(np.arange(1, len(length_cqr[:100]) + 1), length_cqr[:100], color='lightblue', lw=1.5, label='CQR')\n",
    "\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper center', ncol=4, frameon=False, bbox_to_anchor=(0.5, 1.1))\n",
    "plt.savefig(\"fig/regres/lineplot_length.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.DataFrame({\n",
    "        'length': length_split,\n",
    "        'in_the_range': in_the_range_split\n",
    "})\n",
    "\n",
    "df_cvp = pd.DataFrame({\n",
    "        'length': length_cvp,\n",
    "        'in_the_range': in_the_range_cvp\n",
    "})\n",
    "\n",
    "df_cqr = pd.DataFrame({\n",
    "        'length': length_cqr,\n",
    "        'in_the_range': in_the_range_cqr\n",
    "})\n",
    "\n",
    "df_best = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "})\n",
    "\n",
    "\n",
    "# Ajout de la colonne \"méthode\"\n",
    "df_split['méthode'] = 'Split'\n",
    "df_cvp['méthode'] = 'CV+ K folder'\n",
    "df_cqr['méthode'] = 'CQR'\n",
    "df_best['méthode'] = 'Best'\n",
    "\n",
    "# Concaténation des dataframes\n",
    "df_concat = pd.concat([df_split, df_cvp, df_cqr, df_best], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(data=df_concat, x=\"méthode\", y='length', hue='in_the_range', alpha=0.8, palette={True: 'green', False: 'red'},\n",
    "    split=True, gap=0.05, inner=\"quart\", hue_order=[True, False])\n",
    "plt.ylim(0,df_concat['length'].max()/2)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper left', title=\"Vrai valeur dans l'intervalle de prédiction:\")\n",
    "plt.savefig(\"fig/regres/violin_length_zoom.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
