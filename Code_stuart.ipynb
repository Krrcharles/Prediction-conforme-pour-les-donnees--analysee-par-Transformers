{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Understanding the trajectory of text data in cancer studies\n",
    "## Troisieme rapport\n",
    "\n",
    "Dmytro Zhovtobriukh\n",
    "\\\n",
    "Anna Novototskykh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting signatory==1.2.6.1.9.0\n",
      "  Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: signatory\n",
      "  Building wheel for signatory (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for signatory: filename=signatory-1.2.6.1.9.0-cp311-cp311-linux_x86_64.whl size=250332 sha256=8664f40dcced5496bb9f7a7cc0ae05506db94ddff7cc95333ac74d4485da5a80\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ror_4ry/wheels/6a/79/bb/6012413145dd168da55413ef8bc837f507bf829a08a176c329\n",
      "Successfully built signatory\n",
      "Installing collected packages: signatory\n",
      "  Attempting uninstall: signatory\n",
      "    Found existing installation: signatory 1.2.6.1.9.0\n",
      "    Uninstalling signatory-1.2.6.1.9.0:\n",
      "      Successfully uninstalled signatory-1.2.6.1.9.0\n",
      "Successfully installed signatory-1.2.6.1.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in /opt/mamba/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/mamba/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/mamba/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"# Répertoire actuel\n",
    "current_directory = os.getcwd()\n",
    "print(\"Vous êtes dans le répertoire:\", current_directory)\n",
    "\n",
    "# Accéder au répertoire parent (A)\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Changer de répertoire de travail vers le répertoire parent\n",
    "os.chdir(parent_directory)\n",
    "# Vérifier le nouveau répertoire\n",
    "new_directory = os.getcwd()\n",
    "print(\"Vous êtes maintenant dans le répertoire:\", new_directory)\"\"\"\n",
    "\n",
    "# define whether to use demo dataset or the original\n",
    "USE_DEMO = False\n",
    "\n",
    "if USE_DEMO:\n",
    "    pth = 'C:\\\\Users\\\\pc\\\\Documents\\\\Recherche\\\\physionet.org\\\\files\\\\mimiciii-demo\\\\1.4'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv')]\n",
    "else:\n",
    "    pth = 'data/mimiciii'\n",
    "    files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid `ParserError: Error tokenizing data. C error: out of memory`\n",
    "\n",
    "https://stackoverflow.com/questions/41303246/error-tokenizing-data-c-error-out-of-memory-pandas-python-large-file-csv\n",
    "\n",
    "```python\n",
    "mylist = []\n",
    "for chunk in  pd.read_csv('train.csv', chunksize=20000):\n",
    "    mylist.append(chunk)\n",
    "big_data = pd.concat(mylist, axis= 0)\n",
    "del mylist\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "big_data = pd.read_csv('train.csv', engine='python')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "===============================ADMISSIONS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ADMITTIME\n",
    "DISCHTIME\n",
    "DEATHTIME\n",
    "ADMISSION_TYPE\n",
    "ADMISSION_LOCATION\n",
    "DISCHARGE_LOCATION\n",
    "INSURANCE\n",
    "LANGUAGE\n",
    "RELIGION\n",
    "MARITAL_STATUS\n",
    "ETHNICITY\n",
    "EDREGTIME\n",
    "EDOUTTIME\n",
    "DIAGNOSIS\n",
    "HOSPITAL_EXPIRE_FLAG\n",
    "HAS_CHARTEVENTS_DATA\n",
    "=================================CALLOUT.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SUBMIT_WARDID\n",
    "SUBMIT_CAREUNIT\n",
    "CURR_WARDID\n",
    "CURR_CAREUNIT\n",
    "CALLOUT_WARDID\n",
    "CALLOUT_SERVICE\n",
    "REQUEST_TELE\n",
    "REQUEST_RESP\n",
    "REQUEST_CDIFF\n",
    "REQUEST_MRSA\n",
    "REQUEST_VRE\n",
    "CALLOUT_STATUS\n",
    "CALLOUT_OUTCOME\n",
    "DISCHARGE_WARDID\n",
    "ACKNOWLEDGE_STATUS\n",
    "CREATETIME\n",
    "UPDATETIME\n",
    "ACKNOWLEDGETIME\n",
    "OUTCOMETIME\n",
    "FIRSTRESERVATIONTIME\n",
    "CURRENTRESERVATIONTIME\n",
    "===============================CAREGIVERS.csv.gz================================\n",
    "ROW_ID\n",
    "CGID\n",
    "LABEL\n",
    "DESCRIPTION\n",
    "===============================CHARTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "================================CPTEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "COSTCENTER\n",
    "CHARTDATE\n",
    "CPT_CD\n",
    "CPT_NUMBER\n",
    "CPT_SUFFIX\n",
    "TICKET_ID_SEQ\n",
    "SECTIONHEADER\n",
    "SUBSECTIONHEADER\n",
    "DESCRIPTION\n",
    "=============================DATETIMEEVENTS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CGID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "WARNING\n",
    "ERROR\n",
    "RESULTSTATUS\n",
    "STOPPED\n",
    "==============================DIAGNOSES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================DRGCODES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "DRG_TYPE\n",
    "DRG_CODE\n",
    "DESCRIPTION\n",
    "DRG_SEVERITY\n",
    "DRG_MORTALITY\n",
    "==================================D_CPT.csv.gz==================================\n",
    "ROW_ID\n",
    "CATEGORY\n",
    "SECTIONRANGE\n",
    "SECTIONHEADER\n",
    "SUBSECTIONRANGE\n",
    "SUBSECTIONHEADER\n",
    "CODESUFFIX\n",
    "MINCODEINSUBSECTION\n",
    "MAXCODEINSUBSECTION\n",
    "=============================D_ICD_DIAGNOSES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "============================D_ICD_PROCEDURES.csv.gz=============================\n",
    "ROW_ID\n",
    "ICD9_CODE\n",
    "SHORT_TITLE\n",
    "LONG_TITLE\n",
    "=================================D_ITEMS.csv.gz=================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "ABBREVIATION\n",
    "DBSOURCE\n",
    "LINKSTO\n",
    "CATEGORY\n",
    "UNITNAME\n",
    "PARAM_TYPE\n",
    "CONCEPTID\n",
    "===============================D_LABITEMS.csv.gz================================\n",
    "ROW_ID\n",
    "ITEMID\n",
    "LABEL\n",
    "FLUID\n",
    "CATEGORY\n",
    "LOINC_CODE\n",
    "================================ICUSTAYS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "FIRST_CAREUNIT\n",
    "LAST_CAREUNIT\n",
    "FIRST_WARDID\n",
    "LAST_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "=============================INPUTEVENTS_CV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALAMOUNTUOM\n",
    "ORIGINALROUTE\n",
    "ORIGINALRATE\n",
    "ORIGINALRATEUOM\n",
    "ORIGINALSITE\n",
    "=============================INPUTEVENTS_MV.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "AMOUNT\n",
    "AMOUNTUOM\n",
    "RATE\n",
    "RATEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCOMPONENTTYPEDESCRIPTION\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "PATIENTWEIGHT\n",
    "TOTALAMOUNT\n",
    "TOTALAMOUNTUOM\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "ORIGINALAMOUNT\n",
    "ORIGINALRATE\n",
    "================================LABEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ITEMID\n",
    "CHARTTIME\n",
    "VALUE\n",
    "VALUENUM\n",
    "VALUEUOM\n",
    "FLAG\n",
    "===========================MICROBIOLOGYEVENTS.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "SPEC_ITEMID\n",
    "SPEC_TYPE_DESC\n",
    "ORG_ITEMID\n",
    "ORG_NAME\n",
    "ISOLATE_NUM\n",
    "AB_ITEMID\n",
    "AB_NAME\n",
    "DILUTION_TEXT\n",
    "DILUTION_COMPARISON\n",
    "DILUTION_VALUE\n",
    "INTERPRETATION\n",
    "===============================NOTEEVENTS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "CHARTDATE\n",
    "CHARTTIME\n",
    "STORETIME\n",
    "CATEGORY\n",
    "DESCRIPTION\n",
    "CGID\n",
    "ISERROR\n",
    "TEXT\n",
    "==============================OUTPUTEVENTS.csv.gz===============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "CHARTTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "STORETIME\n",
    "CGID\n",
    "STOPPED\n",
    "NEWBOTTLE\n",
    "ISERROR\n",
    "================================PATIENTS.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "GENDER\n",
    "DOB\n",
    "DOD\n",
    "DOD_HOSP\n",
    "DOD_SSN\n",
    "EXPIRE_FLAG\n",
    "==============================PRESCRIPTIONS.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTDATE\n",
    "ENDDATE\n",
    "DRUG_TYPE\n",
    "DRUG\n",
    "DRUG_NAME_POE\n",
    "DRUG_NAME_GENERIC\n",
    "FORMULARY_DRUG_CD\n",
    "GSN\n",
    "NDC\n",
    "PROD_STRENGTH\n",
    "DOSE_VAL_RX\n",
    "DOSE_UNIT_RX\n",
    "FORM_VAL_DISP\n",
    "FORM_UNIT_DISP\n",
    "ROUTE\n",
    "===========================PROCEDUREEVENTS_MV.csv.gz============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "STARTTIME\n",
    "ENDTIME\n",
    "ITEMID\n",
    "VALUE\n",
    "VALUEUOM\n",
    "LOCATION\n",
    "LOCATIONCATEGORY\n",
    "STORETIME\n",
    "CGID\n",
    "ORDERID\n",
    "LINKORDERID\n",
    "ORDERCATEGORYNAME\n",
    "SECONDARYORDERCATEGORYNAME\n",
    "ORDERCATEGORYDESCRIPTION\n",
    "ISOPENBAG\n",
    "CONTINUEINNEXTDEPT\n",
    "CANCELREASON\n",
    "STATUSDESCRIPTION\n",
    "COMMENTS_EDITEDBY\n",
    "COMMENTS_CANCELEDBY\n",
    "COMMENTS_DATE\n",
    "=============================PROCEDURES_ICD.csv.gz==============================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "SEQ_NUM\n",
    "ICD9_CODE\n",
    "================================SERVICES.csv.gz=================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "TRANSFERTIME\n",
    "PREV_SERVICE\n",
    "CURR_SERVICE\n",
    "================================TRANSFERS.csv.gz================================\n",
    "ROW_ID\n",
    "SUBJECT_ID\n",
    "HADM_ID\n",
    "ICUSTAY_ID\n",
    "DBSOURCE\n",
    "EVENTTYPE\n",
    "PREV_CAREUNIT\n",
    "CURR_CAREUNIT\n",
    "PREV_WARDID\n",
    "CURR_WARDID\n",
    "INTIME\n",
    "OUTTIME\n",
    "LOS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=================================ADMISSIONS.csv=================================\n",
    "   row_id  subject_id  hadm_id            admittime            dischtime  \\\n",
    "0   12258       10006   142345  2164-10-23 21:09:00  2164-11-01 17:15:00   \n",
    "\n",
    "  deathtime admission_type    admission_location discharge_location insurance  \\\n",
    "0       NaN      EMERGENCY  EMERGENCY ROOM ADMIT   HOME HEALTH CARE  Medicare   \n",
    "\n",
    "  language  religion marital_status               ethnicity  \\\n",
    "0      NaN  CATHOLIC      SEPARATED  BLACK/AFRICAN AMERICAN   \n",
    "\n",
    "             edregtime            edouttime diagnosis  hospital_expire_flag  \\\n",
    "0  2164-10-23 16:43:00  2164-10-23 23:00:00    SEPSIS                     0   \n",
    "\n",
    "   has_chartevents_data  \n",
    "0                     1  \n",
    "==================================CALLOUT.csv===================================\n",
    "   row_id  subject_id  hadm_id  submit_wardid submit_careunit  curr_wardid  \\\n",
    "0    3917       10017   199207              7             NaN           45   \n",
    "\n",
    "  curr_careunit  callout_wardid callout_service  request_tele  ...  \\\n",
    "0           CCU               1             MED             1  ...   \n",
    "\n",
    "   callout_status  callout_outcome  discharge_wardid  acknowledge_status  \\\n",
    "0        Inactive       Discharged              45.0        Acknowledged   \n",
    "\n",
    "            createtime           updatetime      acknowledgetime  \\\n",
    "0  2149-05-31 10:44:34  2149-05-31 10:44:34  2149-05-31 15:08:04   \n",
    "\n",
    "           outcometime firstreservationtime currentreservationtime  \n",
    "0  2149-05-31 22:40:02                  NaN                    NaN  \n",
    "\n",
    "[1 rows x 24 columns]\n",
    "=================================CAREGIVERS.csv=================================\n",
    "   row_id   cgid label description\n",
    "0    2228  16174    RO   Read Only\n",
    "================================CHARTEVENTS.csv=================================\n",
    "    row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  5279021       40124   126179    279554.0  223761  2130-02-04 04:00:00   \n",
    "\n",
    "             storetime   cgid value  valuenum valueuom  warning  error  \\\n",
    "0  2130-02-04 04:35:00  19085  95.9      95.9       ?F      0.0    0.0   \n",
    "\n",
    "  resultstatus stopped  \n",
    "0          NaN     NaN  \n",
    "=================================CPTEVENTS.csv==================================\n",
    "   row_id  subject_id  hadm_id costcenter chartdate  cpt_cd  cpt_number  \\\n",
    "0    4615       10117   105150        ICU       NaN   99254       99254   \n",
    "\n",
    "   cpt_suffix  ticket_id_seq              sectionheader subsectionheader  \\\n",
    "0         NaN            1.0  Evaluation and management    Consultations   \n",
    "\n",
    "  description  \n",
    "0         NaN  \n",
    "===============================DATETIMEEVENTS.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id  itemid            charttime  \\\n",
    "0  208474       10076   198503    201006.0    5684  2107-03-25 04:00:00   \n",
    "\n",
    "             storetime   cgid                value valueuom  warning  error  \\\n",
    "0  2107-03-25 04:34:00  20482  2107-03-24 00:00:00     Date      NaN    NaN   \n",
    "\n",
    "   resultstatus   stopped  \n",
    "0           NaN  NotStopd  \n",
    "===============================DIAGNOSES_ICD.csv================================\n",
    "   row_id  subject_id  hadm_id  seq_num icd9_code\n",
    "0  112344       10006   142345        1     99591\n",
    "==================================DRGCODES.csv==================================\n",
    "   row_id  subject_id  hadm_id drg_type  drg_code  \\\n",
    "0    1338       10130   156668     HCFA       148   \n",
    "\n",
    "                                         description  drg_severity  \\\n",
    "0  MAJOR SMALL & LARGE BOWEL PROCEDURES WITH COMP...           NaN   \n",
    "\n",
    "   drg_mortality  \n",
    "0            NaN  \n",
    "===================================D_CPT.csv====================================\n",
    "   row_id  category sectionrange              sectionheader subsectionrange  \\\n",
    "0       1         1  99201-99499  Evaluation and management     99201-99216   \n",
    "\n",
    "                   subsectionheader codesuffix  mincodeinsubsection  \\\n",
    "0  Office/other outpatient services        NaN                99201   \n",
    "\n",
    "   maxcodeinsubsection  \n",
    "0                99216  \n",
    "==============================D_ICD_DIAGNOSES.csv===============================\n",
    "   row_id icd9_code              short_title  \\\n",
    "0       1     01716  Erythem nod tb-oth test   \n",
    "\n",
    "                                          long_title  \n",
    "0  Erythema nodosum with hypersensitivity reactio...  \n",
    "==============================D_ICD_PROCEDURES.csv==============================\n",
    "   row_id  icd9_code               short_title  \\\n",
    "0       1       1423  Chorioret les xenon coag   \n",
    "\n",
    "                                          long_title  \n",
    "0  Destruction of chorioretinal lesion by xenon a...  \n",
    "==================================D_ITEMS.csv===================================\n",
    "   row_id  itemid               label abbreviation dbsource      linksto  \\\n",
    "0       1    1435  Sustained Nystamus          NaN  carevue  chartevents   \n",
    "\n",
    "  category unitname param_type  conceptid  \n",
    "0      NaN      NaN        NaN        NaN  \n",
    "=================================D_LABITEMS.csv=================================\n",
    "   row_id  itemid          label  fluid   category loinc_code\n",
    "0       1   50800  SPECIMEN TYPE  BLOOD  BLOOD GAS        NaN\n",
    "==================================ICUSTAYS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource first_careunit  \\\n",
    "0   12742       10006   142345      206504  carevue           MICU   \n",
    "\n",
    "  last_careunit  first_wardid  last_wardid               intime  \\\n",
    "0          MICU            52           52  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime     los  \n",
    "0  2164-10-25 12:21:07  1.6325  \n",
    "===============================INPUTEVENTS_CV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    1184       10114   167957      234989  2171-11-03 15:00:00   30056   \n",
    "\n",
    "   amount amountuom  rate rateuom  ...  orderid  linkorderid  stopped  \\\n",
    "0   400.0        ml   NaN     NaN  ...  2557279      2557279      NaN   \n",
    "\n",
    "   newbottle originalamount  originalamountuom  originalroute originalrate  \\\n",
    "0        NaN            NaN                 ml           Oral          NaN   \n",
    "\n",
    "  originalrateuom  originalsite  \n",
    "0             NaN           NaN  \n",
    "\n",
    "[1 rows x 22 columns]\n",
    "===============================INPUTEVENTS_MV.csv===============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0  118897       42367   139932      250305  2147-10-29 16:45:00   \n",
    "\n",
    "               endtime  itemid  amount amountuom  rate  ... totalamountuom  \\\n",
    "0  2147-10-29 16:46:00  225799    60.0        ml   NaN  ...             ml   \n",
    "\n",
    "  isopenbag  continueinnextdept  cancelreason  statusdescription  \\\n",
    "0         0                   0             0    FinishedRunning   \n",
    "\n",
    "  comments_editedby comments_canceledby comments_date originalamount  \\\n",
    "0               NaN                 NaN           NaN           60.0   \n",
    "\n",
    "   originalrate  \n",
    "0          60.0  \n",
    "\n",
    "[1 rows x 31 columns]\n",
    "=================================LABEVENTS.csv==================================\n",
    "    row_id  subject_id  hadm_id  itemid            charttime value  valuenum  \\\n",
    "0  6244563       10006      NaN   50868  2164-09-24 20:21:00    19      19.0   \n",
    "\n",
    "  valueuom flag  \n",
    "0    mEq/L  NaN  \n",
    "=============================MICROBIOLOGYEVENTS.csv=============================\n",
    "   row_id  subject_id  hadm_id            chartdate            charttime  \\\n",
    "0  134694       10006   142345  2164-10-23 00:00:00  2164-10-23 15:30:00   \n",
    "\n",
    "   spec_itemid spec_type_desc  org_itemid                            org_name  \\\n",
    "0        70012  BLOOD CULTURE     80155.0  STAPHYLOCOCCUS, COAGULASE NEGATIVE   \n",
    "\n",
    "   isolate_num  ab_itemid ab_name dilution_text dilution_comparison  \\\n",
    "0          2.0        NaN     NaN           NaN                 NaN   \n",
    "\n",
    "   dilution_value interpretation  \n",
    "0             NaN            NaN  \n",
    "=================================NOTEEVENTS.csv=================================\n",
    "Empty DataFrame\n",
    "Columns: [row_id, subject_id, hadm_id, chartdate, charttime, storetime, category, description, cgid, iserror, text]\n",
    "Index: []\n",
    "================================OUTPUTEVENTS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            charttime  itemid  \\\n",
    "0    6540       10114   167957    234989.0  2171-10-30 20:00:00   40055   \n",
    "\n",
    "   value valueuom            storetime   cgid  stopped  newbottle  iserror  \n",
    "0   39.0       ml  2171-10-30 20:38:00  15029      NaN        NaN      NaN  \n",
    "==================================PATIENTS.csv==================================\n",
    "   row_id  subject_id gender                  dob                  dod  \\\n",
    "0    9467       10006      F  2094-03-05 00:00:00  2165-08-12 00:00:00   \n",
    "\n",
    "              dod_hosp              dod_ssn  expire_flag  \n",
    "0  2165-08-12 00:00:00  2165-08-12 00:00:00            1  \n",
    "===============================PRESCRIPTIONS.csv================================\n",
    "   row_id  subject_id  hadm_id  icustay_id            startdate  \\\n",
    "0   32600       42458   159647         NaN  2146-07-21 00:00:00   \n",
    "\n",
    "               enddate drug_type                         drug  \\\n",
    "0  2146-07-22 00:00:00      MAIN  Pneumococcal Vac Polyvalent   \n",
    "\n",
    "                 drug_name_poe            drug_name_generic formulary_drug_cd  \\\n",
    "0  Pneumococcal Vac Polyvalent  PNEUMOcoccal Vac Polyvalent           PNEU25I   \n",
    "\n",
    "       gsn        ndc     prod_strength dose_val_rx dose_unit_rx  \\\n",
    "0  48548.0  6494300.0  25mcg/0.5mL Vial         0.5           mL   \n",
    "\n",
    "  form_val_disp form_unit_disp route  \n",
    "0             1           VIAL    IM  \n",
    "=============================PROCEDUREEVENTS_MV.csv=============================\n",
    "   row_id  subject_id  hadm_id  icustay_id            starttime  \\\n",
    "0    8641       42367   139932      250305  2147-10-03 16:40:00   \n",
    "\n",
    "               endtime  itemid  value valueuom        location  ...  \\\n",
    "0  2147-10-06 20:00:00  224263   4520      min  Right Femoral.  ...   \n",
    "\n",
    "  ordercategoryname secondaryordercategoryname  ordercategorydescription  \\\n",
    "0    Invasive Lines                        NaN                      Task   \n",
    "\n",
    "   isopenbag  continueinnextdept cancelreason  statusdescription  \\\n",
    "0          1                   0            0    FinishedRunning   \n",
    "\n",
    "  comments_editedby  comments_canceledby  comments_date  \n",
    "0               NaN                  NaN            NaN  \n",
    "\n",
    "[1 rows x 25 columns]\n",
    "===============================PROCEDURES_ICD.csv===============================\n",
    "   row_id  subject_id  hadm_id  seq_num  icd9_code\n",
    "0    3994       10114   167957        1       3605\n",
    "==================================SERVICES.csv==================================\n",
    "   row_id  subject_id  hadm_id         transfertime prev_service curr_service\n",
    "0   14974       10006   142345  2164-10-23 21:10:15          NaN          MED\n",
    "=================================TRANSFERS.csv==================================\n",
    "   row_id  subject_id  hadm_id  icustay_id dbsource eventtype prev_careunit  \\\n",
    "0   54440       10006   142345    206504.0  carevue     admit           NaN   \n",
    "\n",
    "  curr_careunit  prev_wardid  curr_wardid               intime  \\\n",
    "0          MICU          NaN         52.0  2164-10-23 21:10:15   \n",
    "\n",
    "               outtime    los  \n",
    "0  2164-10-25 12:21:07  39.18  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données et les packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axJrX0R--kky"
   },
   "source": [
    "Dans cette section, nous incluons les bibliothèques nécessaires et chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures.\n",
    "\n",
    "Voici le but d'utilisation de modules:\n",
    "* Pandas - le module pour gérer facilement les tables (dataframes)\n",
    "* Numpy - le module de base pour la plupart de modules scientifiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,:]['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtaj1bVu-kky"
   },
   "outputs": [],
   "source": [
    "# Lire les données à partir d'un fichier CSV dans un DataFrame Pandas\n",
    "#data = pd.read_csv(\"/kaggle/input/clean-data/clean_data.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "# Convertir la colonne 'TIME' en entiers\n",
    "# data['TIME'] = data['TIME'].astype(int)\n",
    "data['TIME'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(adm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire les données d'admission à partir d'un autre fichier CSV dans un DataFrame Pandas\n",
    "# adm = pd.read_csv(\"/kaggle/input/clean-data/clean_adm.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITTIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMTTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"SUBJECT_ID\"].isin(data[\"SUBJECT_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHigEHe_-kk1"
   },
   "source": [
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T13:22:27.799253Z",
     "iopub.status.busy": "2023-08-27T13:22:27.798779Z",
     "iopub.status.idle": "2023-08-27T13:22:41.546351Z",
     "shell.execute_reply": "2023-08-27T13:22:41.544957Z",
     "shell.execute_reply.started": "2023-08-27T13:22:27.799212Z"
    },
    "id": "f3I6vt3q-kk1"
   },
   "outputs": [],
   "source": [
    "# Créer un dataframe vide avec deux colonnes pour le nouveau dataframe\n",
    "new_data = pd.DataFrame(columns=[\"SUBJECT_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
    "\n",
    "# Grouper les données d'admission par \"SUBJECT_ID\"\n",
    "grouped_adm = adm.groupby(\"SUBJECT_ID\")\n",
    "print(len(grouped_adm))\n",
    "\n",
    "# Parcourir chaque groupe de données associées à un \"SUBJECT_ID\"\n",
    "for subject_id, group in grouped_adm:\n",
    "    # Vérifier si le groupe contient au moins un enregistrement avec la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    if group[\"HOSPITAL_EXPIRE_FLAG\"].eq(1).any():\n",
    "        # Si oui, ajouter le \"SUBJECT_ID\" et la valeur 1 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [1]})], ignore_index=True)\n",
    "    else:\n",
    "        # Sinon, ajouter le \"SUBJECT_ID\" et la valeur 0 dans le nouveau dataframe\n",
    "        new_data = pd.concat([new_data, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [0]})], ignore_index=True)\n",
    "\n",
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "print(len(label_0))\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0\n",
    "label_0 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "print(len(label_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "# Fonction pour diviser les documents en plus petits morceaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVkUXJXk-kk1"
   },
   "source": [
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7I4Tdbb-kk2"
   },
   "source": [
    "# Jeux de données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QDtdhfB-kk2"
   },
   "source": [
    "Cette section prépare l'ensemble de données d'apprentissage en extrayant et traitant les embeddings ClinicalBERT à partir des informations relatives aux patients. Ces embeddings sont ensuite organisés en vue d'analyses ultérieures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## [TRAIN] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36wqGMV3-kk3"
   },
   "source": [
    "Dans cette partie, nous chargeons le modèle ClinicalBERT à partir du référentiel Hugging Face et sélectionnons un échantillon spécifique de patients pour l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\n",
    "sample = pd.concat([label_1.sample(n=1000, random_state=56), label_0.sample(n=1000, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\n",
    "filtered_data = data[data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'SUBJECT_ID' en agrégeant les listes de 'TEXT' et 'TIME'\n",
    "grouped_sample = filtered_data.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## [TRAIN] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgSkdofN-kk3"
   },
   "source": [
    "Dans cette partie, le code traite les données textuelles en les divisant en segments plus petits et extrait les embeddings ClinicalBERT correspondants à ces segments. Le résultat est un dictionnaire qui associe chaque patient à ses embeddings ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "time_list = []\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Parcourir les patients et leurs données associées\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer la liste des documents\n",
    "    times = values['TIME']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "    embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "        # Diviser le texte en parties égales\n",
    "        encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "        n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "        n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "        parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "        # Stocker les embeddings des différentes parties du document\n",
    "        cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for partie in parties:\n",
    "            # Convertir la partie dans un format compatible avec le modèle\n",
    "            inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # Stocker l'embedding dans la liste\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "            # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "            time_list.append(time)\n",
    "\n",
    "        # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "    embeddings_dict[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## [TRAIN] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yid4ozu9-kk4"
   },
   "source": [
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### [TRAIN] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "# Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings pour créer une matrice unique\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "# Diviser les embeddings réduits pour chaque patient\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### [TRAIN] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QjMOhW3-kk4"
   },
   "source": [
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings en une seule matrice\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "    # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Afficher la variance totale expliquée par toutes les composantes principales\n",
    "print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## [TRAIN] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHA3C_Ib-kk5"
   },
   "source": [
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronquée\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "for key, value in reduced_embeddings_dict.items():\n",
    "    # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "    tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "\n",
    "    # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "    # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "\n",
    "    # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "    log_signature_dict[key] = log_signature\n",
    "\n",
    "# À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(log_signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiJE3_iL-kk5"
   },
   "source": [
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.877945Z",
     "iopub.status.busy": "2023-08-27T15:50:54.877545Z",
     "iopub.status.idle": "2023-08-27T15:50:54.888896Z",
     "shell.execute_reply": "2023-08-27T15:50:54.887365Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.877912Z"
    },
    "id": "XZaS5YUq-kk6",
    "outputId": "470eeea0-678e-4f82-8b87-e5ab52c4a3f4"
   },
   "outputs": [],
   "source": [
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXfOmn2w-kk6"
   },
   "source": [
    "# Jeux de données test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7D4ONXb-kk6"
   },
   "source": [
    "Cette partie prépare un ensemble de données distinct pour les tests en choisissant au hasard un groupe de patients avec des étiquettes équilibrées de décès à l'hôpital. Ces données seront utilisées pour évaluer les performances du modèle sur de nouvelles observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI4KlKLo-kk6"
   },
   "source": [
    "## [TEST] Charger le modèle ClinicalBERT depuis Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_bJh3DE-kk6"
   },
   "source": [
    "Dans cette partie, le code charge le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients dans l'ensemble de données de test. Cela permet d'avoir accès au modèle pré-entraîné afin de traiter les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:54.891762Z",
     "iopub.status.busy": "2023-08-27T15:50:54.891334Z",
     "iopub.status.idle": "2023-08-27T15:50:57.431020Z",
     "shell.execute_reply": "2023-08-27T15:50:57.429444Z",
     "shell.execute_reply.started": "2023-08-27T15:50:54.891728Z"
    },
    "id": "ytlaOQLJ-kk6",
    "outputId": "9964e28b-6a66-47e5-e26a-e7e73547bf55"
   },
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle pré-entraîné Bio_ClinicalBERT et son tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "# Filtrer les données test (new_data) pour ne conserver que les patients absents dans le dictionnaire des embeddings (flattened_embeddings_dict)\n",
    "new_data_test = new_data[~new_data[\"SUBJECT_ID\"].isin(flattened_embeddings_dict.keys())]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1.sample(n=150, random_state=56), label_0.sample(n=850, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"SUBJECT_ID\"].isin(sample_test[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kQsnSe8-kk7"
   },
   "source": [
    "## [TEST] Extraire les tokens CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV77RZCc-kk7"
   },
   "source": [
    "Cette partie du code se focalise sur l'extraction des embeddings ClinicalBERT à partir du texte des patients présents dans le jeu de données de test. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Les embeddings de chaque patient sont sauvegardés pour une utilisation ultérieure lors de l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:35:17.753028Z",
     "iopub.status.busy": "2023-08-09T12:35:17.752625Z",
     "iopub.status.idle": "2023-08-09T15:35:47.773903Z",
     "shell.execute_reply": "2023-08-09T15:35:47.771299Z",
     "shell.execute_reply.started": "2023-08-09T12:35:17.752990Z"
    },
    "id": "ReL_-53L-kk7"
   },
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample_test.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Créer une liste pour stocker les valeurs de 'TIME' de chaque partie d'un document\n",
    "time_list_test = []\n",
    "\n",
    "# Créer un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict_test = {}\n",
    "\n",
    "# Parcourir les patients et leurs données textuelles\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer les documents textuels\n",
    "    times = values['TIME']  # Récupérer les valeurs de 'TIME' associées\n",
    "\n",
    "    embeddings_list = []  # Stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "\n",
    "        # Diviser le texte en parties de longueur appropriée\n",
    "        encoded_text = tokenizer.encode(text)\n",
    "        n_tokens = len(encoded_text)\n",
    "        n_chunks = max(1, n_tokens // 512)\n",
    "        windows = split_text(text, n_chunks)\n",
    "\n",
    "        cls_embeddings_list = []  # Stocker les embeddings CLS de chaque partie\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for window in windows:\n",
    "            # Convertir la fenêtre en un format compatible avec le modèle\n",
    "            inputs = tokenizer(window, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence sur le modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque fenêtre\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "\n",
    "            # Ajouter la valeur 'TIME' correspondante à la liste\n",
    "            time_list_test.append(time)\n",
    "\n",
    "        # Moyenne des embeddings CLS pour chaque fenêtre\n",
    "        #avg_cls_embedding = torch.mean(torch.stack(cls_embeddings_list), dim=0)\n",
    "        #embeddings_list.append(avg_cls_embedding)\n",
    "        # Ajouter la liste des embeddings CLS à la liste des embeddings pour ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans le dictionnaire avec le n° du patient comme clé\n",
    "    embeddings_dict_test[subject_id] = torch.stack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS0t1T5G-kk8"
   },
   "source": [
    "## [TEST] Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HDmvGSq-kk8"
   },
   "source": [
    "### [TEST] Projection gaussienne aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyIj1Zak-kk8"
   },
   "source": [
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:57.434945Z",
     "iopub.status.busy": "2023-08-27T15:50:57.434509Z",
     "iopub.status.idle": "2023-08-27T15:50:58.953013Z",
     "shell.execute_reply": "2023-08-27T15:50:58.951571Z",
     "shell.execute_reply.started": "2023-08-27T15:50:57.434911Z"
    },
    "id": "eKOGB5UY-kk8",
    "outputId": "66191fbd-a498-45a8-d3f8-8f1a11663b10"
   },
   "outputs": [],
   "source": [
    "# Importer la classe GaussianRandomProjection de la bibliothèque scikit-learn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque sujet\n",
    "lengths_test = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Appliquer la projection gaussienne aléatoire sur les embeddings\n",
    "reduced_embeddings_np_test = transformer.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths_test)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 101)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 101)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKwKl1V-kk8"
   },
   "source": [
    "### [TEST] ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEl9Ashr-kk8"
   },
   "source": [
    "La technique de l'Analyse en Composantes Principales (ACP) est utilisée pour réduire la dimensionnalité des plongements dans le jeu de données de test, tout en maintenant les informations pertinentes. De plus, cette méthode permet d'afficher la variance expliquée par chaque composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-27T14:35:06.528046Z",
     "iopub.status.busy": "2023-08-27T14:35:06.527090Z",
     "iopub.status.idle": "2023-08-27T14:35:10.238543Z",
     "shell.execute_reply": "2023-08-27T14:35:10.237285Z",
     "shell.execute_reply.started": "2023-08-27T14:35:06.528009Z"
    },
    "id": "Ptm55O3q-kk9",
    "outputId": "28717a71-974b-4b8c-e5e5-87a83a4c65d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice numpy\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser l'analyse en composantes principales (ACP) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle ACP sur les données et les transformer\n",
    "reduced_embeddings_np_test = pca.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 100)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    # Créer un tableau vide pour stocker les embeddings et le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        # Concaténer l'embedding réduit avec la valeur 'time'\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETKJU6AP-kk9"
   },
   "source": [
    "## [TEST] Calculer les signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGvVJ66Q-kk9"
   },
   "source": [
    "Ici, le programme calcule les signatures logarithmiques pour les embeddings des données de test. Ces signatures contiennent des informations plus avancées qui peuvent être bénéfiques pour l'entraînement du modèle. Le résultat est un dictionnaire où les embeddings de chaque patient sont représentés sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:58.954909Z",
     "iopub.status.busy": "2023-08-27T15:50:58.954544Z",
     "iopub.status.idle": "2023-08-27T15:50:59.778955Z",
     "shell.execute_reply": "2023-08-27T15:50:59.777505Z",
     "shell.execute_reply.started": "2023-08-27T15:50:58.954877Z"
    },
    "id": "fcqBX74m-kk9"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronqué\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict_test = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits pour chaque patient\n",
    "for key, value in reduced_embeddings_dict_test.items():\n",
    "    # Convertir le numpy array en un torch tensor et spécifier le type comme float\n",
    "    tensor = torch.from_numpy(value).float()\n",
    "    # Alternative : utiliser directement torch.tensor(value, dtype=torch.float)\n",
    "\n",
    "    # Ajouter une dimension supplémentaire pour correspondre à l'exigence du modèle (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Calculer la log signature en utilisant le modèle signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension batch que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0)\n",
    "\n",
    "    # Ajouter le résultat au dictionnaire avec le numéro du patient comme clé\n",
    "    log_signature_dict_test[key] = log_signature\n",
    "\n",
    "# Maintenant, log_signature_dict est un dictionnaire où chaque clé correspond à un patient et chaque valeur est la log signature de ce sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgoeroMB-kk-"
   },
   "source": [
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:09:07.874069Z",
     "iopub.status.busy": "2023-08-27T16:09:07.873428Z",
     "iopub.status.idle": "2023-08-27T16:09:07.883912Z",
     "shell.execute_reply": "2023-08-27T16:09:07.882548Z",
     "shell.execute_reply.started": "2023-08-27T16:09:07.874025Z"
    },
    "id": "b74xdF2K-kk-",
    "outputId": "a38e34db-3786-4170-ea4c-4ddb26ba7da3"
   },
   "outputs": [],
   "source": [
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 1 pour la colonne 'HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrement données traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/traite/data_train', 'wb') as f1:\n",
    "    data_train = pickle.dump(df_final, f1)\n",
    "with open('data/traite/data_test', 'wb') as f1:\n",
    "    data_test = pickle.dump(df_final_test, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation données pré-traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2         3         4         5         6     \\\n",
      "0  1.458817  4.473032  2.431972  2.288308  0.020622  1.843649 -2.540986   \n",
      "1  3.737214  7.210907  2.749512  1.521459  1.030078  1.512282 -3.087829   \n",
      "\n",
      "       7         8         9     ...      5141      5142      5143  5144  \\\n",
      "0 -2.913781  2.027094  0.210492  ...  0.074286 -0.014916  0.110722   0.0   \n",
      "1  1.846207 -1.026754 -0.349634  ...  0.009107 -0.038609  0.050572   0.0   \n",
      "\n",
      "       5145      5146  5147      5148  5149  5150  \n",
      "0  0.035938 -0.132720   0.0  0.036458   0.0   0.0  \n",
      "1  0.004521 -0.034182   0.0  0.123466   0.0   0.0  \n",
      "\n",
      "[2 rows x 5151 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/traite/data_train', 'rb') as f1:\n",
    "    df_final = pickle.load(f1)\n",
    "with open('data/traite/data_test', 'rb') as f1:\n",
    "    df_final_test = pickle.load(f1)\n",
    "\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "print(X_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "# Reg Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VFYF2nn-kk-"
   },
   "source": [
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 59.42%\n",
      "Rappel: 58.67%\n",
      "F1-score: 30.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + small sets pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical coverage is: 0.9160401002506265\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['1']\n",
      "The real value is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = model.predict_proba(X_test)\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + meilleur méthode ? pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n",
      "(200, 2)\n",
      "The empirical coverage is: 0.8822055137844611\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 1\n",
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = model.predict_proba(X_test)\n",
    "\n",
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode full conformal + small sets pour régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction set is: ['0', '1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 0\n",
      "The prediction set is: ['0']\n",
      "The real value is: 0\n",
      "The prediction set is: ['1']\n",
      "The real value is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=1000)\n",
    "\n",
    "def full_conformal(k):\n",
    "    alpha = 0.1 # 1-alpha is the desired coverage\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(5):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAFUB5vy-kk_"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGeOedqw-kk_"
   },
   "source": [
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T21:31:54.195444Z",
     "iopub.status.busy": "2023-08-26T21:31:54.194905Z",
     "iopub.status.idle": "2023-08-26T21:34:30.027397Z",
     "shell.execute_reply": "2023-08-26T21:34:30.025943Z",
     "shell.execute_reply.started": "2023-08-26T21:31:54.195399Z"
    },
    "id": "AYUwBEIa-kk_",
    "outputId": "8ba3b6c2-e274-4acd-d3a0-899e27ed97d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "best_model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest sur la valeur absolue des résidus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement\n",
    "r_train = abs(df_final[\"HOSPITAL_EXPIRE_FLAG\"] - y_pred)\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, r_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "best_model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "best_model.fit(X_train, r_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "r_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + small sets pour random  forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = best_model.predict_proba(X_test)\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode split conformal + meilleur méthode ? pour random  forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = best_model.predict_proba(X_test)\n",
    "\n",
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels].mean()\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme pb regression méthode split conformal pour random  forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = y_pred\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_value, val_value = y_test[idx], y_test[~idx]\n",
    "cal_residual, val_residual = r_pred[idx], r_pred[~idx]\n",
    "\n",
    "# cal_smx=E(Y|X), and cal_residual dans le même principe que stddev(Y|X)\n",
    "\n",
    "# 1: get conformal scores. \n",
    "scores = abs(cal_value-cal_smx)/cal_residual\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = torch.quantile(scores, q_level) # valeur du 9 ème décile environ\n",
    "\n",
    "prediction_sets = (val_smx-val_residual*qhat, val_smx+val_residual*qhat)\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "for i in range(10):\n",
    "    interval_min = prediction_sets[0][i]\n",
    "    interval_max = prediction_sets[1][i]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction conforme méthode full conformal + small sets pour random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "\n",
    "\n",
    "def full_conformal(k):\n",
    "    alpha = 0.1 # 1-alpha is the desired coverage\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB3vSRvc-kk_"
   },
   "source": [
    "# Sauvegarder les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KdnmRvR-kk_"
   },
   "source": [
    "Nous procédons à une sauvegarde des données en enregistrant les plongements traités ainsi que les valeurs temporelles dans des fichiers JSON et Pickle, afin de pouvoir les utiliser ou les analyser ultérieurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:12:12.863066Z",
     "iopub.status.busy": "2023-08-09T18:12:12.861870Z",
     "iopub.status.idle": "2023-08-09T18:12:37.434603Z",
     "shell.execute_reply": "2023-08-09T18:12:37.432844Z",
     "shell.execute_reply.started": "2023-08-09T18:12:12.863011Z"
    },
    "id": "5oTPx7uX-kk_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis convertis en listes\n",
    "flattened_embeddings_dict_converted = {}\n",
    "\n",
    "# Parcourir le dictionnaire flattened_embeddings_dict_test\n",
    "for clé, valeur in flattened_embeddings_dict_test.items():\n",
    "    # Convertir chaque valeur (numpy array) en liste et stocker dans le nouveau dictionnaire\n",
    "    flattened_embeddings_dict_converted[clé] = valeur.tolist()\n",
    "\n",
    "# Sauvegarder le dictionnaire converti en tant que fichier JSON\n",
    "\n",
    "# Spécifier le chemin du fichier JSON où vous souhaitez enregistrer les données\n",
    "with open('embeddings_dict_test.json', 'w') as fichier:\n",
    "    # Utiliser la bibliothèque JSON pour écrire le dictionnaire converti dans le fichier JSON\n",
    "    json.dump(flattened_embeddings_dict_converted, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T18:13:16.081552Z",
     "iopub.status.busy": "2023-08-09T18:13:16.081068Z",
     "iopub.status.idle": "2023-08-09T18:13:16.091682Z",
     "shell.execute_reply": "2023-08-09T18:13:16.090132Z",
     "shell.execute_reply.started": "2023-08-09T18:13:16.081516Z"
    },
    "id": "OZs6btSi-klA"
   },
   "outputs": [],
   "source": [
    "# Importer le module pour la serialization des objets\n",
    "import pickle\n",
    "\n",
    "# Sauvegarder la liste dans un fichier binaire (pickle)\n",
    "with open('time_list_test.pkl', 'wb') as f:\n",
    "    # Utiliser la bibliothèque pickle pour écrire la liste dans le fichier binaire\n",
    "    pickle.dump(time_list_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TLL-YYePVIg6"
   },
   "outputs": [],
   "source": [
    "!pip install nbconvert "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
