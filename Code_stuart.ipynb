{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP5FSMTSRKA5"
   },
   "source": [
    "# Prédiction conforme pour les données textuelles analysées par Transformers : application sur rapports médicaux\n",
    "\n",
    "Benoliel Stuart\n",
    "\\\n",
    "Carrere Charles\n",
    "\\\n",
    "Thomas Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.version\n",
    "\n",
    "# Importer les bibliothèques nécessaires\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 1\n",
    "pth = 'data/mimiciii'\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seulement si besoin d'appliquer le Bert\n",
    "%pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas transformers\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install quantile-forest\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore files in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Répertoire actuel\n",
    "current_directory = os.getcwd()\n",
    "print(\"Vous êtes dans le répertoire:\", current_directory)\n",
    "\n",
    "# Accéder au répertoire parent (A)\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Changer de répertoire de travail vers le répertoire parent\n",
    "os.chdir(parent_directory)\n",
    "# Vérifier le nouveau répertoire\n",
    "new_directory = os.getcwd()\n",
    "print(\"Vous êtes maintenant dans le répertoire:\", new_directory)\"\"\"\n",
    "\n",
    "files = [f for f in os.listdir(pth) if f.endswith('.csv.gz')]\n",
    "print(f'Found {len(files)} files')\n",
    "print('\\n'.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data fields in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print('\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for d in pd.read_csv(os.path.join(pth, f), low_memory=False, chunksize=200000):\n",
    "        df = d\n",
    "        break\n",
    "    print(f\"{f :=^80}\")\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgpweiHQ-kkw"
   },
   "source": [
    "# Importer les données + Stat descriptives\n",
    "Dans cette section, nous chargeons les données à partir de fichiers CSV dans des DataFrames Pandas. Nous effectuons également des transformations sur les types de données et procédons au nettoyage des données en vue d'analyses futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW_ID                                                       174\n",
      "SUBJECT_ID                                                 22532\n",
      "HADM_ID                                                   167853\n",
      "CHARTDATE                                    2151-08-04 00:00:00\n",
      "CHARTTIME                                                    NaT\n",
      "STORETIME                                                    NaN\n",
      "CATEGORY                                       Discharge summary\n",
      "DESCRIPTION                                               Report\n",
      "CGID                                                         NaN\n",
      "ISERROR                                                      NaN\n",
      "TEXT           Admission Date:  [**2151-7-16**]       Dischar...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'NOTEEVENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "len(data)\n",
    "print(data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0,:]['TEXT'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>165315</td>\n",
       "      <td>2196-04-09 12:26:00</td>\n",
       "      <td>2196-04-10 15:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DISC-TRAN CANCER/CHLDRN H</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2196-04-09 10:06:00</td>\n",
       "      <td>2196-04-09 13:24:00</td>\n",
       "      <td>BENZODIAZEPINE OVERDOSE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>152223</td>\n",
       "      <td>2153-09-03 07:15:00</td>\n",
       "      <td>2153-09-08 19:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>124321</td>\n",
       "      <td>2157-10-18 19:34:00</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>161859</td>\n",
       "      <td>2139-06-06 16:14:00</td>\n",
       "      <td>2139-06-09 12:48:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR MYOCARDIAL INFARCTION</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>129635</td>\n",
       "      <td>2160-11-02 02:06:00</td>\n",
       "      <td>2160-11-05 14:55:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2160-11-02 01:01:00</td>\n",
       "      <td>2160-11-02 04:27:00</td>\n",
       "      <td>ACUTE CORONARY SYNDROME</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58971</th>\n",
       "      <td>58594</td>\n",
       "      <td>98800</td>\n",
       "      <td>191113</td>\n",
       "      <td>2131-03-30 21:13:00</td>\n",
       "      <td>2131-04-02 15:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-03-30 19:44:00</td>\n",
       "      <td>2131-03-30 22:41:00</td>\n",
       "      <td>TRAUMA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58972</th>\n",
       "      <td>58595</td>\n",
       "      <td>98802</td>\n",
       "      <td>101071</td>\n",
       "      <td>2151-03-05 20:00:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>2151-03-06 09:10:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2151-03-05 17:23:00</td>\n",
       "      <td>2151-03-05 21:06:00</td>\n",
       "      <td>SAH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58973</th>\n",
       "      <td>58596</td>\n",
       "      <td>98805</td>\n",
       "      <td>122631</td>\n",
       "      <td>2200-09-12 07:15:00</td>\n",
       "      <td>2200-09-20 12:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENAL CANCER/SDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58974</th>\n",
       "      <td>58597</td>\n",
       "      <td>98813</td>\n",
       "      <td>170407</td>\n",
       "      <td>2128-11-11 02:29:00</td>\n",
       "      <td>2128-12-22 13:11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>SNF</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2128-11-10 23:48:00</td>\n",
       "      <td>2128-11-11 03:16:00</td>\n",
       "      <td>S/P FALL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58975</th>\n",
       "      <td>58598</td>\n",
       "      <td>98813</td>\n",
       "      <td>190264</td>\n",
       "      <td>2131-10-25 03:09:00</td>\n",
       "      <td>2131-10-26 17:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2131-10-25 00:08:00</td>\n",
       "      <td>2131-10-25 04:35:00</td>\n",
       "      <td>INTRACRANIAL HEMORRHAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58976 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
       "0          21          22   165315  2196-04-09 12:26:00  2196-04-10 15:54:00   \n",
       "1          22          23   152223  2153-09-03 07:15:00  2153-09-08 19:10:00   \n",
       "2          23          23   124321  2157-10-18 19:34:00  2157-10-25 14:00:00   \n",
       "3          24          24   161859  2139-06-06 16:14:00  2139-06-09 12:48:00   \n",
       "4          25          25   129635  2160-11-02 02:06:00  2160-11-05 14:55:00   \n",
       "...       ...         ...      ...                  ...                  ...   \n",
       "58971   58594       98800   191113  2131-03-30 21:13:00  2131-04-02 15:02:00   \n",
       "58972   58595       98802   101071  2151-03-05 20:00:00  2151-03-06 09:10:00   \n",
       "58973   58596       98805   122631  2200-09-12 07:15:00  2200-09-20 12:08:00   \n",
       "58974   58597       98813   170407  2128-11-11 02:29:00  2128-12-22 13:11:00   \n",
       "58975   58598       98813   190264  2131-10-25 03:09:00  2131-10-26 17:44:00   \n",
       "\n",
       "                 DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
       "0                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "1                      NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "2                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "3                      NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "4                      NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "...                    ...            ...                        ...   \n",
       "58971                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58972  2151-03-06 09:10:00      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "58973                  NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "58974                  NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "58975                  NaN      EMERGENCY  CLINIC REFERRAL/PREMATURE   \n",
       "\n",
       "              DISCHARGE_LOCATION INSURANCE LANGUAGE           RELIGION  \\\n",
       "0      DISC-TRAN CANCER/CHLDRN H   Private      NaN       UNOBTAINABLE   \n",
       "1               HOME HEALTH CARE  Medicare      NaN           CATHOLIC   \n",
       "2               HOME HEALTH CARE  Medicare     ENGL           CATHOLIC   \n",
       "3                           HOME   Private      NaN  PROTESTANT QUAKER   \n",
       "4                           HOME   Private      NaN       UNOBTAINABLE   \n",
       "...                          ...       ...      ...                ...   \n",
       "58971                       HOME   Private     ENGL      NOT SPECIFIED   \n",
       "58972               DEAD/EXPIRED  Medicare     ENGL           CATHOLIC   \n",
       "58973           HOME HEALTH CARE   Private     ENGL      NOT SPECIFIED   \n",
       "58974                        SNF   Private     ENGL           CATHOLIC   \n",
       "58975                       HOME   Private     ENGL           CATHOLIC   \n",
       "\n",
       "      MARITAL_STATUS ETHNICITY            EDREGTIME            EDOUTTIME  \\\n",
       "0            MARRIED     WHITE  2196-04-09 10:06:00  2196-04-09 13:24:00   \n",
       "1            MARRIED     WHITE                  NaN                  NaN   \n",
       "2            MARRIED     WHITE                  NaN                  NaN   \n",
       "3             SINGLE     WHITE                  NaN                  NaN   \n",
       "4            MARRIED     WHITE  2160-11-02 01:01:00  2160-11-02 04:27:00   \n",
       "...              ...       ...                  ...                  ...   \n",
       "58971         SINGLE     WHITE  2131-03-30 19:44:00  2131-03-30 22:41:00   \n",
       "58972        WIDOWED     WHITE  2151-03-05 17:23:00  2151-03-05 21:06:00   \n",
       "58973        MARRIED     WHITE                  NaN                  NaN   \n",
       "58974        MARRIED     WHITE  2128-11-10 23:48:00  2128-11-11 03:16:00   \n",
       "58975        MARRIED     WHITE  2131-10-25 00:08:00  2131-10-25 04:35:00   \n",
       "\n",
       "                                               DIAGNOSIS  \\\n",
       "0                                BENZODIAZEPINE OVERDOSE   \n",
       "1      CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...   \n",
       "2                                             BRAIN MASS   \n",
       "3                         INTERIOR MYOCARDIAL INFARCTION   \n",
       "4                                ACUTE CORONARY SYNDROME   \n",
       "...                                                  ...   \n",
       "58971                                             TRAUMA   \n",
       "58972                                                SAH   \n",
       "58973                                   RENAL CANCER/SDA   \n",
       "58974                                           S/P FALL   \n",
       "58975                            INTRACRANIAL HEMORRHAGE   \n",
       "\n",
       "       HOSPITAL_EXPIRE_FLAG  HAS_CHARTEVENTS_DATA  \n",
       "0                         0                     1  \n",
       "1                         0                     1  \n",
       "2                         0                     1  \n",
       "3                         0                     1  \n",
       "4                         0                     1  \n",
       "...                     ...                   ...  \n",
       "58971                     0                     1  \n",
       "58972                     1                     1  \n",
       "58973                     0                     1  \n",
       "58974                     0                     0  \n",
       "58975                     0                     1  \n",
       "\n",
       "[58976 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre d'admissions est de : 58289\n",
      "Le nombre d'individus est de : 46082\n"
     ]
    }
   ],
   "source": [
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMITTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DELTATIME'] = (adm['DISCHTIME'] - adm['ADMITTIME']).dt.total_seconds() / 3600\n",
    "adm = adm.drop(adm[adm['DELTATIME'] < 0].index)\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"HADM_ID\"].isin(data[\"HADM_ID\"].unique())]\n",
    "\n",
    "print(\"Le nombre d'admissions est de :\", len(adm))\n",
    "print(\"Le nombre d'individus est de :\", len(adm.groupby(\"SUBJECT_ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = pd.concat([chunk for chunk in pd.read_csv(os.path.join(pth, 'PATIENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "len(pat)\n",
    "pat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "df['DELTATIME'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('HOSPITAL_EXPIRE_FLAG')['DELTATIME'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les quantiles\n",
    "def quantile_plot(data, quantiles):\n",
    "    q_values = data.quantile(quantiles)\n",
    "    plt.plot(quantiles, q_values, marker='.', linestyle='-')\n",
    "\n",
    "df =  adm\n",
    "# Calcul des quantiles à afficher (par exemple, de 0 à 1 avec un pas de 0.01)\n",
    "quantiles = np.arange(0.01, 1.01, 0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 0\n",
    "data_0 = df[df['HOSPITAL_EXPIRE_FLAG'] == 0]['DELTATIME']\n",
    "quantile_plot(data_0, quantiles)\n",
    "    \n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 1\n",
    "data_1 = df[df['HOSPITAL_EXPIRE_FLAG'] == 1]['DELTATIME']\n",
    "quantile_plot(data_1, quantiles)\n",
    "\n",
    "plt.xlabel('Quantiles')\n",
    "plt.ylabel('DELTATIME')\n",
    "plt.title('Graphique de Quantiles pour DELTATIME par HOSPITAL_EXPIRE_FLAG')\n",
    "plt.legend(['Survivants (Flag=0)', 'Décédés (Flag=1)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 0\n",
    "data_0 = df[df['HOSPITAL_EXPIRE_FLAG'] == 0]['DELTATIME']\n",
    "quantile_plot(data_0, quantiles)\n",
    "    \n",
    "# Plot pour 'HOSPITAL_EXPIRE_FLAG' = 1\n",
    "data_1 = df[df['HOSPITAL_EXPIRE_FLAG'] == 1]['DELTATIME']\n",
    "quantile_plot(data_1, quantiles)\n",
    "\n",
    "plt.xlabel('Quantiles')\n",
    "plt.xlim(left = 0, right = 1)\n",
    "plt.ylabel('DELTATIME')\n",
    "plt.ylim(0, 1000)\n",
    "plt.title('Graphique de Quantiles pour DELTATIME par HOSPITAL_EXPIRE_FLAG')\n",
    "plt.legend(['Survivants (Flag=0)', 'Décédés (Flag=1)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "# Réglage du style (optionnel)\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=50, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme empilé de DELTATIME par HOSPITAL_EXPIRE_FLAG ')\n",
    "plt.legend(['Décédés (Flag=1)', 'Survivants (Flag=0)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm\n",
    "# Réglage du style (optionnel)\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "histogram = sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme empilé de DELTATIME par HOSPITAL_EXPIRE_FLAG ')\n",
    "plt.legend(['Décédés (Flag=1)', 'Survivants (Flag=0)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==1]\n",
    "sns.set(style='ticks')\n",
    "palette = sns.color_palette(\"deep\")\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", color = palette[1])\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les décès')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==0]\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=10, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 0, right=1000)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les survivants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  adm[adm['HOSPITAL_EXPIRE_FLAG']==0]\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Création de l'histogramme empilé avec Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='DELTATIME', hue='HOSPITAL_EXPIRE_FLAG', binwidth=1, kde=False, stat='density', common_norm=False, multiple='stack', element=\"bars\", palette=\"deep\")\n",
    "\n",
    "plt.xlabel('DELTATIME')\n",
    "plt.xlim(left = 20, right=60)\n",
    "plt.ylabel('Fréquence relative')\n",
    "plt.title('Histogramme de DELTATIME pour les survivants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQDd6ljc-kk0"
   },
   "source": [
    "# Créer les jeux de test et d'entraînement\n",
    "Cette section concerne la création des ensembles de données utilisés pour l'apprentissage et les tests. Nous regroupons les données relatives aux patients selon leurs identifiants uniques, puis nous les étiquetons en fonction de la présence ou non d'une condition spécifique. Cette étape permet ainsi la formation de l'ensemble d'apprentissage. De plus, nous sélectionnons aléatoirement un sous-ensemble de patients pour constituer l'ensemble de test.\n",
    "\n",
    "Le but de cet étape est la division le jeu de données pour laisser entrainer le modèle et ainsi le évaluer. La division raisonnable est crucial pour obtenir un vrai metrique de modèle et aussi éviter \"overfitting\" ou \"underfitting\".\n",
    "\n",
    "Puis, le modèle s'entrainera sur le jeu d'entrainement et après on calcule la metrique sur le jeu de test. Ce metrique montre comment notre modèle marche sur les données reéls et ainsi on peut comparer les modèles differentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrysD5b-kk1"
   },
   "source": [
    "## Fonction pour diviser les documents en plus petits morceaux\n",
    "Dans cette partie, nous définissons une fonction permettant de découper les documents textuels en segments plus petits afin de faciliter leur traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T12:33:01.473589Z",
     "iopub.status.busy": "2023-08-09T12:33:01.472819Z",
     "iopub.status.idle": "2023-08-09T12:33:01.483123Z",
     "shell.execute_reply": "2023-08-09T12:33:01.482065Z",
     "shell.execute_reply.started": "2023-08-09T12:33:01.473543Z"
    },
    "id": "YbFqGuQK-kk1"
   },
   "outputs": [],
   "source": [
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0IORYrm-kk2"
   },
   "source": [
    "## Charger le modèle ClinicalBERT depuis Hugging Face\n",
    "Dans cette partie, nous chargeons le modèle ClinicalBERT ainsi que son tokenizer depuis la bibliothèque Hugging Face. Ces éléments sont indispensables pour extraire les représentations vectorielles à partir des informations textuelles des patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "\n",
    "# Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "print('Using device:', device)\n",
    "\n",
    "def calculate_days_since_earliest_date(dates_list):\n",
    "    earliest_date = min(dates_list)\n",
    "    return [(date - earliest_date).days for date in dates_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T12:30:17.771238Z",
     "iopub.status.idle": "2023-08-09T12:30:17.771592Z",
     "shell.execute_reply": "2023-08-09T12:30:17.771429Z",
     "shell.execute_reply.started": "2023-08-09T12:30:17.771412Z"
    },
    "id": "BsQ6h7_Y-kk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre d'individus morts est de : 5582\n",
      "Le nombre d'individus ayant toujours survécu est de : 52707\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CHARTDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100003</td>\n",
       "      <td>[Admission Date:  [**2150-4-17**]             ...</td>\n",
       "      <td>[4, 1, 0, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100028</td>\n",
       "      <td>[Admission Date:  [**2142-12-23**]            ...</td>\n",
       "      <td>[7, 5, 0, 0, 5, 3, 1, 2, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100053</td>\n",
       "      <td>[Admission Date:  [**2124-7-14**]             ...</td>\n",
       "      <td>[5, 2, 2, 0, 0, 0, 1, 3, 3, 1, 0, 4, 0, 0, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100061</td>\n",
       "      <td>[Admission Date:  [**2178-12-25**]            ...</td>\n",
       "      <td>[2, 2, 1, 0, 2, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100068</td>\n",
       "      <td>[Admission Date:  [**2192-1-5**]              ...</td>\n",
       "      <td>[15, 8, 1, 14, 8, 7, 0, 0, 0, 12, 12, 12, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>199844</td>\n",
       "      <td>[Admission Date:  [**2168-7-26**]             ...</td>\n",
       "      <td>[5, 1, 3, 2, 1, 0, 4, 4, 0, 4, 4, 0, 0, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>199882</td>\n",
       "      <td>[Admission Date:  [**2197-1-7**]     Discharge...</td>\n",
       "      <td>[12, 11, 5, 4, 3, 2, 1, 0, 0, 0, 12, 8, 11, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>199912</td>\n",
       "      <td>[Admission Date:  [**2198-9-19**]             ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>199919</td>\n",
       "      <td>[Admission Date:  [**2178-9-20**]             ...</td>\n",
       "      <td>[8, 8, 7, 5, 0, 7, 3, 1, 4, 3, 4, 4, 4, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>199964</td>\n",
       "      <td>[Admission Date:  [**2149-1-14**]     Discharg...</td>\n",
       "      <td>[3, 12, 3, 2, 0, 0, 7, 5, 6, 6, 6, 7, 1, 1, 2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HADM_ID                                               TEXT  \\\n",
       "0     100003  [Admission Date:  [**2150-4-17**]             ...   \n",
       "1     100028  [Admission Date:  [**2142-12-23**]            ...   \n",
       "2     100053  [Admission Date:  [**2124-7-14**]             ...   \n",
       "3     100061  [Admission Date:  [**2178-12-25**]            ...   \n",
       "4     100068  [Admission Date:  [**2192-1-5**]              ...   \n",
       "...      ...                                                ...   \n",
       "4995  199844  [Admission Date:  [**2168-7-26**]             ...   \n",
       "4996  199882  [Admission Date:  [**2197-1-7**]     Discharge...   \n",
       "4997  199912  [Admission Date:  [**2198-9-19**]             ...   \n",
       "4998  199919  [Admission Date:  [**2178-9-20**]             ...   \n",
       "4999  199964  [Admission Date:  [**2149-1-14**]     Discharg...   \n",
       "\n",
       "                                              CHARTDATE  \n",
       "0     [4, 1, 0, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, ...  \n",
       "1                     [7, 5, 0, 0, 5, 3, 1, 2, 0, 2, 0]  \n",
       "2     [5, 2, 2, 0, 0, 0, 1, 3, 3, 1, 0, 4, 0, 0, 5, ...  \n",
       "3                           [2, 2, 1, 0, 2, 0, 1, 0, 0]  \n",
       "4     [15, 8, 1, 14, 8, 7, 0, 0, 0, 12, 12, 12, 12, ...  \n",
       "...                                                 ...  \n",
       "4995  [5, 1, 3, 2, 1, 0, 4, 4, 0, 4, 4, 0, 0, 2, 2, ...  \n",
       "4996  [12, 11, 5, 4, 3, 2, 1, 0, 0, 0, 12, 8, 11, 3, 0]  \n",
       "4997            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]  \n",
       "4998  [8, 8, 7, 5, 0, 7, 3, 1, 4, 3, 4, 4, 4, 3, 3, ...  \n",
       "4999  [3, 12, 3, 2, 0, 0, 7, 5, 6, 6, 6, 7, 1, 1, 2,...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1 = adm[adm[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "print(\"Le nombre d'individus morts est de :\", len(label_1))\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0 ???\n",
    "label_0 = adm[adm[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "print(\"Le nombre d'individus ayant toujours survécu est de :\", len(label_0))\n",
    "\n",
    "# Sélectionner aléatoirement 2500 individus de chaque classe (label_1 et label_0)\n",
    "sample = pd.concat([label_1.sample(n=2500, random_state=seed), label_0.sample(n=2500, random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer le dataframe de données en ne conservant que les patients sélectionnés précédemment\n",
    "filtered_data = data[data[\"HADM_ID\"].isin(sample[\"HADM_ID\"].values)]\n",
    "\n",
    "# Regrouper les données filtrées par 'HADM_ID' en agrégeant les listes de 'TEXT' et 'TIME'\n",
    "grouped_sample = filtered_data.groupby('HADM_ID').agg({'TEXT': list, 'CHARTDATE': list}).reset_index()\n",
    "\n",
    "# Appliquer cette fonction à chaque ligne de la colonne CHARTDATE\n",
    "grouped_sample['CHARTDATE'] = grouped_sample['CHARTDATE'].apply(calculate_days_since_earliest_date)\n",
    "\n",
    "grouped_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CHARTDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100150</td>\n",
       "      <td>[Admission Date:  [**2175-8-10**]             ...</td>\n",
       "      <td>[12, 4, 1, 2, 0, 2, 3, 3, 0, 3, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100185</td>\n",
       "      <td>[Admission Date:  [**2162-10-10**]            ...</td>\n",
       "      <td>[4, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100191</td>\n",
       "      <td>[Admission Date:  [**2146-11-17**]            ...</td>\n",
       "      <td>[8, 4, 0, 5, 0, 0, 5, 0, 7, 8, 1, 2, 1, 2, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100427</td>\n",
       "      <td>[Admission Date:  [**2142-4-25**]             ...</td>\n",
       "      <td>[4, 1, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100522</td>\n",
       "      <td>[Admission Date:  [**2139-7-22**]             ...</td>\n",
       "      <td>[15, 0, 5, 0, 0, 3, 0, 3, 9, 0, 4, 4, 4, 5, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>199741</td>\n",
       "      <td>[Admission Date:  [**2111-7-15**]             ...</td>\n",
       "      <td>[2, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>199789</td>\n",
       "      <td>[Admission Date:  [**2176-5-9**]              ...</td>\n",
       "      <td>[6, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>199840</td>\n",
       "      <td>[Admission Date:  [**2159-2-10**]     Discharg...</td>\n",
       "      <td>[1, 0, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>199871</td>\n",
       "      <td>[Admission Date:  [**2201-11-11**]            ...</td>\n",
       "      <td>[9, 4, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>199968</td>\n",
       "      <td>[Admission Date:  [**2107-11-4**]     Discharg...</td>\n",
       "      <td>[8, 0, 1, 0, 2, 4, 6, 6, 5, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    HADM_ID                                               TEXT  \\\n",
       "0    100150  [Admission Date:  [**2175-8-10**]             ...   \n",
       "1    100185  [Admission Date:  [**2162-10-10**]            ...   \n",
       "2    100191  [Admission Date:  [**2146-11-17**]            ...   \n",
       "3    100427  [Admission Date:  [**2142-4-25**]             ...   \n",
       "4    100522  [Admission Date:  [**2139-7-22**]             ...   \n",
       "..      ...                                                ...   \n",
       "995  199741  [Admission Date:  [**2111-7-15**]             ...   \n",
       "996  199789  [Admission Date:  [**2176-5-9**]              ...   \n",
       "997  199840  [Admission Date:  [**2159-2-10**]     Discharg...   \n",
       "998  199871  [Admission Date:  [**2201-11-11**]            ...   \n",
       "999  199968  [Admission Date:  [**2107-11-4**]     Discharg...   \n",
       "\n",
       "                                             CHARTDATE  \n",
       "0                [12, 4, 1, 2, 0, 2, 3, 3, 0, 3, 3, 4]  \n",
       "1     [4, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0]  \n",
       "2    [8, 4, 0, 5, 0, 0, 5, 0, 7, 8, 1, 2, 1, 2, 1, ...  \n",
       "3                                [4, 1, 0, 0, 0, 0, 1]  \n",
       "4    [15, 0, 5, 0, 0, 3, 0, 3, 9, 0, 4, 4, 4, 5, 5,...  \n",
       "..                                                 ...  \n",
       "995                                       [2, 0, 0, 2]  \n",
       "996  [6, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "997                                    [1, 0, 1, 0, 1]  \n",
       "998                                       [9, 4, 0, 2]  \n",
       "999                     [8, 0, 1, 0, 2, 4, 6, 6, 5, 6]  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrer les données test (new_data) pour ne conserver que les patients absents\n",
    "new_data_test = adm[~adm[\"HADM_ID\"].isin(sample[\"HADM_ID\"].values)]\n",
    "\n",
    "# Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "label_1 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "label_0 = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1.sample(n=150, random_state=seed), label_0.sample(n=850, random_state=seed+1)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"HADM_ID\"].isin(sample_test[\"HADM_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"HADM_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('HADM_ID').agg({'TEXT': list, 'CHARTDATE': list}).reset_index()\n",
    "\n",
    "# Appliquer cette fonction à chaque ligne de la colonne CHARTDATE\n",
    "grouped_sample_test['CHARTDATE'] = grouped_sample_test['CHARTDATE'].apply(calculate_days_since_earliest_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5mqfM-b-kk3"
   },
   "source": [
    "## Extraire les tokens CLS\n",
    "Dans cette partie, le code se focalise sur l'extraction des embeddings ClinicalBERT. Les embeddings sont extraits en découpant le texte en parts et en calculant les représentations pour chaque part. Le résultat est un dictionnaire qui associe chaque patient à ses embeddings ClinicalBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token(grouped_sample):\n",
    "    # Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "    grouped_texts_dict = grouped_sample.set_index('HADM_ID')[['TEXT', 'CHARTDATE']].to_dict(orient='index')\n",
    "\n",
    "    # Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "    time_list = []\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les embeddings\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    # Parcourir les patients et leurs données associées\n",
    "    for subject_id, values in grouped_texts_dict.items():\n",
    "        texts = values['TEXT']  # Récupérer la liste des documents\n",
    "        times = values['CHARTDATE']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "        embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "        # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "        for text, time in zip(texts, times):\n",
    "            # Diviser le texte en parties égales\n",
    "            encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "            n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "            n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "            parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "\n",
    "            # Stocker les embeddings des différentes parties du document\n",
    "            cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "\n",
    "            # Parcourir les parties du document\n",
    "            for partie in parties:\n",
    "                # Convertir la partie dans un format compatible avec le modèle\n",
    "                inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                    outputs = model(**inputs)\n",
    "\n",
    "                # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                # Stocker l'embedding dans la liste\n",
    "                cls_embeddings_list.append(cls_embeddings)\n",
    "                # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "                time_list.append(time)\n",
    "\n",
    "            # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "            embeddings_list += cls_embeddings_list\n",
    "\n",
    "        # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "        embeddings_dict[subject_id] = torch.stack(embeddings_list)\n",
    "    return embeddings_dict , time_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxU5oJhf-kk3"
   },
   "outputs": [],
   "source": [
    "embeddings_dict, time_list = extract_token(grouped_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_test, time_list_test = extract_token(grouped_sample_test)\n",
    "len(embeddings_dict_test)\n",
    "len(time_list_test)\n",
    "time_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmzH1gbB-kk3"
   },
   "source": [
    "## Réduction de dimension\n",
    "Ici, le code se concentre sur la réduction de la dimensionnalité des embeddings obtenus lors de l'étape précédente. Il utilise une technique appelée projection gaussienne aléatoire pour transformer les embeddings dans un espace de dimension inférieure, ce qui rend les données plus gérables et peut potentiellement améliorer les performances du modèle. Le résultat est un dictionnaire contenant les embeddings réduits pour chaque patient.\n",
    "\n",
    "Le but de cette action c'est de faire plus simple le modele donc il demande moins de ressources pour entrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIx4IAnK-kk4"
   },
   "source": [
    "### Projection gaussienne aléatoire\n",
    "Cette partie du code a pour objectif de réduire la dimensionnalité des embeddings extraits lors de l'étape précédente en utilisant une projection gaussienne aléatoire. Le résultat est un ensemble d'embeddings de plus petite dimension qui peut faciliter l'analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe random_projection du module sklearn\n",
    "from sklearn import random_projection\n",
    "\n",
    "def project_gaussian(embeddings_dict, time_list):\n",
    "\n",
    "    # ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "    # Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings pour créer une matrice unique\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "    transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "    # Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "    reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "\n",
    "    # Diviser les embeddings réduits pour chaque patient\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    return reduced_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:15:20.284244Z",
     "iopub.status.busy": "2023-08-27T15:15:20.283833Z",
     "iopub.status.idle": "2023-08-27T15:15:23.514131Z",
     "shell.execute_reply": "2023-08-27T15:15:23.512645Z",
     "shell.execute_reply.started": "2023-08-27T15:15:20.284205Z"
    },
    "id": "GFp3EdsS-kk4",
    "outputId": "e6dc4516-2302-4cae-dea1-099c34e4eb94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = project_gaussian(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = project_gaussian(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DBRUdO8-kk4"
   },
   "source": [
    "### ACP\n",
    "Dans cette section, nous appliquons une Analyse en Composantes Principales (ACP) aux embeddings. L'objectif de l'ACP est de réduire davantage la dimensionnalité des données tout en préservant autant d'informations que possible. Le code calcule la variance expliquée par chaque composante principale, ce qui permet d'évaluer l'efficacité de la réduction de dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def ACP(embeddings_dict, time_list):\n",
    "    # Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "\n",
    "    # Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "    # Concaténer tous les embeddings en une seule matrice\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "    # Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "    pca = PCA(n_components=100)\n",
    "\n",
    "    # Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "    reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    # Séparer les embeddings transformés pour chaque sujet\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "    # Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    # Afficher la variance expliquée par chaque composante principale\n",
    "    print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "    # Afficher la variance totale expliquée par toutes les composantes principales\n",
    "    print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "    return reduced_embeddings_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T14:11:57.027640Z",
     "iopub.status.busy": "2023-08-27T14:11:57.027069Z",
     "iopub.status.idle": "2023-08-27T14:12:03.012113Z",
     "shell.execute_reply": "2023-08-27T14:12:03.010510Z",
     "shell.execute_reply.started": "2023-08-27T14:11:57.027599Z"
    },
    "id": "SVFMQ1mo-kk4",
    "outputId": "103279a3-ab20-41a5-b82e-126d29bf8a66"
   },
   "outputs": [],
   "source": [
    "reduced_embeddings_dict = ACP(embeddings_dict, time_list)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_dict_test = ACP(embeddings_dict_test, time_list_test)\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciTWZtgK-kk5"
   },
   "source": [
    "## Calculer les signatures\n",
    "Cette partie du code calcule les signatures logarithmiques pour les embeddings réduits.\n",
    "Les signatures logarithmiques capturent des informations plus complexes dans les données, ce qui peut être très utile lors de l'entraînement d'un modèle de prédiction. Cela aboutit à la création d'un dictionnaire où chaque patient est représenté par des plongements sous forme de signatures logarithmiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import signatory\n",
    "\n",
    "def log_signa(reduced_embeddings_dict):\n",
    "\n",
    "    # Ordre de la signature tronquée\n",
    "    depth = 2\n",
    "\n",
    "    # Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "    log_signature_dict = {}\n",
    "\n",
    "    # Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "    for key, value in reduced_embeddings_dict.items():\n",
    "        # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "        tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "\n",
    "        # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "        tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "        log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "        # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "        log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "\n",
    "        # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "        log_signature_dict[key] = log_signature\n",
    "    return log_signature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRAIN] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:48.981427Z",
     "iopub.status.busy": "2023-08-27T15:25:48.980891Z",
     "iopub.status.idle": "2023-08-27T15:25:50.470327Z",
     "shell.execute_reply": "2023-08-27T15:25:50.468937Z",
     "shell.execute_reply.started": "2023-08-27T15:25:48.981386Z"
    },
    "id": "07AM0RJJ-kk5"
   },
   "outputs": [],
   "source": [
    "log_signature_dict = log_signa(reduced_embeddings_dict)\n",
    "# À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n",
    "print(log_signature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TEST] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_signature_dict_test = log_signa(reduced_embeddings_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugQsRJ--kk5"
   },
   "source": [
    "## [TRAIN] Dataframe utilisé pour l'entraînement\n",
    "Après avoir effectué l'extraction, la réduction de dimension et le calcul des signatures logarithmiques pour les embeddings, le code transforme les résultats en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:25:52.432748Z",
     "iopub.status.busy": "2023-08-27T15:25:52.432281Z",
     "iopub.status.idle": "2023-08-27T15:50:54.875126Z",
     "shell.execute_reply": "2023-08-27T15:50:54.873249Z",
     "shell.execute_reply.started": "2023-08-27T15:25:52.432711Z"
    },
    "id": "H6FB6hfq-kk5"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "\n",
    "# Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "for col in df_features.columns:\n",
    "    df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "# Afficher la forme (nombre de lignes et de colonnes) du DataFrame df_final\n",
    "df_final.shape\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbm5Azec-kk9"
   },
   "source": [
    "## [TEST] Dataframe utilisé pour le test\n",
    "Enfin, cette partie transforme les résultats de l'analyse en un DataFrame Pandas prêt à être utilisé pour évaluer comment le modèle se comporte sur le jeu de données de test. Ce DataFrame comprend également les étiquettes des patients, ce qui facilite l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T15:50:59.782003Z",
     "iopub.status.busy": "2023-08-27T15:50:59.781511Z",
     "iopub.status.idle": "2023-08-27T16:01:06.286918Z",
     "shell.execute_reply": "2023-08-27T16:01:06.285520Z",
     "shell.execute_reply.started": "2023-08-27T15:50:59.781960Z"
    },
    "id": "menpz9WR-kk-"
   },
   "outputs": [],
   "source": [
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "# Afficher le nombre de colonnes du DataFrame final (nombre de caractéristiques + 1 pour la colonne 'HOSPITAL_EXPIRE_FLAG')\n",
    "\n",
    "df_final_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement données traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'wb') as f1:\n",
    "    pickle.dump(df_final, f1)\n",
    "with open('data/traite/data_test', 'wb') as f1:\n",
    "    pickle.dump(df_final_test, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation données pré-traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/data_train', 'rb') as f1:\n",
    "    df_final = pickle.load(f1)\n",
    "with open('data/traite/data_test', 'rb') as f1:\n",
    "    df_final_test = pickle.load(f1)\n",
    "\n",
    "print(\"Dataset: %s\" % (df_final.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (df_final_test.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_cqr', 'rb') as f1:\n",
    "    pred_cqr = pickle.load(f1)\n",
    "with open('data/traite/pred_rf', 'rb') as f1:\n",
    "    pred_rf = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cqr', 'rb') as f1:\n",
    "    y_lower_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cqr', 'rb') as f1:\n",
    "    y_upper_cqr = pickle.load(f1)\n",
    "with open('data/traite/y_lower_cvp', 'rb') as f1:\n",
    "    y_lower_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_upper_cvp', 'rb') as f1:\n",
    "    y_upper_cvp = pickle.load(f1)\n",
    "with open('data/traite/y_lower_split', 'rb') as f1:\n",
    "    y_lower_split = pickle.load(f1)\n",
    "with open('data/traite/y_upper_split', 'rb') as f1:\n",
    "    y_upper_split = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(y,\n",
    "              y_u=None,\n",
    "              y_l=None,\n",
    "              pred=None,\n",
    "              shade_color=\"\",\n",
    "              method_name=\"\",\n",
    "              title=\"\",\n",
    "              filename=None,\n",
    "              save_figures=False,\n",
    "              max_show=100):\n",
    "    \n",
    "    \"\"\" Scatter plot of (x,y) points along with the constructed prediction interval \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array, target response variable (length n)\n",
    "    pred : numpy array, the estimated prediction. It may be the conditional mean,\n",
    "           or low and high conditional quantiles.\n",
    "    shade_color : string, desired color of the prediciton interval\n",
    "    method_name : string, name of the method\n",
    "    title : string, the title of the figure\n",
    "    filename : sting, name of the file to save the figure\n",
    "    save_figures : boolean, save the figure (True) or not (False)\n",
    "    \n",
    "    \"\"\"\n",
    "    y_ = y[:max_show]\n",
    "    x_ = np.arange(1, len(y_) + 1)\n",
    "\n",
    "    if y_u is not None:\n",
    "        y_u_ = y_u[:max_show]\n",
    "    if y_l is not None:\n",
    "        y_l_ = y_l[:max_show]\n",
    "    if pred is not None:\n",
    "        pred_ = pred[:max_show]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x_, y_, 'k.', alpha=.2, markersize=10,\n",
    "             fillstyle='none', label=u'Observations')\n",
    "    \n",
    "    if (y_u is not None) and (y_l is not None):\n",
    "        plt.fill(np.concatenate([x_, x_[::-1]]),\n",
    "                 np.concatenate([y_u_, y_l_[::-1]]),\n",
    "                 alpha=.3, fc=shade_color, ec='None',\n",
    "                 label = method_name + ' prediction interval')\n",
    "    \n",
    "    if pred is not None:\n",
    "        if pred_.ndim == 2:\n",
    "            plt.plot(x_, pred_[:,0], 'k', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted low and high quantiles')\n",
    "            plt.plot(x_, pred_[:,1], 'k', lw=2, alpha=0.9)\n",
    "        else:\n",
    "            plt.plot(x_, pred_, 'k--', lw=2, alpha=0.9,\n",
    "                     label=u'Predicted value')\n",
    "    \n",
    "    plt.xlabel('$X$')\n",
    "    plt.ylabel('$Y$')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_hist(length,\n",
    "              in_the_range,\n",
    "              x_name=\"\",\n",
    "              dec_x_quant=[0,0,0],\n",
    "              draw_quant=True,\n",
    "              filename=None,\n",
    "              save_figures=False):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.histplot(data=df, x='length', hue='in_the_range', multiple=\"stack\", stat='percent',\n",
    "        palette={True: 'green', False: 'red'}, bins=20, hue_order=[True, False])\n",
    "    sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=2, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "    if draw_quant:\n",
    "        # Calcul des quantiles\n",
    "        quantiles = [np.percentile(length, i) for i in range(25, 100, 25)]\n",
    "\n",
    "        # Calcul de la hauteur maximale pour les barres verticales\n",
    "        ymax = plt.ylim()[1] \n",
    "\n",
    "        # Tracé des lignes verticales pour les quantiles et ajout des étiquettes\n",
    "        for i, quantile in enumerate(quantiles, 1):\n",
    "            plt.vlines(x=quantile, ymin=0, ymax=ymax*9/10 , linestyle='--', linewidth=1.5, color='black')\n",
    "            plt.text(quantile+dec_x_quant[i-1], ymax*9/10, f'Q{i}:\\n{round(quantile)}', va='bottom', ha='center', color='black', fontsize=9, rotation=0)\n",
    "\n",
    "    plt.xlabel(x_name)\n",
    "    if save_figures and (filename is not None):\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout variables + Changement var d'intérêt si souhaité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "Regression = False # Choix d'aller sur regression ou rester sur binaire\n",
    "\n",
    "adm = pd.concat([chunk for chunk in pd.read_csv(os.path.join('data/mimiciii', 'ADMISSIONS.csv.gz'), chunksize=20000)], axis=0)\n",
    "pat = pd.concat([chunk for chunk in pd.read_csv(os.path.join('data/mimiciii', 'PATIENTS.csv.gz'), chunksize=20000)], axis=0)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "pat['SUBJECT_ID'] = pat['SUBJECT_ID'].astype(str)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train = df_final.iloc[:, 0:-1]\n",
    "X_train = pd.merge(X_train, adm, on='SUBJECT_ID', how='left')\n",
    "X_train = pd.merge(X_train, pat, on='SUBJECT_ID', how='left')\n",
    "X_train['ADMITTIME'] = pd.to_datetime(X_train['ADMITTIME'])\n",
    "X_train['DISCHTIME'] = pd.to_datetime(X_train['DISCHTIME'])\n",
    "X_train['DELTATIME'] = (X_train['DISCHTIME'] - X_train['ADMITTIME']).dt.total_seconds() / 3600\n",
    "X_train = X_train.drop(X_train[X_train['DELTATIME'] < 0].index)\n",
    "X_train = X_train.sort_values(by='ADMITTIME', ascending=False)\n",
    "\n",
    "# Extraction des noms de colonnes des variables numériques\n",
    "numerical_columns = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Conversion des variables catégorielles en valeurs encodées\n",
    "x_categorical = X_train.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Création du DataFrame des variables numériques avec les noms de colonnes\n",
    "x_numerical = X_train[numerical_columns]\n",
    "\n",
    "# Concaténation des variables numériques et catégorielles\n",
    "X_train = pd.concat([x_numerical, x_categorical], axis=1)\n",
    "\n",
    "# Supprimez les doublons conservés dans 'X_train' en conservant uniquement la première occurrence (la plus tardive)\n",
    "X_train = X_train.drop_duplicates(subset='SUBJECT_ID', keep='first')\n",
    "\n",
    "if Regression :\n",
    "    y_train = X_train['DELTATIME']\n",
    "else :\n",
    "    y_train = X_train[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "X_train = X_train.iloc[:, list(range(1, 5151)) ] # + [5162, 5165, 5166, 5173] \n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "X_test = df_final_test.iloc[:, 0:-1]\n",
    "X_test = pd.merge(X_test, adm, on='SUBJECT_ID', how='left')\n",
    "X_test = pd.merge(X_test, pat, on='SUBJECT_ID', how='left')\n",
    "X_test['ADMITTIME'] = pd.to_datetime(X_test['ADMITTIME'])\n",
    "X_test['DISCHTIME'] = pd.to_datetime(X_test['DISCHTIME'])\n",
    "X_test['DELTATIME'] = (X_test['DISCHTIME'] - X_test['ADMITTIME']).dt.total_seconds() / 3600\n",
    "X_test = X_test.drop(X_test[X_test['DELTATIME'] < 0].index)\n",
    "X_test = X_test.sort_values(by='ADMITTIME', ascending=False)\n",
    "\n",
    "# Extraction des noms de colonnes des variables numériques\n",
    "numerical_columns = X_test.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Conversion des variables catégorielles en valeurs encodées\n",
    "x_categorical = X_test.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Création du DataFrame des variables numériques avec les noms de colonnes\n",
    "x_numerical = X_test[numerical_columns]\n",
    "\n",
    "# Concaténation des variables numériques et catégorielles\n",
    "X_test = pd.concat([x_numerical, x_categorical], axis=1)\n",
    "\n",
    "# Supprimez les doublons conservés dans 'X_test' en conservant uniquement la première occurrence (la plus tardive)\n",
    "X_test = X_test.drop_duplicates(subset='SUBJECT_ID', keep='first')\n",
    "\n",
    "if Regression :\n",
    "    y_test = X_test['DELTATIME']\n",
    "else :\n",
    "    y_test = X_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "X_test = X_test.iloc[:, list(range(1, 5151)) ] # + [5162, 5165, 5166, 5173] <=> 'ADMISSION_TYPE', 'INSURANCE', 'LANGUAGE', 'GENDER'\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "print(\"Dataset: %s\" % (X_train.shape,))\n",
    "print(\"Dataset de test (+ calibration): %s\" % (X_test.shape,))\n",
    "\n",
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "\n",
    "# Split the data into calibration and validation sets (save the shuffling)\n",
    "idx = np.array([1] * n + [0] * (X_test.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=seed)\n",
    "# divide the data into proper training set and calibration set\n",
    "# idx = np.random.permutation(n_train)\n",
    "# n_half = int(np.floor(n_train/2))\n",
    "# idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles ML Survie des patients (binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATsI8dNs-kk-"
   },
   "source": [
    "## Reg Logistique\n",
    "\n",
    "Dans cette partie, nous utilisons la classification par régression logistique pour analyser les données qui ont été préparées à partir des ensembles d'entraînement et de test. Le code commence par configurer un modèle de régression, puis le forme en utilisant les données d'entraînement et effectue des prédictions sur les données de test. Ensuite, il affiche la précision, le rappel et le score F1 du modèle. Cette section nous permet d'évaluer à quel point le modèle de régression logistique prédit avec précision la mortalité à l'hôpital en se basant sur les représentations réduites des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T16:01:06.300776Z",
     "iopub.status.busy": "2023-08-27T16:01:06.300275Z",
     "iopub.status.idle": "2023-08-27T16:04:58.786600Z",
     "shell.execute_reply": "2023-08-27T16:04:58.782078Z",
     "shell.execute_reply.started": "2023-08-27T16:01:06.300731Z"
    },
    "id": "KDoWMKGA-kk-",
    "outputId": "729dfaea-230c-4a93-d8ff-076de3d6bac0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialisation du modèle de régression logistique\n",
    "\n",
    "# Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés et tuning du paramètres Cs\n",
    "# inverse of regularization strength -> smaller values specify stronger regularization\n",
    "model = LogisticRegressionCV(penalty='l1', solver='saga', max_iter=1000, class_weight = 'balanced', random_state = seed, cv=5)\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_lr = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle (!= precision)\n",
    "# Taux de bien classés\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle = Sensibilité\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "# Quand le recall est haut, cela veut plutôt dire qu’il ne ratera aucun positif.\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(matrix)\n",
    "\n",
    "# 51min\n",
    "# Précision: 57.67%\n",
    "# Rappel: 59.06%\n",
    "# F1-score: 29.43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Dans cette partie, nous utilisons un modèle de classification de forêt aléatoire pour estimer la probabilité de décès à l'hôpital. Nous optimisons les hyperparamètres en utilisant GridSearchCV afin de trouver la meilleure configuration pour le nombre maximal de caractéristiques (max_features). Les performances du modèle sont évaluées selon la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, class_weight='balanced',random_state = seed)\n",
    "# Recherche des meilleurs hyperparamètres\n",
    "\n",
    "# Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "# cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestClassifier(criterion='entropy', n_estimators=150, class_weight='balanced',\n",
    "    max_features=best_max_features, oob_score =True, random_state = seed)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "smx_rf = model.predict_proba(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "oob_score = model.oob_score_\n",
    "print(\"Out-of-Bag Score: %.2f\" % (oob_score))\n",
    "\n",
    "# Calculer et afficher l'accuracy du modèle (!= precision)\n",
    "# Taux de bien classés\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Calculer et afficher le rappel du modèle = Sensibilité\n",
    "# le nombre de positifs bien prédit (Vrai Positif) divisé par l’ensemble des positifs (Vrai Positif + Faux Négatif).\n",
    "# estime la probabilité de bien détecter un positif\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "\n",
    "# Calculer et afficher le F1-score du modèle\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "# 13 min\n",
    "# OOB : 0.74\n",
    "# Précision: 47.04%\n",
    "# Rappel: 71.81%\n",
    "# F1-score: 28.84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison performances des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer le taux de faux positifs (FPR), le taux de vrais positifs (TPR) et les seuils\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, smx_lr[:, 1])\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, smx_rf[:, 1])\n",
    "# Calculer l'aire sous la courbe ROC (AUC)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label='Logistic regression (AUC = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr_rf, tpr_rf, color='lightblue', lw=2, label='Random forest (AUC = %0.2f)' % roc_auc_rf)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "\n",
    "# Afficher les points correspondant aux seuils avec un pas de 0.1\n",
    "threshold_indices_lr = [i for i, threshold in enumerate(thresholds_lr) if i % 20 == 0]\n",
    "threshold_indices_rf = [i for i, threshold in enumerate(thresholds_rf) if i % 10 == 0]\n",
    "\n",
    "plt.scatter(fpr_lr[threshold_indices_lr], tpr_lr[threshold_indices_lr], marker='o', color='darkorange', label='Thresholds (LR)')\n",
    "plt.scatter(fpr_rf[threshold_indices_rf], tpr_rf[threshold_indices_rf], marker='o', color='lightblue', label='Thresholds (RF)')\n",
    "\n",
    "# Ajouter des annotations indiquant la valeur des seuils\n",
    "for i in threshold_indices_lr:\n",
    "    plt.text(fpr_lr[i], tpr_lr[i], f'{thresholds_lr[i]:.2f}', fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "for i in threshold_indices_rf:\n",
    "    plt.text(fpr_rf[i], tpr_rf[i], f'{thresholds_rf[i]:.2f}', fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('TFP = 1 - Spécificité')\n",
    "plt.ylabel('TVP = Sensibilité')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"fig/classi/roc.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des seuils de classification pour lesquels on va calculer\n",
    "# les f1 scores\n",
    "threshold_array = np.linspace(0, 1, 100)\n",
    "f1_list_lr = []\n",
    "f1_list_rf = []\n",
    "\n",
    "# Boucle pour calculer le f1 score pour chaque seuil\n",
    "for threshold in threshold_array:\n",
    "    # Prédiction des étiquettes pour un seuil donné\n",
    "    pred_threshold_lr = (smx_lr[:, 1] > threshold).astype(int)\n",
    "    pred_threshold_rf = (smx_rf[:, 1] > threshold).astype(int)\n",
    "    \n",
    "    # Calcul du f1 score pour un seuil donné\n",
    "    f1_threshold_lr = f1_score(y_true=y_test, y_pred=pred_threshold_lr)\n",
    "    f1_threshold_rf = f1_score(y_true=y_test, y_pred=pred_threshold_rf)\n",
    "    \n",
    "    # Ajout du f1 score à la liste\n",
    "    f1_list_lr.append(f1_threshold_lr)\n",
    "    f1_list_rf.append(f1_threshold_rf)\n",
    "\n",
    "# Tracer la courbe des f1 scores en fonction des seuils\n",
    "plt.plot(threshold_array, f1_list_lr, color='darkorange', lw=2,label='Logistic regression')\n",
    "plt.plot(threshold_array, f1_list_rf, color='lightblue', lw=2,label='Random forest')\n",
    "plt.xlabel('Seuil score')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"fig/classi/f1_score.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'score': smx_lr[:, 1],\n",
    "    'true_label': y_test\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.histplot(data=df, x='score', hue='true_label', multiple=\"stack\", stat='percent',\n",
    "    palette={1: 'darkorange', 0: 'lightblue'}, bins=20, hue_order=[0, 1])\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\",\n",
    "    bbox_to_anchor=(.5, 1), ncol=2, frameon=False,\n",
    ")\n",
    "ax.legend(['Mort', 'Vivant'])\n",
    "plt.xlabel(\"Score prédits logistic regression\")\n",
    "plt.savefig(\"fig/classi/histo_score_lr.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'score': smx_rf[:, 1],\n",
    "    'true_label': y_test\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.histplot(data=df, x='score', hue='true_label', multiple=\"stack\", stat='percent',\n",
    "    palette={1: 'darkorange', 0: 'lightblue'}, bins=20, hue_order=[0, 1])\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\",\n",
    "    bbox_to_anchor=(.5, 1), ncol=2, frameon=False,\n",
    ")\n",
    "ax.legend(['Mort', 'Vivant'])\n",
    "plt.xlabel(\"Score prédits random forest\")\n",
    "plt.savefig(\"fig/classi/histo_score_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction conforme (binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n = 500 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "smx = smx_rf\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# ix est un vecteur de taille 50 000 (nb d'images) qui contient n True et 50 000 - n False disposé aléatoirement\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx,:]\n",
    "cal_labels, val_labels = y_test[idx], y_test[~idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "q = 0.6\n",
    "\n",
    "# Calcul des scores de conformité Vi pour les données d'étalonnage\n",
    "cal_scores = cal_labels - cal_smx[:, 1]\n",
    "cal_scores_bis = 100*cal_labels - cal_smx[:, 1]\n",
    "# Calcul des scores de conformité Vn+j pour les données de test\n",
    "test_scores = c - val_smx[:, 1]\n",
    "test_scores_bis = 100 - val_smx[:, 1]\n",
    "\n",
    "def BH(calib_scores, test_scores, q = 0.1):\n",
    "    ntest = len(test_scores)\n",
    "    ncalib = len(calib_scores)\n",
    "    pvals = np.zeros(ntest)\n",
    "    \n",
    "    for j in range(ntest):\n",
    "        pvals[j] = (np.sum(calib_scores < test_scores[j]) + 1) / (ncalib+1)\n",
    "         \n",
    "    # BH(q) \n",
    "    df_test = pd.DataFrame({\"id\": range(ntest), \"pval\": pvals}).sort_values(by='pval')\n",
    "    \n",
    "    df_test['threshold'] = q * np.linspace(1, ntest, num=ntest) / ntest \n",
    "    idx_smaller = [j for j in range(ntest) if df_test.iloc[j,1] <= df_test.iloc[j,2]]\n",
    "    \n",
    "    if len(idx_smaller) == 0:\n",
    "        return(np.array([]))\n",
    "    else:\n",
    "        idx_sel = np.array(df_test.index[range(np.max(idx_smaller)+1)])\n",
    "        return(idx_sel)\n",
    "\n",
    "# BH using residuals\n",
    "BH_res= BH(cal_scores, test_scores, q)\n",
    "# The FDR (FDP) is a natural measure of type-I error for binary classification\n",
    "# Power = Rappel\n",
    "if len(BH_res) == 0:\n",
    "    BH_res_fdp = 0\n",
    "    BH_res_power = 0\n",
    "else:\n",
    "    BH_res_fdp = np.sum(val_labels.reset_index(drop=True)[BH_res] <= c) / len(BH_res)\n",
    "    BH_res_power = np.sum(val_labels.reset_index(drop=True)[BH_res] > c) / sum(val_labels > c)\n",
    "        \n",
    "print(BH_res_fdp,BH_res_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des p-valeurs conformes\n",
    "p_values = np.zeros(len(test_scores))\n",
    "p_values_bis = np.zeros(len(test_scores))\n",
    "# Calcul des p-valeurs conformes pour chaque score de test\n",
    "for j in range(len(test_scores)):\n",
    "    count_V_less_than_Vnj = np.sum(cal_scores < test_scores[j])\n",
    "    count_V_equal_to_Vnj = np.sum(cal_scores == test_scores[j])\n",
    "    U_j = np.random.uniform()\n",
    "    p_values[j] = (count_V_less_than_Vnj + (1 + count_V_equal_to_Vnj) * U_j) / (len(cal_scores) + 1)\n",
    "    p_values_bis[j] = (count_V_less_than_Vnj + 1) / (len(cal_scores) + 1)\n",
    "\n",
    "# Affichage des p-valeurs conformes\n",
    "print(\"P-values conformes :\", p_values, p_values_bis)\n",
    "\n",
    "# Initialiser k* à 0\n",
    "k_star = 0\n",
    "\n",
    "# Parcourir les p-valeurs conformes dans l'ordre décroissant\n",
    "for k in range(len(test_scores)):\n",
    "    # Calculer la proportion de p-valeurs conformes inférieures ou égales à qk/m\n",
    "    proportion = np.sum(p_values <=  q*k/ m)\n",
    "    \n",
    "    # Vérifier si la proportion est supérieure ou égale à k\n",
    "    if proportion >= k:\n",
    "        # Mettre à jour k* avec la valeur actuelle de k\n",
    "        k_star = k + 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores\n",
    "cal_scores = cal_smx[:, 1]\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx[:, 1] > qhat\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets == val_labels\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Trouver les cas où la prédiction est négative mais la vraie classe est positive\n",
    "erreur_type_2 = (prediction_sets == False) & (val_labels == 1)\n",
    "\n",
    "# Calculer le pourcentage d'erreurs de type 2 par rapport au total des prédictions positives\n",
    "erreur_type_2_rate = erreur_type_2.sum() / val_labels.sum()\n",
    "\n",
    "print(\"Taux d'erreur de type 2 :\", erreur_type_2_rate)\n",
    "\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'], 1, 0)\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage.reset_index(drop=True)\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.xlabel(\"Valeur prédite random forest\")\n",
    "plt.savefig(\"fig/classi/outlier_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# LR\n",
    "# The empirical coverage is: 0.7991967871485943\n",
    "# Taux d'erreur de type 2 : 0.8260869565217391\n",
    "\n",
    "# RF \n",
    "# The empirical coverage is: 0.8052208835341366\n",
    "# Taux d'erreur de type 2 : 0.825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "# Pour chacunes des images du set de calibration, score de conformité = 1-softmax associé au vrai label (liste de n éléments)\n",
    "# score de conformité élevé quand softmax faible = prédiction du model mauvaise\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "qhat = np.quantile(cal_scores, q_level, method='higher') # valeur du 9 ème décile environ\n",
    "\n",
    "# 3: form prediction sets\n",
    "prediction_sets = val_smx >= (1-qhat)\n",
    "# Pour chaque image du set de validation, on garde les labels dont le softmax dépasse le seuil (on remplace les valeurs par True/False) \n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_small_sets_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split conformal + meilleur RAPS (Random Adaptative Prediction Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RAPS regularization parameters (larger lam_reg and smaller k_reg leads to smaller sets)\n",
    "lam_reg = 0.01\n",
    "k_reg = 1\n",
    "disallow_zero_sets = False # Set this to False in order to see the coverage upper bound hold\n",
    "rand = True # Set this to True in order to see the coverage upper bound hold\n",
    "reg_vec = np.array(k_reg*[0,] + (smx.shape[1]-k_reg)*[lam_reg,])[None,:]\n",
    "\n",
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; \n",
    "print(cal_pi.shape)\n",
    "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1)\n",
    "print(cal_srt.shape)\n",
    "cal_srt_reg = cal_srt + reg_vec\n",
    "cal_L = np.where(cal_pi == cal_labels.values[:, None])[1]\n",
    "cal_scores = cal_srt_reg.cumsum(axis=1)[np.arange(n),cal_L] - np.random.rand(n)*cal_srt_reg[np.arange(n),cal_L]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "# Deploy\n",
    "n_val = val_smx.shape[0]\n",
    "val_pi = val_smx.argsort(1)[:,::-1]\n",
    "val_srt = np.take_along_axis(val_smx,val_pi,axis=1)\n",
    "val_srt_reg = val_srt + reg_vec\n",
    "val_srt_reg_cumsum = val_srt_reg.cumsum(axis=1)\n",
    "indicators = (val_srt_reg.cumsum(axis=1) - np.random.rand(n_val,1)*val_srt_reg) <= qhat if rand else val_srt_reg.cumsum(axis=1) - val_srt_reg <= qhat\n",
    "if disallow_zero_sets: indicators[:,0] = True\n",
    "prediction_sets = np.take_along_axis(indicators,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(n_val),val_labels]\n",
    "# On regarde en moyenne si le set de prédiction contient bien le vrai label\n",
    "print(f\"The empirical coverage is: {empirical_coverage.mean()}\")\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(prediction_sets, columns=['col1', 'col2'])\n",
    "\n",
    "# Création de la nouvelle colonne\n",
    "df['set'] = np.where(df['col1'] & df['col2'], '[0,1]',\n",
    "                         np.where(df['col1'], '[0]', \n",
    "                                  np.where(df['col2'], '[1]', '[]')))\n",
    "\n",
    "df['empirical_coverage'] = empirical_coverage\n",
    "\n",
    "df['val_labels'] = val_labels.reset_index(drop=True)\n",
    "df['val_labels'] = df['val_labels'].replace({0: '[0]', 1: '[1]'})\n",
    "\n",
    "# Spécification de l'ordre des catégories\n",
    "order = ['[0]', '[1]', '[0,1]']\n",
    "\n",
    "# Conversion de la colonne 'set' en catégorie avec l'ordre spécifié\n",
    "df['set'] = pd.Categorical(df['set'], categories=order, ordered=True)\n",
    "\n",
    "# Création du countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=\"set\", hue=\"empirical_coverage\", palette={True: 'green', False: 'red'},\n",
    "              hue_order=[True, False], ax=ax, stat='percent')\n",
    "\n",
    "# Countplot des val_labels sur le deuxième axe y\n",
    "sns.countplot(data=df, x=\"val_labels\", alpha=0.2, stat='percent', label='True proportion', color='gray')\n",
    "sns.move_legend(\n",
    "        ax, \"lower center\",\n",
    "        bbox_to_anchor=(.5, 1), ncol=3, title=\"Vrai valeur dans l'intervalle de prédiction:\", frameon=False,\n",
    "    )\n",
    "\n",
    "# Réglages des légendes et sauvegarde du graphique\n",
    "plt.savefig(\"fig/classi/split_raps_rf.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "for i in range(10):\n",
    "    prediction_set = smx[i] > 1-qhat\n",
    "    label_strings = [\"0\",\"1\"]\n",
    "    filtered_labels = [label for keep, label in zip(list(prediction_set), label_strings) if keep]\n",
    "    print(f\"The prediction set is: {filtered_labels}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full conformal + small sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements de convergence\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def full_conformal(k):\n",
    "    X = pd.concat([X_train, X_test.iloc[k:k+1]])\n",
    "    y = y_train.copy() \n",
    "    n = len(y_train)\n",
    "    prediction = []\n",
    "    for i in range(2):\n",
    "        y.loc[n] = i\n",
    "        model.fit(X, y)\n",
    "        smx = model.predict_proba(X)\n",
    "        scores = 1-smx[np.arange(n+1), y]\n",
    "        qhat = np.quantile(scores[:-1], q_level, method='higher') # valeur du 9 ème décile environ\n",
    "        if scores[-1] <= qhat:\n",
    "            prediction.append(str(i))\n",
    "    return prediction\n",
    "\n",
    "for i in range(5):\n",
    "    prediction_set = full_conformal(i)\n",
    "    print(f\"The prediction set is: {prediction_set}\")\n",
    "    print(f\"The real value is: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles ML Durée de séjour des patients (régression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Notice that the random forests regressor **estimates the conditional mean** of $Y_i$ given $X_i=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "import math\n",
    "\n",
    "# Définir les valeurs des hyperparamètres à tester\n",
    "\n",
    "# Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "param_grid = {\n",
    "    'max_features': list(range(30, 121, 10))\n",
    "}\n",
    "\n",
    "# Initialisation du modèle Random Forest\n",
    "\n",
    "# Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, random_state = seed)\n",
    "\n",
    "# Définir la métrique MSE comme score\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=mse_scorer)\n",
    "\n",
    "# Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleures valeurs des hyperparamètres\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "# Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "\n",
    "# Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "model = RandomForestRegressor(criterion='squared_error', n_estimators=150, max_features=best_max_features, \n",
    "    random_state = seed, oob_score=True)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "oob_score = model.oob_score_\n",
    "print(\"Out-of-Bag Score: %.2f\" % (oob_score))\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %.2f\" % (mse))\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared: %.2f\" % (r2))\n",
    "\n",
    "# OOB : 0.22\n",
    "# MSE : 70702.01\n",
    "# R^2 : 0.26\n",
    "# 20 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import AbsErrorErrFunc\n",
    "\n",
    "# define a conformal prediction object \n",
    "nc = RegressorNc(model, AbsErrorErrFunc())\n",
    "\n",
    "# build a regualr split conformal prediction object \n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# fit the conditional mean regression to the proper training data\n",
    "icp.fit(X_train, y_train)\n",
    "\n",
    "# compute the absolute residual error on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower_split = np.maximum(predictions[:, 0], 0)\n",
    "y_upper_split = predictions[:,1]\n",
    "\n",
    "# compute the conditional mean estimation\n",
    "pred_rf = model.predict(X_test[~idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display the average coverage\n",
    "in_the_range_split = (y_test[~idx] >= y_lower_split) & (y_test[~idx]<= y_upper_split)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_split) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_split = y_upper_split - y_lower_split\n",
    "print(\"Average length:\", round(np.mean(length_split)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_split)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_split,y_l=y_lower_split,pred=pred_rf,shade_color='tomato',\n",
    "          method_name=\"Split:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_split.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_split, in_the_range = in_the_range_split, x_name=\"Length Split\", dec_x_quant= [0,0,0],\n",
    "      filename=\"fig/regres/histo_length_split.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 92.76%\n",
    "# Average length: 593\n",
    "# Standard deviation of length: 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_split', 'wb') as f1:\n",
    "    pickle.dump(y_upper_split, f1)\n",
    "with open('data/traite/y_lower_split', 'wb') as f1:\n",
    "    pickle.dump(y_lower_split, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV+ for K-fold cross-validation (Jacknife + if K=len(X_train_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "X_train_K = np.concatenate((X_train, X_test[idx]), axis=0)\n",
    "y_train_K = np.concatenate((y_train, y_test[idx]), axis=0)\n",
    "\n",
    "# Initialisez l'objet KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Initialisez une liste pour stocker les index des échantillons d'entraînement\n",
    "residus = np.zeros(len(X_train_K))\n",
    "pred = np.zeros((len(X_train_K),len(X_test[~idx])))\n",
    "\n",
    "# Parcourez les splits et stockez les index des échantillons d'entraînement\n",
    "for train_indices, test_indices in kf.split(X_train_K):\n",
    "    model.fit(X_train_K[train_indices], y_train_K[train_indices])\n",
    "    residus[test_indices] = abs(y_train_K[test_indices]-model.predict(X_train_K[test_indices]))\n",
    "    pred[test_indices] = np.repeat(model.predict(X_test[~idx]).reshape(1, -1), len(test_indices), axis=0)\n",
    "\n",
    "# display the results\n",
    "\n",
    "y_upper_cvp = np.quantile(pred+residus.reshape(-1, 1), q_level, method='higher', axis=0)\n",
    "y_lower_cvp = np.maximum(- np.quantile(-pred+residus.reshape(-1, 1), q_level, method='higher', axis=0), 0)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "model.fit(X_train, y_train)\n",
    "pred_rf = model.predict(X_test[~idx])\n",
    "\n",
    "# 4 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display the average coverage\n",
    "in_the_range_cvp = (y_test[~idx] >= y_lower_cvp) & (y_test[~idx]<= y_upper_cvp)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_cvp) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length_cvp = y_upper_cvp - y_lower_cvp\n",
    "print(\"Average length:\", round(np.mean(length_cvp)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_cvp)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_cvp,y_l=y_lower_cvp,pred=pred_rf,shade_color='gray',\n",
    "          method_name=\"CV+ K folder:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_cvp.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cvp, in_the_range = in_the_range_cvp, x_name=\"Length CV+ K folder\", dec_x_quant= [-20,0,0],\n",
    "        filename=\"fig/regres/histo_length_cvp.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 92.96%\n",
    "# Average length: 601\n",
    "# Standard deviation of length: 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_rf', 'wb') as f1:\n",
    "    pickle.dump(pred_rf, f1)\n",
    "with open('data/traite/y_upper_cvp', 'wb') as f1:\n",
    "    pickle.dump(y_upper_cvp, f1)\n",
    "with open('data/traite/y_lower_cvp', 'wb') as f1:\n",
    "    pickle.dump(y_lower_cvp, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Définir les constantes\n",
    "n_splits_range = range(2, 16)\n",
    "length_cvp_means = []\n",
    "length_cvp_med = []\n",
    "\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "\n",
    "X_train_K = np.concatenate((X_train, X_test[idx]), axis=0)\n",
    "y_train_K = np.concatenate((y_train, y_test[idx]), axis=0)\n",
    "\n",
    "# Initialisez une liste pour stocker les index des échantillons d'entraînement\n",
    "residus = np.zeros(len(X_train_K))\n",
    "pred = np.zeros((len(X_train_K),len(X_test[~idx])))\n",
    "\n",
    "# Boucle sur le nombre de splits\n",
    "for n_splits in n_splits_range:\n",
    "    # Initialisez l'objet KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Parcourez les splits et exécutez le processus de validation croisée\n",
    "    for train_indices, test_indices in kf.split(X_train_K):\n",
    "        model.fit(X_train_K[train_indices], y_train_K[train_indices])\n",
    "        residus[test_indices] = abs(y_train_K[test_indices] - model.predict(X_train_K[test_indices]))\n",
    "        pred[test_indices] = np.repeat(model.predict(X_test[~idx]).reshape(1, -1), len(test_indices), axis=0)\n",
    "\n",
    "    # Calcul de length_cvp pour chaque test point\n",
    "    y_upper = np.quantile(pred + residus.reshape(-1, 1), q_level, method='higher', axis=0)\n",
    "    y_lower = np.maximum(- np.quantile(-pred + residus.reshape(-1, 1), q_level, method='higher', axis=0), 0)\n",
    "    length_cvp = y_upper - y_lower\n",
    "    length_cvp_means.append(np.mean(length_cvp))\n",
    "    length_cvp_med.append(np.quantile(length_cvp, 0.5))\n",
    "    print(n_splits)\n",
    "    print(\"Random Forests: Average length:\", np.mean(length_cvp), np.quantile(length_cvp, 0.5))\n",
    "\n",
    "# Tracer l'évolution de length_cvp en fonction du nombre de splits\n",
    "plt.plot(n_splits_range, length_cvp_means, color='lightblue', label='Moyenne')\n",
    "plt.plot(n_splits_range, length_cvp_med, color ='darkorange', label='Médiane')\n",
    "plt.xlabel('Nombre de splits')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Évolution de length en fonction du nombre de splits')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"fig/regres/lineplot_k_folder.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 147 min pour K jusqu'à 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQR Random Forests\n",
    "\n",
    "Given any quantile regression algorithm $\\mathcal{A}$ (the function `QuantileForestRegressorAdapter` in the code below), we then fit two conditional quantile functions $\\hat{q}_{\\alpha_{lo}}$ and $\\hat{q}_{\\alpha_{hi}}$ on the proper training set: $$ \\{ \\hat{q}_{\\alpha_{lo}}, \\hat{q}_{\\alpha_{hi}} \\} \\leftarrow \\mathcal{A}(\\left\\lbrace (X_i, Y_i): i \\in I_1 \\right\\rbrace). $$\n",
    "This is done by calling the function `icp.fit`.\n",
    "\n",
    "In the essential next step, the function `icp.calibrate` computes conformity scores (using `QuantileRegErrFunc`) that quantify the error made by the plug-in prediction interval $ \\hat{C}(x) = [\\hat{q}_{\\alpha_{lo}}(x), \\ \\hat{q}_{\\alpha_{hi}}(x)]  $. The scores are evaluated on the calibration set as\n",
    "$$\n",
    "\tE_i := \\max\\{\\hat{q}_{\\alpha_{lo}}(X_i) - Y_i, Y_i - \\hat{q}_{\\alpha_{hi}}(X_i)\\},\n",
    "$$\n",
    "for each $i \\in I_2$. The conformity score accounts for both undercoverage and overcoverage.\n",
    "\n",
    "Finally, given new input data $X_{n+1}$, we construct the prediction interval for $Y_{n+1}$ as\n",
    "$$\n",
    "C(X_{n+1}) = \\left[ \\hat{q}_{\\alpha_{lo}}(X_{n+1}) - Q_{1-\\alpha}(E, I_2) , \\ \\hat{q}_{\\alpha_{hi}}(X_{n+1}) + Q_{1-\\alpha}(E, I_2) \\right],\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Q_{1-\\alpha}(E, I_2) :=  (1-\\alpha)(1+1/|I_2|)\\text{-th empirical quantile of} \\left\\{E_i : i \\in I_2\\right\\}\n",
    "$$\n",
    "conformalizes the plug-in prediction interval. This is done by calling the function `icp.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install setuptools numpy scipy scikit-learn cython\n",
    "# pip3 install scikit-garden python version 3.7 (code à modifier)\n",
    "# OU pip3 install quantile_forest\n",
    "\n",
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import QuantileRegErrFunc, QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 150\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = X_train.shape[1]\n",
    "\n",
    "# target quantile levels\n",
    "# desired quanitile levels\n",
    "quantiles = [5, 95]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 1 #0.80\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = seed\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 10\n",
    "\n",
    "# define quantile random forests (QRF) parameters\n",
    "params_qforest = dict()\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"max_features\"] = max_features\n",
    "params_qforest[\"CV\"] = cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"] = cv_test_ratio\n",
    "params_qforest[\"random_state\"] = cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals\n",
    "\n",
    "# define the QRF model\n",
    "model = helper.QuantileForestRegressorAdapter(model=None,fit_params=None,\n",
    "                                                quantiles=quantiles,\n",
    "                                                params=params_qforest)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 61 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install setuptools numpy scipy scikit-learn cython\n",
    "# pip3 install scikit-garden python version 3.7 (code à modifier)\n",
    "# OU pip3 install quantile_forest\n",
    "\n",
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.nc import QuantileRegErrFunc, QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# define the CQR object, computing the absolute residual error of points \n",
    "# located outside the estimated QRF band \n",
    "nc = RegressorNc(model, QuantileRegErrFunc()) # ou QuantileRegAsymmetricErrFunc / QuantileRegErrFunc\n",
    "\n",
    "# build the split CQR object\n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# compute the absolute errors on calibration data\n",
    "icp.calibrate(X_test[idx], y_test[idx])\n",
    "\n",
    "# produce predictions for the test set, with confidence equal to significance\n",
    "predictions = icp.predict(X_test[~idx].values, significance=alpha)\n",
    "y_lower_cqr = np.maximum(predictions[:, 0], 0)\n",
    "y_upper_cqr = predictions[:,1]\n",
    "\n",
    "# compute the low and high conditional quantile estimation\n",
    "pred_qr = model.predict(X_test[~idx].values)\n",
    "y_lower_qr = np.maximum(pred_qr[:, 0], 0)\n",
    "y_upper_qr = pred_qr[:,1]\n",
    "\n",
    "# 62 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display the average coverage\n",
    "in_the_range_qr = (y_test[~idx] >= y_lower_qr) & (y_test[~idx]<= y_upper_qr)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_qr) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the conformal interval per each test point\n",
    "length_qr = y_upper_qr - y_lower_qr\n",
    "print(\"Average length:\", round(np.mean(length_qr)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_qr)))\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range_cqr = (y_test[~idx] >= y_lower_cqr) & (y_test[~idx]<= y_upper_cqr)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range_cqr) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the conformal interval per each test point\n",
    "length_cqr = y_upper_cqr - y_lower_cqr\n",
    "print(\"Average length:\", round(np.mean(length_cqr)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length_cqr)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper_cqr,y_l=y_lower_cqr,pred=pred_qr,shade_color='lightblue',\n",
    "          method_name=\"CQR:\",title=\"CQR Random Forests (quantile regression)\",\n",
    "          filename=\"fig/regres/lineplot_cqr.png\",save_figures=True)\n",
    "\n",
    "plot_hist(length = length_cqr, in_the_range = in_the_range_cqr, x_name=\"Length CQR\", dec_x_quant= [-200,90,70],\n",
    "        filename=\"fig/regres/histo_length_cqr.png\", save_figures=True)\n",
    "\n",
    "plot_hist(length = length_qr, in_the_range = in_the_range_qr, x_name=\"Length QR\", dec_x_quant= [-200,90,70],\n",
    "        filename=\"fig/regres/histo_length_cqr.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 95.77%\n",
    "# Average length: 665\n",
    "# Standard deviation of length: 579\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 93.76%\n",
    "# Average length: 648\n",
    "# Standard deviation of length: 579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/traite/pred_qr', 'wb') as f1:\n",
    "    pickle.dump(pred_qr, f1)\n",
    "with open('data/traite/y_upper_cqr', 'wb') as f1:\n",
    "    pickle.dump(y_upper_cqr, f1)\n",
    "with open('data/traite/y_lower_cqr', 'wb') as f1:\n",
    "    pickle.dump(y_lower_cqr, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best (approximativement symmétrique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower = y_lower_split.copy()\n",
    "y_upper = y_upper_split.copy()\n",
    "\n",
    "for i in range(len(y_test[~idx])):\n",
    "    if pred_rf[i] < y_test[~idx].iloc[i]:\n",
    "        y_lower[i] = max(pred_rf[i] - y_test[~idx].iloc[i]/2,0)\n",
    "        y_upper[i] = y_test[~idx].iloc[i]\n",
    "    else:\n",
    "        y_lower[i] = max(y_test[~idx].iloc[i],0)\n",
    "        y_upper[i] = pred_rf[i] + y_test[~idx].iloc[i]/2\n",
    "\n",
    "# compute and display the average coverage\n",
    "in_the_range = (y_test[~idx] >= y_lower) & (y_test[~idx]<= y_upper)\n",
    "print(\"Percentage in the range (expecting %.2f%%): %.2f%%\" % (100*(1-alpha), np.sum(in_the_range) / len(y_test[~idx]) * 100))\n",
    "\n",
    "# compute length of the interval per each test point\n",
    "length = y_upper - y_lower\n",
    "print(\"Average length:\", round(np.mean(length)))\n",
    "print(\"Median length:\", round(np.median(length)))\n",
    "print(\"Standard deviation of length:\", round(np.std(length)))\n",
    "\n",
    "plot_func(y=y_test[~idx],y_u=y_upper,y_l=y_lower,pred=pred_rf,shade_color='green',\n",
    "          method_name=\"Best:\",title=\"Random Forests (mean regression)\",\n",
    "          filename=\"fig/regres/lineplot_best.png\",save_figures=True)\n",
    "          \n",
    "plot_hist(length = length, in_the_range = in_the_range, x_name=\"Ideal length\", draw_quant=False,\n",
    "        filename=\"fig/regres/histo_length_best.png\", save_figures=True)\n",
    "\n",
    "# Percentage in the range (expecting 90.0%): 100.0\n",
    "# Average length: 255\n",
    "# Median length: 169\n",
    "# Standard deviation of length: 270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des méthodes (régression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via lineplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, len(length[:100]) + 1), length[:100], color='green', lw=1.5, alpha=0.4, label='Best')\n",
    "plt.plot(np.arange(1, len(length_split[:100]) + 1), length_split[:100], color='tomato', lw=1.5, alpha=0.8, label='Split')\n",
    "plt.plot(np.arange(1, len(length_cvp[:100]) + 1), length_cvp[:100], color='gray', lw=1.5, alpha=0.8, label='CV+ K folder')\n",
    "plt.plot(np.arange(1, len(length_cqr[:100]) + 1), length_cqr[:100], color='lightblue', lw=1.5, label='CQR')\n",
    "\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper center', ncol=4, frameon=False, bbox_to_anchor=(0.5, 1.1))\n",
    "plt.savefig(\"fig/regres/lineplot_length.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison de la taille des sets via violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.DataFrame({\n",
    "        'length': length_split,\n",
    "        'in_the_range': in_the_range_split\n",
    "})\n",
    "\n",
    "df_cvp = pd.DataFrame({\n",
    "        'length': length_cvp,\n",
    "        'in_the_range': in_the_range_cvp\n",
    "})\n",
    "\n",
    "df_cqr = pd.DataFrame({\n",
    "        'length': length_cqr,\n",
    "        'in_the_range': in_the_range_cqr\n",
    "})\n",
    "\n",
    "df_best = pd.DataFrame({\n",
    "        'length': length,\n",
    "        'in_the_range': in_the_range\n",
    "})\n",
    "\n",
    "\n",
    "# Ajout de la colonne \"méthode\"\n",
    "df_split['méthode'] = 'Split'\n",
    "df_cvp['méthode'] = 'CV+ K folder'\n",
    "df_cqr['méthode'] = 'CQR'\n",
    "df_best['méthode'] = 'Best'\n",
    "\n",
    "# Concaténation des dataframes\n",
    "df_concat = pd.concat([df_split, df_cvp, df_cqr, df_best], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(data=df_concat, x=\"méthode\", y='length', hue='in_the_range', alpha=0.8, palette={True: 'green', False: 'red'},\n",
    "    split=True, gap=0.05, inner=\"quart\", hue_order=[True, False])\n",
    "plt.ylim(0,df_concat['length'].max()/2)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Length')\n",
    "plt.legend(loc='upper left', title=\"Vrai valeur dans l'intervalle de prédiction:\")\n",
    "plt.savefig(\"fig/regres/violin_length_zoom.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
