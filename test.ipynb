{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 's3fs'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# define whether to use demo dataset or the original\u001B[39;00m\n\u001B[0;32m      8\u001B[0m USE_DEMO \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01ms3fs\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m     12\u001B[0m fs \u001B[38;5;241m=\u001B[39m s3fs\u001B[38;5;241m.\u001B[39mS3FileSystem(client_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mendpoint_url\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminio.lab.sspcloud.fr\u001B[39m\u001B[38;5;124m'\u001B[39m},key \u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTGUSYC6I0MWF4DAXHBY2\u001B[39m\u001B[38;5;124m'\u001B[39m, secret \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m9z8EK2630IV1RkvN3dF7KnMdWMpm4rvg28Iv8yq7\u001B[39m\u001B[38;5;124m'\u001B[39m, token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJUR1VTWUM2STBNV0Y0REFYSEJZMiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzEwMzYxMjkyLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImxvdWlzLnRob21hc0BlbGV2ZS5lbnNhaS5mciIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJleHAiOjE3MTEwMTc1NzMsImZhbWlseV9uYW1lIjoiVGhvbWFzIiwiZ2l2ZW5fbmFtZSI6IkxvdWlzIiwiZ3JvdXBzIjpbIlVTRVJfT05ZWElBIl0sImlhdCI6MTcxMDQxMjc3MywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6IjQ0OTg5NGQzLTdhMjctNDQ2ZC1hNGRmLWYzNmI0ZTRmYjQ4MSIsIm5hbWUiOiJMb3VpcyBUaG9tYXMiLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoibGtqbXQiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzaWQiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzdWIiOiJlNzg1YzMxMi1jYTgwLTQ2NTctYTYwOS0wYTk3N2U5YTU4MzIiLCJ0eXAiOiJCZWFyZXIifQ.g-85BQTit_nPKtyiCQIs6h0vu4sX0Zv8-y_zlMwE9PF2b3VI1TFInXpvurlRJxLXX9P4mVtBqE9080Kggdp3Zg\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 's3fs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.version\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# define whether to use demo dataset or the original\n",
    "USE_DEMO = False\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='TGUSYC6I0MWF4DAXHBY2', secret = '9z8EK2630IV1RkvN3dF7KnMdWMpm4rvg28Iv8yq7', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJUR1VTWUM2STBNV0Y0REFYSEJZMiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzEwMzYxMjkyLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImxvdWlzLnRob21hc0BlbGV2ZS5lbnNhaS5mciIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJleHAiOjE3MTEwMTc1NzMsImZhbWlseV9uYW1lIjoiVGhvbWFzIiwiZ2l2ZW5fbmFtZSI6IkxvdWlzIiwiZ3JvdXBzIjpbIlVTRVJfT05ZWElBIl0sImlhdCI6MTcxMDQxMjc3MywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6IjQ0OTg5NGQzLTdhMjctNDQ2ZC1hNGRmLWYzNmI0ZTRmYjQ4MSIsIm5hbWUiOiJMb3VpcyBUaG9tYXMiLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoibGtqbXQiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzaWQiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzdWIiOiJlNzg1YzMxMi1jYTgwLTQ2NTctYTYwOS0wYTk3N2U5YTU4MzIiLCJ0eXAiOiJCZWFyZXIifQ.g-85BQTit_nPKtyiCQIs6h0vu4sX0Zv8-y_zlMwE9PF2b3VI1TFInXpvurlRJxLXX9P4mVtBqE9080Kggdp3Zg')\n",
    "\n",
    "BUCKET = \"lkjmt\"\n",
    "FILE_KEY_S3 = \"prjet stat /ADMISSIONS.csv.gz\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "\n",
    "USE_DEMO = False  # ou False, selon le contexte\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},key ='TGUSYC6I0MWF4DAXHBY2', secret = '9z8EK2630IV1RkvN3dF7KnMdWMpm4rvg28Iv8yq7', token = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJUR1VTWUM2STBNV0Y0REFYSEJZMiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzEwMzYxMjkyLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6ImxvdWlzLnRob21hc0BlbGV2ZS5lbnNhaS5mciIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJleHAiOjE3MTEwMTc1NzMsImZhbWlseV9uYW1lIjoiVGhvbWFzIiwiZ2l2ZW5fbmFtZSI6IkxvdWlzIiwiZ3JvdXBzIjpbIlVTRVJfT05ZWElBIl0sImlhdCI6MTcxMDQxMjc3MywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6IjQ0OTg5NGQzLTdhMjctNDQ2ZC1hNGRmLWYzNmI0ZTRmYjQ4MSIsIm5hbWUiOiJMb3VpcyBUaG9tYXMiLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoibGtqbXQiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNlc3Npb25fc3RhdGUiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzaWQiOiI5MThlYTA2Yy04YjkzLTRhNGUtOGZhYS1kYTllYzQxYjMxYzIiLCJzdWIiOiJlNzg1YzMxMi1jYTgwLTQ2NTctYTYwOS0wYTk3N2U5YTU4MzIiLCJ0eXAiOiJCZWFyZXIifQ.g-85BQTit_nPKtyiCQIs6h0vu4sX0Zv8-y_zlMwE9PF2b3VI1TFInXpvurlRJxLXX9P4mVtBqE9080Kggdp3Zg')\n",
    "\n",
    "BUCKET = \"lkjmt\"\n",
    "FILE_KEY_S3 = \"prjet stat /NOTEEVENTS.csv.gz\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "with fs.open(FILE_PATH_S3, mode=\"rb\") as file:\n",
    "    data = pd.concat([chunk for chunk in pd.read_csv(file, compression='gzip', chunksize=20000)], axis=0)\n",
    "    print(len(data))\n",
    "# Lire les données à partir d'un fichier CSV dans un DataFrame Pandas\n",
    "#data = pd.read_csv(\"/kaggle/input/clean-data/clean_data.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "data['SUBJECT_ID'] = data['SUBJECT_ID'].astype(str)\n",
    "data['HADM_ID'] = data['HADM_ID'].astype(str)\n",
    "\n",
    "# Le nettoyage de données. Remplacer la chaîne \"nan\" par des valeurs NaN réelles dans la colonne 'HADM_ID'\n",
    "data['HADM_ID'] = data['HADM_ID'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Convertir la colonne 'CHARTTIME' qui contient les timestamps en un format datetime avec le format spécifié\n",
    "data['CHARTTIME'] = pd.to_datetime(data['CHARTTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Supprimer les lignes ayant des valeurs manquantes dans la colonne 'HADM_ID'\n",
    "data = data.dropna(subset=[\"HADM_ID\"])\n",
    "\n",
    "# Nettoyer le dataframe de champs nulles par supprimant les deux derniers caractères de la colonne 'HADM_ID'\n",
    "data[\"HADM_ID\"] = data[\"HADM_ID\"].str[:-2]\n",
    "\n",
    "# Convertir la colonne 'CHARTDATE' qui contient les timestamps en un format datetime\n",
    "data['CHARTDATE'] = pd.to_datetime(data['CHARTDATE'])\n",
    "\n",
    "# Convertir la colonne 'TIME' en entiers\n",
    "# data['TIME'] = data['TIME'].astype(int)\n",
    "data['TIME'] = 0\n",
    "\n",
    "BUCKET = \"lkjmt\"\n",
    "FILE_KEY_S3 = \"prjet stat /ADMISSIONS.csv.gz\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "with fs.open(FILE_PATH_S3, mode=\"rb\") as file:\n",
    "    adm = pd.concat([chunk for chunk in pd.read_csv(file, compression='gzip', chunksize=20000)], axis=0)\n",
    "    print(len(adm))\n",
    "# Lire les données d'admission à partir d'un autre fichier CSV dans un DataFrame Pandas\n",
    "# adm = pd.read_csv(\"/kaggle/input/clean-data/clean_adm.csv\", low_memory=False)\n",
    "\n",
    "# Convertir les colonnes 'SUBJECT_ID' et 'HADM_ID' en chaînes de caractères\n",
    "adm['SUBJECT_ID'] = adm['SUBJECT_ID'].astype(str)\n",
    "adm['HADM_ID'] = adm['HADM_ID'].astype(str)\n",
    "\n",
    "# Convertir la colonne 'HOSPITAL_EXPIRE_FLAG' en entiers\n",
    "adm['HOSPITAL_EXPIRE_FLAG'] = adm['HOSPITAL_EXPIRE_FLAG'].astype(int)\n",
    "\n",
    "# Convertir les colonnes 'ADMITTIME' et 'DISCHTIME' en un format datetime avec le format spécifié\n",
    "adm['ADMTTIME'] = pd.to_datetime(adm['ADMITTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "adm['DISCHTIME'] = pd.to_datetime(adm['DISCHTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Filtrer les données d'admission pour inclure uniquement les lignes avec des valeurs 'SUBJECT_ID' présentes dans le DataFrame 'data'\n",
    "adm = adm[adm[\"SUBJECT_ID\"].isin(data[\"SUBJECT_ID\"].unique())]\n",
    "len(adm)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "# Créer un dataframe vide avec deux colonnes pour le nouveau dataframe\n",
    "new_data_test = pd.DataFrame(columns=[\"SUBJECT_ID\", \"HOSPITAL_EXPIRE_FLAG\"])\n",
    "\n",
    "# Grouper les données d'admission par \"SUBJECT_ID\"\n",
    "grouped_adm = adm.groupby(\"SUBJECT_ID\")\n",
    "print(len(grouped_adm))\n",
    "\n",
    "# Parcourir chaque groupe de données associées à un \"SUBJECT_ID\"\n",
    "for subject_id, group in grouped_adm:\n",
    "    # Vérifier si le groupe contient au moins un enregistrement avec la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "    if group[\"HOSPITAL_EXPIRE_FLAG\"].eq(1).any():\n",
    "        # Si oui, ajouter le \"SUBJECT_ID\" et la valeur 1 dans le nouveau dataframe\n",
    "        new_data_test = pd.concat([new_data_test, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [1]})], ignore_index=True)\n",
    "    else:\n",
    "        # Sinon, ajouter le \"SUBJECT_ID\" et la valeur 0 dans le nouveau dataframe\n",
    "        new_data_test = pd.concat([new_data_test, pd.DataFrame({\"SUBJECT_ID\": [subject_id], \"HOSPITAL_EXPIRE_FLAG\": [0]})], ignore_index=True)\n",
    "\n",
    "print(len(new_data_test))\n",
    "# Créer un nouveau dataframe avec les \"SUBJECT_ID\" ayant la valeur 1 dans la colonne \"HOSPITAL_EXPIRE_FLAG\"\n",
    "label_1_test = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "\n",
    "# Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur 0\n",
    "label_0_test = new_data_test[new_data_test[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "def split_text(text, k):\n",
    "    # Convertir le texte en une liste de mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Déterminer le nombre total de mots dans le texte\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculer le nombre de mots par partie\n",
    "    words_per_part = num_words // k\n",
    "\n",
    "    # Calculer le nombre de mots restants si num_words n'est pas un multiple de k\n",
    "    remainder = num_words % k\n",
    "\n",
    "    # Initialiser une liste pour stocker les parties découpées du texte\n",
    "    parts = []\n",
    "\n",
    "    # Initialiser l'indice de début pour la découpe\n",
    "    start = 0\n",
    "\n",
    "    # Parcourir chaque partie\n",
    "    for i in range(k):\n",
    "        # Calculer la position de fin pour la i-ème partie\n",
    "        end = start + words_per_part + (i < remainder)\n",
    "        # La variable \"end\" correspond à la position du dernier mot de la i-ème partie\n",
    "\n",
    "        # Ajouter la partie actuelle à la liste des parties\n",
    "        parts.append(words[start:end])\n",
    "\n",
    "        # Mettre à jour l'indice de début pour la prochaine partie\n",
    "        start = end\n",
    "\n",
    "    # Convertir les listes de mots en chaînes de caractères\n",
    "    parts = [\" \".join(part) for part in parts]\n",
    "\n",
    "    return parts\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "list_absice = []\n",
    "\n",
    "list_ordonnees_preci_rf = []\n",
    "\n",
    "list_ordonnees_rappel_rf = []\n",
    "\n",
    "list_ordonnees_f1_rf = []\n",
    "\n",
    "list_ordonnees_preci_regl = []\n",
    "\n",
    "list_ordonnees_rappel_regl = []\n",
    "\n",
    "list_ordonnees_f1_regl = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "    \n",
    "# Importer les bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Créer un échantillon test en combinant les données décédées (150 patients) et non décédées (850 patients)\n",
    "sample_test = pd.concat([label_1_test.sample(n=510 , random_state=56), label_0_test.sample(n=2490, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "filtered_data_test = data[data[\"SUBJECT_ID\"].isin(sample_test[\"SUBJECT_ID\"].values)]\n",
    "\n",
    "# Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "grouped_sample_test = filtered_data_test.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()\n",
    "\n",
    "# Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "grouped_texts_dict = grouped_sample_test.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "\n",
    "# Créer une liste pour stocker les valeurs de 'TIME' de chaque partie d'un document\n",
    "time_list_test = []\n",
    "\n",
    "# Créer un dictionnaire pour stocker les embeddings\n",
    "embeddings_dict_test = {}\n",
    "\n",
    "# Parcourir les patients et leurs données textuelles\n",
    "for subject_id, values in grouped_texts_dict.items():\n",
    "    texts = values['TEXT']  # Récupérer les documents textuels\n",
    "    times = values['TIME']  # Récupérer les valeurs de 'TIME' associées\n",
    "\n",
    "    embeddings_list = []  # Stocker les embeddings de toutes les parties de tous les documents\n",
    "\n",
    "    # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "    for text, time in zip(texts, times):\n",
    "\n",
    "        # Diviser le texte en parties de longueur appropriée\n",
    "        encoded_text = tokenizer.encode(text)\n",
    "        n_tokens = len(encoded_text)\n",
    "        n_chunks = max(1, n_tokens // 512)\n",
    "        windows = split_text(text, n_chunks)\n",
    "\n",
    "        cls_embeddings_list = []  # Stocker les embeddings CLS de chaque partie\n",
    "\n",
    "        # Parcourir les parties du document\n",
    "        for window in windows:\n",
    "            # Convertir la fenêtre en un format compatible avec le modèle\n",
    "            inputs = tokenizer(window, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Effectuer l'inférence sur le modèle\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Récupérer l'embedding du token [CLS] pour chaque fenêtre\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            cls_embeddings_list.append(cls_embeddings)\n",
    "\n",
    "            # Ajouter la valeur 'TIME' correspondante à la liste\n",
    "            time_list_test.append(time)\n",
    "\n",
    "        # Moyenne des embeddings CLS pour chaque fenêtre\n",
    "        #avg_cls_embedding = torch.mean(torch.stack(cls_embeddings_list), dim=0)\n",
    "        #embeddings_list.append(avg_cls_embedding)\n",
    "        # Ajouter la liste des embeddings CLS à la liste des embeddings pour ce document\n",
    "        embeddings_list += cls_embeddings_list\n",
    "\n",
    "    # Stocker les embeddings dans le dictionnaire avec le n° du patient comme clé\n",
    "    embeddings_dict_test[subject_id] = torch.stack(embeddings_list)\n",
    "# Importer la classe GaussianRandomProjection de la bibliothèque scikit-learn\n",
    "from sklearn import random_projection\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque sujet\n",
    "lengths_test = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "\n",
    "# Appliquer la projection gaussienne aléatoire sur les embeddings\n",
    "reduced_embeddings_np_test = transformer.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths_test)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 101)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 101)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n",
    "    \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict_test = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict_test.values()]\n",
    "\n",
    "# Concaténer tous les embeddings dans une seule matrice numpy\n",
    "embeddings_np_test = np.concatenate(list(flattened_embeddings_dict_test.values()))\n",
    "\n",
    "# Initialiser l'analyse en composantes principales (ACP) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle ACP sur les données et les transformer\n",
    "reduced_embeddings_np_test = pca.fit_transform(embeddings_np_test)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list_test = np.split(reduced_embeddings_np_test, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict_test = {key: tensor for key, tensor in zip(flattened_embeddings_dict_test.keys(), reduced_embeddings_list_test)}\n",
    "\n",
    "# Filtrer les embeddings pour exclure ceux de forme (1, 100)\n",
    "filtered_embeddings_dict_test = {key: value for key, value in reduced_embeddings_dict_test.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict_test = filtered_embeddings_dict_test\n",
    "del filtered_embeddings_dict_test\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding réduit\n",
    "for key, time in zip(reduced_embeddings_dict_test.keys(), time_list_test):\n",
    "    # Créer un tableau vide pour stocker les embeddings et le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict_test[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict_test[key].shape[0]):\n",
    "        # Concaténer l'embedding réduit avec la valeur 'time'\n",
    "        array = np.append(reduced_embeddings_dict_test[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict_test[key] = array_vide\n",
    "    \n",
    "temps = [i for i in range(0,100)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(temps, pca.explained_variance_ratio_)\n",
    "plt.xlabel('X-axis Label')  # Add x-axis label\n",
    "plt.ylabel('Y-axis Label')  # Add y-axis label\n",
    "plt.title('Title of the Plot')  # Add title\n",
    "plt.show()\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Total variance explained:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict_test.keys():\n",
    "    print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict_test[key].shape}\")\n",
    "    break\n",
    "# Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict_test.items()}\n",
    "\n",
    "# Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "\n",
    "# Concaténer tous les embeddings en une seule matrice\n",
    "embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "\n",
    "# Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "# Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "\n",
    "# Séparer les embeddings transformés pour chaque sujet\n",
    "reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "\n",
    "# Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "\n",
    "# Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "reduced_embeddings_dict = filtered_embeddings_dict\n",
    "del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "\n",
    "# Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "for key, time in zip(reduced_embeddings_dict.keys(), time_list_test):\n",
    "    # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "    array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "    for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "        # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "        array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "        array_vide[i:] = array\n",
    "    reduced_embeddings_dict[key] = array_vide\n",
    "\n",
    "# Afficher la variance expliquée par chaque composante principale\n",
    "print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Afficher la variance totale expliquée par toutes les composantes principales\n",
    "print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "for key in reduced_embeddings_dict.keys():\n",
    "    print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "    break\n",
    "    \n",
    "#import torch\n",
    "import signatory\n",
    "\n",
    "# Ordre de la signature tronqué\n",
    "depth = 2\n",
    "\n",
    "# Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "log_signature_dict_test = {}\n",
    "\n",
    "# Parcourir le dictionnaire des embeddings réduits pour chaque patient\n",
    "for key, value in reduced_embeddings_dict_test.items():\n",
    "    # Convertir le numpy array en un torch tensor et spécifier le type comme float\n",
    "    tensor = torch.from_numpy(value).float()\n",
    "    # Alternative : utiliser directement torch.tensor(value, dtype=torch.float)\n",
    "\n",
    "    # Ajouter une dimension supplémentaire pour correspondre à l'exigence du modèle (batch, stream, channel)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Calculer la log signature en utilisant le modèle signatory\n",
    "    log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "\n",
    "    # Enlever la dimension batch que nous avons ajoutée précédemment\n",
    "    log_signature = log_signature.squeeze(0)\n",
    "\n",
    "    # Ajouter le résultat au dictionnaire avec le numéro du patient comme clé\n",
    "    log_signature_dict_test[key] = log_signature\n",
    "\n",
    "# Maintenant, log_signature_dict est un dictionnaire où chaque clé correspond à un patient et chaque valeur est la log signature de ce sujet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir le dictionnaire en un DataFrame\n",
    "df_features_test = pd.DataFrame.from_dict(log_signature_dict_test, orient='index')\n",
    "\n",
    "# Réinitialiser l'index du DataFrame pour que SUBJECT_ID devienne une colonne\n",
    "df_features_test.reset_index(inplace=True)\n",
    "\n",
    "# Renommer la colonne 'index' en 'SUBJECT_ID' pour avoir une colonne de sujet\n",
    "df_features_test.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "\n",
    "# Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float si nécessaire\n",
    "for col in df_features_test.columns:\n",
    "    df_features_test[col] = df_features_test[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "\n",
    "# Fusionner le DataFrame df_features_test avec le DataFrame new_data_test sur la colonne SUBJECT_ID en utilisant une jointure interne\n",
    "df_final_test = pd.merge(df_features_test, new_data_test[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "X_train = df_final_test.iloc[:, 1:-1]\n",
    "\n",
    "# Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "y_train = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "# Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "X_test = df_final_test.iloc[:, 1:-1]\n",
    "y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "\n",
    "####################################################\n",
    "for number_ech_train in range(100,10000, 300):\n",
    "    print(\"on en est à :\")\n",
    "    print(number_ech_train)\n",
    "    list_absice.append(number_ech_train)\n",
    "    # Importer les bibliothèques nécessaires\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    #import torch\n",
    "    from torch import nn\n",
    "    new_data = new_data_test[~new_data_test[\"SUBJECT_ID\"].isin(flattened_embeddings_dict_test.keys())]\n",
    "    \n",
    "        # Charger le modèle pré-entraîné Bio_ClinicalBERT et son tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
    "    \n",
    "    # Filtrer les données test (new_data) pour ne conserver que les patients absents dans le dictionnaire des embeddings (flattened_embeddings_dict)\n",
    "\n",
    "\n",
    "    # Regrouper les données test par \"SUBJECT_ID\" en listes de textes et de temps\n",
    "\n",
    "    # Sélectionner les données test ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 1 (décédés)\n",
    "    label_1 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "    \n",
    "    # Sélectionner aléatoirement le même nombre de \"SUBJECT_ID\" ayant la valeur \"HOSPITAL_EXPIRE_FLAG\" égale à 0 (non décédés)\n",
    "    label_0 = new_data[new_data[\"HOSPITAL_EXPIRE_FLAG\"] == 0].reset_index()\n",
    "\n",
    "    # Charger le modèle de langue pré-entraîné (Bio_ClinicalBERT) et le tokenizer associé\n",
    "    n1 = int(50 * number_ech_train /100)\n",
    "    n2 = int(50 * number_ech_train /100)       \n",
    "    # Sélectionner aléatoirement 1000 individus de chaque classe (label_1 et label_0)\n",
    "    sample = pd.concat([label_1.sample(n=n1, random_state=56), label_0.sample(n=n2, random_state=78)]).reset_index().drop('index', axis=1)\n",
    "        \n",
    "    # Filtrer les données d'observation (data) pour ne conserver que les patients présents dans l'échantillon test (sample_test)\n",
    "    filtered_data = data[data[\"SUBJECT_ID\"].isin(sample[\"SUBJECT_ID\"].values)]\n",
    "    \n",
    "    \n",
    "    grouped_sample = filtered_data.groupby('SUBJECT_ID').agg({'TEXT': list, 'TIME': list}).reset_index()\n",
    "    \n",
    "    # Créer un dictionnaire de la forme {n° patient: [liste des textes, valeurs time]} pour faciliter l'itération\n",
    "    grouped_texts_dict = grouped_sample.set_index('SUBJECT_ID')[['TEXT', 'TIME']].to_dict(orient='index')\n",
    "    \n",
    "    # Initialiser une liste pour stocker les valeurs 'TIME' de chaque partie d'un document\n",
    "    time_list = []\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les embeddings\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    # Parcourir les patients et leurs données associées\n",
    "    for subject_id, values in grouped_texts_dict.items():\n",
    "        texts = values['TEXT']  # Récupérer la liste des documents\n",
    "        times = values['TIME']  # Récupérer la liste des valeurs 'TIME' associées aux documents\n",
    "        embeddings_list = []  # Liste pour stocker les embeddings de toutes les parties de tous les documents\n",
    "    \n",
    "        # Parcourir les documents et leurs valeurs 'TIME' associées\n",
    "        for text, time in zip(texts, times):\n",
    "            # Diviser le texte en parties égales\n",
    "            encoded_text = tokenizer.encode(text)  # Encodage du texte en une séquence de tokens\n",
    "            n_tokens = len(encoded_text)  # Nombre de tokens dans la séquence\n",
    "            n_chunks = max(1, n_tokens // 512)  # Calcul du nombre optimal de parties\n",
    "            parties = split_text(text, n_chunks)  # Liste des parties du texte\n",
    "    \n",
    "            # Stocker les embeddings des différentes parties du document\n",
    "            cls_embeddings_list = []  # Liste pour stocker les embeddings [CLS] des parties\n",
    "    \n",
    "            # Parcourir les parties du document\n",
    "            for partie in parties:\n",
    "                # Convertir la partie dans un format compatible avec le modèle\n",
    "                inputs = tokenizer(partie, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    # Effectuer l'inférence pour obtenir les résultats du modèle\n",
    "                    outputs = model(**inputs)\n",
    "    \n",
    "                # Récupérer l'embedding du token [CLS] pour chaque partie\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                # Stocker l'embedding dans la liste\n",
    "                cls_embeddings_list.append(cls_embeddings)\n",
    "                # Stocker la valeur 'TIME' (la même pour toutes les parties du même document)\n",
    "                time_list.append(time)\n",
    "    \n",
    "            # Ajouter la liste des embeddings [CLS] à la liste des embeddings de ce document\n",
    "            embeddings_list += cls_embeddings_list\n",
    "    \n",
    "        # Stocker les embeddings dans un dictionnaire avec le numéro du patient comme clé\n",
    "        embeddings_dict[subject_id] = torch.stack(embeddings_list)\n",
    "\n",
    "    \n",
    "    # Importer la classe random_projection du module sklearn\n",
    "    from sklearn import random_projection\n",
    "    \n",
    "    # ClinicalBERT renvoie des embeddings au format de tensor PyTorch.\n",
    "    # Nous les convertissons en tableau NumPy pour la réduction de dimension\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "    \n",
    "    # Créer une liste pour stocker le nombre d'embeddings que possède chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "    \n",
    "    # Concaténer tous les embeddings pour créer une matrice unique\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "    \n",
    "    # Initialiser la projection gaussienne aléatoire avec 100 composantes\n",
    "    transformer = random_projection.GaussianRandomProjection(n_components=100)\n",
    "    \n",
    "    # Réduire la dimension des embeddings en utilisant la projection gaussienne aléatoire\n",
    "    reduced_embeddings_np = transformer.fit_transform(embeddings_np)\n",
    "    \n",
    "    # Diviser les embeddings réduits pour chaque patient\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "    \n",
    "    # Recréer le dictionnaire des embeddings réduits avec les numéros de patient correspondants\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "    \n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "    \n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    \n",
    "    # Vérifier que le lien entre chaque sujet et ses embeddings est conservé\n",
    "    for key in reduced_embeddings_dict.keys():\n",
    "        print(f\"Subject ID: {key}, Embedding shape: {reduced_embeddings_dict[key].shape}\")\n",
    "        break\n",
    "        \n",
    "    # Importer la classe PCA (Analyse en Composantes Principales) du module sklearn\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Créer un nouveau dictionnaire pour stocker les embeddings aplatis\n",
    "    flattened_embeddings_dict = {key: tensor.view(tensor.shape[0], -1).cpu().numpy() for key, tensor in embeddings_dict.items()}\n",
    "    \n",
    "    # Créer une liste pour stocker le nombre d'embeddings pour chaque patient\n",
    "    lengths = [len(tensor) for tensor in flattened_embeddings_dict.values()]\n",
    "    \n",
    "    # Concaténer tous les embeddings en une seule matrice\n",
    "    embeddings_np = np.concatenate(list(flattened_embeddings_dict.values()))\n",
    "    \n",
    "    # Initialiser le modèle PCA (Analyse en Composantes Principales) avec 100 composantes\n",
    "    pca = PCA(n_components=100)\n",
    "    \n",
    "    # Ajuster le modèle PCA aux données et les transformer pour réduire la dimension\n",
    "    reduced_embeddings_np = pca.fit_transform(embeddings_np)\n",
    "    \n",
    "    # Séparer les embeddings transformés pour chaque sujet\n",
    "    reduced_embeddings_list = np.split(reduced_embeddings_np, np.cumsum(lengths)[:-1])\n",
    "    \n",
    "    # Recréer le dictionnaire des embeddings transformés avec les clés correspondantes\n",
    "    reduced_embeddings_dict = {key: tensor for key, tensor in zip(flattened_embeddings_dict.keys(), reduced_embeddings_list)}\n",
    "    \n",
    "    # Filtrer les embeddings dont la forme est différente de (1, 100)\n",
    "    filtered_embeddings_dict = {key: value for key, value in reduced_embeddings_dict.items() if value.shape != (1, 100)}\n",
    "    reduced_embeddings_dict = filtered_embeddings_dict\n",
    "    del filtered_embeddings_dict  # Supprimer la variable temporaire\n",
    "    \n",
    "    # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "    for key, time in zip(reduced_embeddings_dict.keys(), time_list):\n",
    "        # Créer un tableau vide pour stocker les embeddings avec le temps\n",
    "        array_vide = np.empty((reduced_embeddings_dict[key].shape[0], 101))\n",
    "        for i in range(reduced_embeddings_dict[key].shape[0]):\n",
    "            # Ajouter la valeur 'time' à la fin de chaque embedding\n",
    "            array = np.append(reduced_embeddings_dict[key][i], time)\n",
    "            array_vide[i:] = array\n",
    "        reduced_embeddings_dict[key] = array_vide\n",
    "    \n",
    "    # Afficher la variance expliquée par chaque composante principale\n",
    "    print(\"Variance expliquée par chaque composante principale:\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Afficher la variance totale expliquée par toutes les composantes principales\n",
    "    print(\"Variance totale expliquée:\", sum(pca.explained_variance_ratio_))\n",
    "    \n",
    "    # Vérifier que le lien entre chaque sujet et ses embeddings est bien conservé\n",
    "    for key in reduced_embeddings_dict.keys():\n",
    "        print(f\"ID du patient : {key}, Forme des embeddings : {reduced_embeddings_dict[key].shape}\")\n",
    "        break\n",
    "        \n",
    "    # Importer les bibliothèques nécessaires\n",
    "    #import torch\n",
    "    import signatory\n",
    "    \n",
    "    # Ordre de la signature tronquée\n",
    "    depth = 2\n",
    "    \n",
    "    # Créer un nouveau dictionnaire pour stocker les résultats de la log signature\n",
    "    log_signature_dict = {}\n",
    "    \n",
    "    # Parcourir le dictionnaire des embeddings réduits (reduced_embeddings_dict)\n",
    "    for key, value in reduced_embeddings_dict.items():\n",
    "        # Convertir les tableaux NumPy en tenseurs PyTorch de type float\n",
    "        tensor = torch.from_numpy(value).float().to(\"cuda\")\n",
    "    \n",
    "        # Ajouter une dimension \"batch\" pour correspondre au format requis (batch, stream, channel)\n",
    "        tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "        # Calculer la log signature en utilisant la bibliothèque Signatory\n",
    "        log_signature = signatory.logsignature(path=tensor, depth=depth)\n",
    "    \n",
    "        # Enlever la dimension \"batch\" que nous avons ajoutée précédemment\n",
    "        log_signature = log_signature.squeeze(0).to(\"cuda\")\n",
    "    \n",
    "        # Ajouter le résultat dans le dictionnaire log_signature_dict\n",
    "        log_signature_dict[key] = log_signature\n",
    "    \n",
    "    # À ce stade, log_signature_dict est un dictionnaire où chaque clé correspond à un numéro de patient, et chaque valeur est la log signature de ce patient.\n",
    "    \n",
    "    \n",
    "    # Convertir le dictionnaire log_signature_dict en un DataFrame\n",
    "    df_features = pd.DataFrame.from_dict(log_signature_dict, orient='index')\n",
    "    \n",
    "    # Réinitialiser l'index pour que 'SUBJECT_ID' devienne une colonne du DataFrame\n",
    "    df_features.reset_index(inplace=True)\n",
    "    \n",
    "    # Renommer la colonne d'index en 'SUBJECT_ID'\n",
    "    df_features.rename(columns={'index':'SUBJECT_ID'}, inplace=True)\n",
    "    \n",
    "    # Parcourir chaque colonne du DataFrame et convertir chaque tenseur en float (si nécessaire)\n",
    "    for col in df_features.columns:\n",
    "        df_features[col] = df_features[col].apply(lambda x: x.item() if torch.is_tensor(x) else x)\n",
    "    \n",
    "    # Fusionner le DataFrame df_features avec le DataFrame new_data sur la colonne 'SUBJECT_ID'\n",
    "    df_final = pd.merge(df_features, new_data[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']], on='SUBJECT_ID', how='inner')\n",
    "    \n",
    "    df_final.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialisation du modèle de régression logistique\n",
    "    \n",
    "    # Créer une instance du modèle de régression logistique avec les hyperparamètres spécifiés\n",
    "    model = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=100000, n_jobs= -1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    \n",
    "    # Entraîner le modèle sur les données d'entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction sur l'ensemble de test\n",
    "    \n",
    "    # Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "    # Évaluation des performances du modèle\n",
    "    \n",
    "    # Calculer et afficher l'accuracy du modèle\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    # Calculer et afficher le rappel du modèle\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "    \n",
    "    # Calculer et afficher le F1-score du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "    \n",
    "    \n",
    "    list_ordonnees_preci_regl.append(accuracy)\n",
    "    \n",
    "    list_ordonnees_rappel_regl.append(recall)\n",
    "    \n",
    "    list_ordonnees_f1_regl.append(f1)\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    import math\n",
    "    \n",
    "    # Séparation des données en ensembles d'entraînement et de test\n",
    "    \n",
    "    # Sélectionner toutes les colonnes sauf la dernière (la colonne 'HOSPITAL_EXPIRE_FLAG') comme caractéristiques d'entraînement\n",
    "    X_train = df_final.iloc[:, 1:-1]\n",
    "    \n",
    "    # Sélectionner la dernière colonne ('HOSPITAL_EXPIRE_FLAG') comme variable cible d'entraînement et la convertir en entiers\n",
    "    y_train = df_final[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "    \n",
    "    # Sélectionner les caractéristiques de l'ensemble de test de manière similaire\n",
    "    X_test = df_final_test.iloc[:, 1:-1]\n",
    "    y_test = df_final_test[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "    \n",
    "    # Définir les valeurs des hyperparamètres à tester\n",
    "    \n",
    "    # Créer un dictionnaire de valeurs à tester pour le nombre maximal de caractéristiques utilisées à chaque division de l'arbre\n",
    "    param_grid = {\n",
    "        'max_features': list(range(30, 121, 10))\n",
    "    }\n",
    "    \n",
    "    # Initialisation du modèle Random Forest\n",
    "    \n",
    "    # Créer une instance du modèle Random Forest avec des hyperparamètres spécifiés\n",
    "    model = RandomForestClassifier(criterion='entropy', n_estimators=150)\n",
    "    \n",
    "    # Recherche des meilleurs hyperparamètres\n",
    "    \n",
    "    # Créer un objet GridSearchCV pour effectuer une recherche des meilleurs hyperparamètres\n",
    "    # cv=5 indique une validation croisée en 5 plis et scoring='f1_micro' utilise le F1-score pour l'évaluation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_micro')\n",
    "    \n",
    "    # Effectuer la recherche des meilleurs hyperparamètres en utilisant les données d'entraînement\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtenir les meilleures valeurs des hyperparamètres\n",
    "    best_max_features = grid_search.best_params_['max_features']\n",
    "    \n",
    "    # Utilisation du modèle avec les meilleurs hyperparamètres pour la prédiction sur l'ensemble de test\n",
    "    \n",
    "    # Créer une nouvelle instance du modèle Random Forest avec les meilleurs hyperparamètres trouvés\n",
    "    best_model = RandomForestClassifier(criterion='entropy', n_estimators=150, max_features=best_max_features)\n",
    "    \n",
    "    # Entraîner le modèle avec les meilleurs hyperparamètres sur les données d'entraînement\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédire les étiquettes sur l'ensemble de test en utilisant le modèle entraîné\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Évaluation des performances du modèle\n",
    "    \n",
    "    # Calculer et afficher l'accuracy du modèle\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Précision: %.2f%%\" % (accuracy * 100.0))\n",
    "    \n",
    "    # Calculer et afficher le rappel du modèle\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(\"Rappel: %.2f%%\" % (recall * 100.0))\n",
    "    \n",
    "    # Calculer et afficher le F1-score du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1-score: %.2f%%\" % (f1 * 100.0))\n",
    "    \n",
    "    list_ordonnees_preci_rf.append(accuracy)\n",
    "\n",
    "    list_ordonnees_rappel_rf.append(recall)\n",
    "    \n",
    "    list_ordonnees_f1_rf.append(f1)\n",
    "    \n",
    "print()\n",
    "\n",
    "print(list_absice)\n",
    "\n",
    "print(list_ordonnees_preci_rf)\n",
    "\n",
    "print(list_ordonnees_rappel_rf)\n",
    "\n",
    "print(list_ordonnees_f1_rf)\n",
    "\n",
    "print(list_ordonnees_preci_regl)\n",
    "\n",
    "print(list_ordonnees_rappel_regl)\n",
    "\n",
    "print(list_ordonnees_f1_regl)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_f1_rf, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"f1 random forest\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"F1\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "    \n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_rappel_rf, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"rappel random forest\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"rappel\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_preci_rf, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"précision random forest\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"précision\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_preci_regl, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"précision régression logistique\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"précision\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_rappel_regl, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"rappel régression logistique\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"rappel\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()  # Optionnel: Définit la taille du graphique\n",
    "plt.plot(list_absice, list_ordonnees_f1_regl, marker='o', linestyle='-', color='b')  # Trace le graphique avec des points reliés par des lignes\n",
    "\n",
    "# Ajouter des titres et des labels\n",
    "plt.title(\"F1 régression logistique\", fontsize=16, fontweight='bold')  # Titre du graphique\n",
    "plt.xlabel(\"nombre de personne\", fontsize=14)  # Label pour l'axe des X\n",
    "plt.ylabel(\"F1\", fontsize=14)  # Label pour l'axe des Y\n",
    "\n",
    "# Optionnel: Ajouter une grille pour une meilleure lisibilité\n",
    "plt.grid(True)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T11:16:03.726074900Z",
     "start_time": "2024-03-18T11:16:01.960350200Z"
    }
   },
   "id": "64b3cc3dc96dfe68",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17eb690837ec30f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
